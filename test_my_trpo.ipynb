{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "from garage import wrap_experiment\n",
    "from garage.envs import GymEnv\n",
    "from garage.experiment.deterministic import set_seed\n",
    "from garage.sampler import LocalSampler\n",
    "from garage.torch.algos import TRPO\n",
    "\n",
    "# from garage.torch.policies import GaussianMLPPolicy\n",
    "from policies.gaussian_mlp_policy import GaussianMLPPolicy\n",
    "\n",
    "from garage.torch.value_functions import GaussianMLPValueFunction\n",
    "from garage.trainer import Trainer\n",
    "\n",
    "from TRPO_DRSOM import TRPO_DRSOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@wrap_experiment(log_dir='drsom_test/momentum-opt-NP}+G')\n",
    "def trpo_pendulum(ctxt=None, seed=1):\n",
    "    set_seed(seed)\n",
    "\n",
    "    env = GymEnv('MountainCarContinuous-v0')\n",
    "    # env = GymEnv('Pendulum-v0')\n",
    "\n",
    "    trainer = Trainer(ctxt)\n",
    "    policy = GaussianMLPPolicy(env.spec,\n",
    "                               hidden_sizes=[32, 32],\n",
    "                               hidden_nonlinearity=torch.tanh,\n",
    "                               output_nonlinearity=None)\n",
    "\n",
    "\n",
    "    value_function = GaussianMLPValueFunction(env_spec=env.spec,\n",
    "                                              hidden_sizes=(32, 32),\n",
    "                                              hidden_nonlinearity=torch.tanh,\n",
    "                                              output_nonlinearity=None)\n",
    "    sampler = LocalSampler(agents=policy,\n",
    "                           envs=env,\n",
    "                           max_episode_length=env.spec.max_episode_length)\n",
    "    algo = TRPO_DRSOM(env_spec=env.spec,\n",
    "                      policy=policy,\n",
    "                      value_function=value_function,\n",
    "                      sampler=sampler,\n",
    "                      discount=0.99,\n",
    "                      center_adv=False)\n",
    "    trainer.setup(algo, env)\n",
    "    trainer.train(n_epochs=200, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:54:50 | [trpo_pendulum] Logging to drsom_test/momentum-opt-NPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\garage\\experiment\\deterministic.py:36: UserWarning: Enabeling deterministic mode in PyTorch can have a performance impact when using GPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:54:51 | [trpo_pendulum] Obtaining samples...\n",
      "old policy para is\n",
      "tensor([ 0.0000, -0.0881,  0.1847,  ..., -0.2844,  0.1172,  0.0000])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0000, -0.0881,  0.1847,  ..., -0.2844,  0.1172,  0.0000])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(0.7056)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[2.2476e+00, 1.6441e-08],\n",
      "        [1.6441e-08, 1.0483e-15]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-5.0000e-01, -3.6575e-09])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0197, -0.0448,  0.0171,  ...,  0.0346,  0.1038,  1.1073])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(8.6540, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(9.3054, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:54:52 | [trpo_pendulum] epoch #0 | Saving snapshot...\n",
      "2022-09-02 00:54:52 | [trpo_pendulum] epoch #0 | Saved\n",
      "2022-09-02 00:54:52 | [trpo_pendulum] epoch #0 | Time 1.22 s\n",
      "2022-09-02 00:54:52 | [trpo_pendulum] epoch #0 | EpochTime 1.22 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -10.3533\n",
      "Evaluation/AverageReturn             -96.1029\n",
      "Evaluation/Iteration                   0\n",
      "Evaluation/MaxReturn                 -92.5148\n",
      "Evaluation/MinReturn                 -99.6909\n",
      "Evaluation/NumEpisodes                 2\n",
      "Evaluation/StdReturn                   3.58808\n",
      "Evaluation/TerminationRate             0\n",
      "TotalEnvSteps                       1998\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0000, -0.0881,  0.1847,  ..., -0.2844,  0.1172,  0.0000])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0197, -0.1330,  0.2018,  ..., -0.2498,  0.2209,  1.1073])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.0871)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-395.5903,   -7.2849],\n",
      "        [  -7.2849,    1.5272]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0105, -0.4999])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0094,  0.0297, -0.0269,  ...,  0.0094, -0.0991, -0.5800])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(39.9787, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(37.4111, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:54:53 | [trpo_pendulum] epoch #1 | Saving snapshot...\n",
      "2022-09-02 00:54:53 | [trpo_pendulum] epoch #1 | Saved\n",
      "2022-09-02 00:54:53 | [trpo_pendulum] epoch #1 | Time 2.43 s\n",
      "2022-09-02 00:54:53 | [trpo_pendulum] epoch #1 | EpochTime 1.21 s\n",
      "----------------------------------  ---------\n",
      "Evaluation/AverageDiscountedReturn   -48.9061\n",
      "Evaluation/AverageReturn            -489.558\n",
      "Evaluation/Iteration                   1\n",
      "Evaluation/MaxReturn                -474.269\n",
      "Evaluation/MinReturn                -504.848\n",
      "Evaluation/NumEpisodes                 2\n",
      "Evaluation/StdReturn                  15.2893\n",
      "Evaluation/TerminationRate             0\n",
      "TotalEnvSteps                       3996\n",
      "----------------------------------  ---------\n",
      "old policy para is\n",
      "tensor([ 0.0197, -0.1330,  0.2018,  ..., -0.2498,  0.2209,  1.1073])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0104, -0.1033,  0.1749,  ..., -0.2404,  0.1218,  0.5273])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(0.3590)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-0.8936,  0.6366],\n",
      "        [ 0.6366,  0.3301]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.4990, 0.0322])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0393,  1.8740,  0.0450,  ...,  0.2118, -0.5933,  0.6979])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-1.8497, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-0.5457, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:54:54 | [trpo_pendulum] epoch #2 | Saving snapshot...\n",
      "2022-09-02 00:54:54 | [trpo_pendulum] epoch #2 | Saved\n",
      "2022-09-02 00:54:54 | [trpo_pendulum] epoch #2 | Time 3.62 s\n",
      "2022-09-02 00:54:54 | [trpo_pendulum] epoch #2 | EpochTime 1.19 s\n",
      "----------------------------------  -------------\n",
      "Evaluation/AverageDiscountedReturn   -13.0401\n",
      "Evaluation/AverageReturn            -130.626\n",
      "Evaluation/Iteration                   2\n",
      "Evaluation/MaxReturn                -130.624\n",
      "Evaluation/MinReturn                -130.628\n",
      "Evaluation/NumEpisodes                 2\n",
      "Evaluation/StdReturn                   0.00171661\n",
      "Evaluation/TerminationRate             0\n",
      "TotalEnvSteps                       5994\n",
      "----------------------------------  -------------\n",
      "old policy para is\n",
      "tensor([ 0.0104, -0.1033,  0.1749,  ..., -0.2404,  0.1218,  0.5273])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0497,  1.7708,  0.2199,  ..., -0.0286, -0.4715,  1.2251])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.1952)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 4.0391e+04, -3.8195e+02],\n",
      "        [-3.8195e+02,  2.0319e+01]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0021, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0214,  0.9317,  0.0049,  ...,  0.1077, -0.3017,  0.3634])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(124.9441, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(124.0123, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:54:55 | [trpo_pendulum] epoch #3 | Saving snapshot...\n",
      "2022-09-02 00:54:55 | [trpo_pendulum] epoch #3 | Saved\n",
      "2022-09-02 00:54:55 | [trpo_pendulum] epoch #3 | Time 4.65 s\n",
      "2022-09-02 00:54:55 | [trpo_pendulum] epoch #3 | EpochTime 1.02 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -154.046\n",
      "Evaluation/AverageReturn            -1532.04\n",
      "Evaluation/Iteration                    3\n",
      "Evaluation/MaxReturn                -1511.42\n",
      "Evaluation/MinReturn                -1552.66\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   20.6176\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                        7992\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0497,  1.7708,  0.2199,  ..., -0.0286, -0.4715,  1.2251])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0711,  2.7025,  0.2248,  ...,  0.0791, -0.7732,  1.5885])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(11.4860)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[227.1346,   0.8688],\n",
      "        [  0.8688,   7.4840]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0226, -0.4995])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0303, -0.4656,  0.0013,  ..., -0.0543,  0.1513, -0.1809])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(37.9716, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(37.7069, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:54:57 | [trpo_pendulum] epoch #4 | Saving snapshot...\n",
      "2022-09-02 00:54:57 | [trpo_pendulum] epoch #4 | Saved\n",
      "2022-09-02 00:54:57 | [trpo_pendulum] epoch #4 | Time 5.96 s\n",
      "2022-09-02 00:54:57 | [trpo_pendulum] epoch #4 | EpochTime 1.31 s\n",
      "----------------------------------  ---------\n",
      "Evaluation/AverageDiscountedReturn   -59.0098\n",
      "Evaluation/AverageReturn            -634.042\n",
      "Evaluation/Iteration                   4\n",
      "Evaluation/MaxReturn                -587.94\n",
      "Evaluation/MinReturn                -680.144\n",
      "Evaluation/NumEpisodes                 2\n",
      "Evaluation/StdReturn                  46.1023\n",
      "Evaluation/TerminationRate             0\n",
      "TotalEnvSteps                       9990\n",
      "----------------------------------  ---------\n",
      "old policy para is\n",
      "tensor([ 0.0711,  2.7025,  0.2248,  ...,  0.0791, -0.7732,  1.5885])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0408,  2.2369,  0.2261,  ...,  0.0248, -0.6219,  1.4076])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.4571)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[385.5330,  -2.0455],\n",
      "        [ -2.0455,   3.4749]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.3314,  0.3744])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.3274, -0.1277,  0.2339,  ...,  0.0517,  0.0228, -0.1874])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(88.6854, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(64.8028, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:54:58 | [trpo_pendulum] epoch #5 | Saving snapshot...\n",
      "2022-09-02 00:54:58 | [trpo_pendulum] epoch #5 | Saved\n",
      "2022-09-02 00:54:58 | [trpo_pendulum] epoch #5 | Time 6.97 s\n",
      "2022-09-02 00:54:58 | [trpo_pendulum] epoch #5 | EpochTime 1.01 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -120.351\n",
      "Evaluation/AverageReturn            -1213.07\n",
      "Evaluation/Iteration                    5\n",
      "Evaluation/MaxReturn                -1192.2\n",
      "Evaluation/MinReturn                -1233.94\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   20.8696\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       11988\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0408,  2.2369,  0.2261,  ...,  0.0248, -0.6219,  1.4076])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.2866,  2.1092,  0.4600,  ...,  0.0765, -0.5991,  1.2202])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.7962)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 0.3152, -0.8869],\n",
      "        [-0.8869,  0.6093]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.1718,  0.4695])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.1489, -0.0534,  0.1144,  ...,  0.0942, -0.0288, -0.0737])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(0.5664, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-0.1722, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:54:59 | [trpo_pendulum] epoch #6 | Saving snapshot...\n",
      "2022-09-02 00:54:59 | [trpo_pendulum] epoch #6 | Saved\n",
      "2022-09-02 00:54:59 | [trpo_pendulum] epoch #6 | Time 7.98 s\n",
      "2022-09-02 00:54:59 | [trpo_pendulum] epoch #6 | EpochTime 1.01 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -10.098\n",
      "Evaluation/AverageReturn             -251.277\n",
      "Evaluation/Iteration                    6\n",
      "Evaluation/MaxReturn                 -202.806\n",
      "Evaluation/MinReturn                 -299.748\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   48.4714\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       13986\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.2866,  2.1092,  0.4600,  ...,  0.0765, -0.5991,  1.2202])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.4355,  2.0558,  0.5744,  ...,  0.1707, -0.6279,  1.1465])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.8187)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-17.5299,  -0.3944],\n",
      "        [ -0.3944,  -0.4439]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0034,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0743, -0.0267,  0.0571,  ...,  0.0481, -0.0149, -0.0374])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-3.3309, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-3.1424, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:00 | [trpo_pendulum] epoch #7 | Saving snapshot...\n",
      "2022-09-02 00:55:00 | [trpo_pendulum] epoch #7 | Saved\n",
      "2022-09-02 00:55:00 | [trpo_pendulum] epoch #7 | Time 9.15 s\n",
      "2022-09-02 00:55:00 | [trpo_pendulum] epoch #7 | EpochTime 1.17 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -28.6449\n",
      "Evaluation/AverageReturn             -233.472\n",
      "Evaluation/Iteration                    7\n",
      "Evaluation/MaxReturn                 -217.781\n",
      "Evaluation/MinReturn                 -249.164\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   15.6914\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       15984\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.4355,  2.0558,  0.5744,  ...,  0.1707, -0.6279,  1.1465])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.5099,  2.0291,  0.6315,  ...,  0.2188, -0.6428,  1.1092])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.6324)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-0.5929,  0.0465],\n",
      "        [ 0.0465, -0.0826]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.2026, 0.4571])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0456, -0.0138,  0.0291,  ..., -0.0015, -0.0052, -0.0154])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(5.1799, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(5.3114, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:01 | [trpo_pendulum] epoch #8 | Saving snapshot...\n",
      "2022-09-02 00:55:01 | [trpo_pendulum] epoch #8 | Saved\n",
      "2022-09-02 00:55:01 | [trpo_pendulum] epoch #8 | Time 10.21 s\n",
      "2022-09-02 00:55:01 | [trpo_pendulum] epoch #8 | EpochTime 1.06 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn    -32.4957\n",
      "Evaluation/AverageReturn             -321.077\n",
      "Evaluation/Iteration                    8\n",
      "Evaluation/MaxReturn                 -318.032\n",
      "Evaluation/MinReturn                 -324.122\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    3.04492\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       17982\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.5099,  2.0291,  0.6315,  ...,  0.2188, -0.6428,  1.1092])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.5555,  2.0153,  0.6607,  ...,  0.2173, -0.6480,  1.0937])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(0.3252)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-7.2956e-01, -2.2954e-04],\n",
      "        [-2.2954e-04, -1.4164e-02]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.1464, 0.4781])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0363, -0.0052,  0.0138,  ..., -0.0098,  0.0134,  0.0086])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(9.8588, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(9.9074, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:02 | [trpo_pendulum] epoch #9 | Saving snapshot...\n",
      "2022-09-02 00:55:02 | [trpo_pendulum] epoch #9 | Saved\n",
      "2022-09-02 00:55:02 | [trpo_pendulum] epoch #9 | Time 11.29 s\n",
      "2022-09-02 00:55:02 | [trpo_pendulum] epoch #9 | EpochTime 1.07 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn    -38.1839\n",
      "Evaluation/AverageReturn             -375.689\n",
      "Evaluation/Iteration                    9\n",
      "Evaluation/MaxReturn                 -367.498\n",
      "Evaluation/MinReturn                 -383.881\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    8.19139\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       19980\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.5555,  2.0153,  0.6607,  ...,  0.2173, -0.6480,  1.0937])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.5917,  2.0100,  0.6745,  ...,  0.2075, -0.6346,  1.1023])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.1171)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-11.5148,  -0.5378],\n",
      "        [ -0.5378,  -0.6210]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.2523, 0.4317])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.1350, -0.0064,  0.0057,  ..., -0.0132,  0.0175,  0.0155])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(17.1216, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(17.0852, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:03 | [trpo_pendulum] epoch #10 | Saving snapshot...\n",
      "2022-09-02 00:55:03 | [trpo_pendulum] epoch #10 | Saved\n",
      "2022-09-02 00:55:03 | [trpo_pendulum] epoch #10 | Time 12.28 s\n",
      "2022-09-02 00:55:03 | [trpo_pendulum] epoch #10 | EpochTime 0.99 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn    -47.3282\n",
      "Evaluation/AverageReturn             -462.448\n",
      "Evaluation/Iteration                   10\n",
      "Evaluation/MaxReturn                 -461.665\n",
      "Evaluation/MinReturn                 -463.232\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    0.783448\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       21978\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.5917,  2.0100,  0.6745,  ...,  0.2075, -0.6346,  1.1023])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.7268,  2.0036,  0.6802,  ...,  0.1944, -0.6171,  1.1178])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(1.4817)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.3588,  0.0048],\n",
      "        [ 0.0048,  0.9094]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.4006, 0.2992])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0420,  0.0024, -0.0031,  ...,  0.1183, -0.0259, -0.0262])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(7.8630, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(6.9185, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:04 | [trpo_pendulum] epoch #11 | Saving snapshot...\n",
      "2022-09-02 00:55:04 | [trpo_pendulum] epoch #11 | Saved\n",
      "2022-09-02 00:55:04 | [trpo_pendulum] epoch #11 | Time 13.28 s\n",
      "2022-09-02 00:55:04 | [trpo_pendulum] epoch #11 | EpochTime 1.00 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn    -37.1438\n",
      "Evaluation/AverageReturn             -370.175\n",
      "Evaluation/Iteration                   11\n",
      "Evaluation/MaxReturn                 -363.058\n",
      "Evaluation/MinReturn                 -377.291\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    7.11646\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       23976\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.7268,  2.0036,  0.6802,  ...,  0.1944, -0.6171,  1.1178])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6847,  2.0060,  0.6771,  ...,  0.3127, -0.6430,  1.0916])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.4482)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.1263,  3.7980],\n",
      "        [ 3.7980,  3.3633]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.1519, -0.4764])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0305,  0.0070, -0.0005,  ..., -0.1276,  0.0140,  0.0142])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(14.7855, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(15.5848, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:05 | [trpo_pendulum] epoch #12 | Saving snapshot...\n",
      "2022-09-02 00:55:05 | [trpo_pendulum] epoch #12 | Saved\n",
      "2022-09-02 00:55:05 | [trpo_pendulum] epoch #12 | Time 14.39 s\n",
      "2022-09-02 00:55:05 | [trpo_pendulum] epoch #12 | EpochTime 1.11 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn    -46.208\n",
      "Evaluation/AverageReturn             -453.922\n",
      "Evaluation/Iteration                   12\n",
      "Evaluation/MaxReturn                 -447.573\n",
      "Evaluation/MinReturn                 -460.272\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    6.34924\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       25974\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6847,  2.0060,  0.6771,  ...,  0.3127, -0.6430,  1.0916])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6542,  2.0130,  0.6766,  ...,  0.1850, -0.6290,  1.1058])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(1.8708)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.6568, -0.3643],\n",
      "        [-0.3643, -4.1845]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.4903, 0.0980])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.1377, -0.0038,  0.0026,  ...,  0.0448, -0.0066, -0.0018])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(5.9406, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(6.0978, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:06 | [trpo_pendulum] epoch #13 | Saving snapshot...\n",
      "2022-09-02 00:55:06 | [trpo_pendulum] epoch #13 | Saved\n",
      "2022-09-02 00:55:06 | [trpo_pendulum] epoch #13 | Time 15.51 s\n",
      "2022-09-02 00:55:06 | [trpo_pendulum] epoch #13 | EpochTime 1.12 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -25.7004\n",
      "Evaluation/AverageReturn             -356.688\n",
      "Evaluation/Iteration                   13\n",
      "Evaluation/MaxReturn                 -331.295\n",
      "Evaluation/MinReturn                 -382.08\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   25.3925\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       27972\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6542,  2.0130,  0.6766,  ...,  0.1850, -0.6290,  1.1058])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.7919,  2.0092,  0.6791,  ...,  0.2299, -0.6356,  1.1041])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.2342)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-0.2063, -1.1612],\n",
      "        [-1.1612, -3.3335]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.4657,  0.1820])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0341,  0.0017, -0.0047,  ...,  0.0376,  0.0033,  0.0201])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(5.6053, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(5.0998, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:07 | [trpo_pendulum] epoch #14 | Saving snapshot...\n",
      "2022-09-02 00:55:07 | [trpo_pendulum] epoch #14 | Saved\n",
      "2022-09-02 00:55:07 | [trpo_pendulum] epoch #14 | Time 16.53 s\n",
      "2022-09-02 00:55:07 | [trpo_pendulum] epoch #14 | EpochTime 1.01 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -21.9131\n",
      "Evaluation/AverageReturn             -354.827\n",
      "Evaluation/Iteration                   14\n",
      "Evaluation/MaxReturn                 -339.556\n",
      "Evaluation/MinReturn                 -370.097\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   15.2708\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       29970\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.7919,  2.0092,  0.6791,  ...,  0.2299, -0.6356,  1.1041])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.7577,  2.0108,  0.6745,  ...,  0.2675, -0.6323,  1.1242])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.0302)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-0.0880, -0.0362],\n",
      "        [-0.0362, -0.0970]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.3996, -0.3005])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0238, -0.0039,  0.0033,  ..., -0.0166,  0.0015, -0.0041])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(0.4241, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(0.5151, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:08 | [trpo_pendulum] epoch #15 | Saving snapshot...\n",
      "2022-09-02 00:55:08 | [trpo_pendulum] epoch #15 | Saved\n",
      "2022-09-02 00:55:08 | [trpo_pendulum] epoch #15 | Time 17.55 s\n",
      "2022-09-02 00:55:08 | [trpo_pendulum] epoch #15 | EpochTime 1.02 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn    -25.6516\n",
      "Evaluation/AverageReturn             -312.93\n",
      "Evaluation/Iteration                   15\n",
      "Evaluation/MaxReturn                 -311.899\n",
      "Evaluation/MinReturn                 -313.961\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    1.03116\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       31968\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.7577,  2.0108,  0.6745,  ...,  0.2675, -0.6323,  1.1242])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.7340,  2.0069,  0.6777,  ...,  0.2509, -0.6309,  1.1201])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(10.8989)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[77.6304, -6.8872],\n",
      "        [-6.8872,  0.5833]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0367, 0.4987])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0418, -0.0043, -0.0012,  ..., -0.0162, -0.0114,  0.0127])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(3.3064, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(3.3230, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:09 | [trpo_pendulum] epoch #16 | Saving snapshot...\n",
      "2022-09-02 00:55:09 | [trpo_pendulum] epoch #16 | Saved\n",
      "2022-09-02 00:55:09 | [trpo_pendulum] epoch #16 | Time 18.51 s\n",
      "2022-09-02 00:55:09 | [trpo_pendulum] epoch #16 | EpochTime 0.96 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -26.469\n",
      "Evaluation/AverageReturn             -314.446\n",
      "Evaluation/Iteration                   16\n",
      "Evaluation/MaxReturn                 -229.845\n",
      "Evaluation/MinReturn                 -399.047\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   84.6011\n",
      "Evaluation/TerminationRate              0.5\n",
      "TotalEnvSteps                       33788\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.7340,  2.0069,  0.6777,  ...,  0.2509, -0.6309,  1.1201])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6921,  2.0026,  0.6765,  ...,  0.2347, -0.6422,  1.1328])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(17.3938)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[27.4288, -3.0920],\n",
      "        [-3.0920,  0.5886]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.1202, -0.4853])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.1401, -0.0117, -0.0012,  ...,  0.0226,  0.0414, -0.0009])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-18.0398, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-18.9262, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:10 | [trpo_pendulum] epoch #17 | Saving snapshot...\n",
      "2022-09-02 00:55:10 | [trpo_pendulum] epoch #17 | Saved\n",
      "2022-09-02 00:55:10 | [trpo_pendulum] epoch #17 | Time 19.42 s\n",
      "2022-09-02 00:55:10 | [trpo_pendulum] epoch #17 | EpochTime 0.90 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -17.7436\n",
      "Evaluation/AverageReturn              -72.8507\n",
      "Evaluation/Iteration                   17\n",
      "Evaluation/MaxReturn                  -44.7867\n",
      "Evaluation/MinReturn                 -122.832\n",
      "Evaluation/NumEpisodes                  3\n",
      "Evaluation/StdReturn                   35.4314\n",
      "Evaluation/TerminationRate              1\n",
      "TotalEnvSteps                       35389\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6921,  2.0026,  0.6765,  ...,  0.2347, -0.6422,  1.1328])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.8323,  1.9909,  0.6753,  ...,  0.2573, -0.6008,  1.1319])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.6576)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-5.3369,  0.3786],\n",
      "        [ 0.3786,  0.0159]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0102, -0.4999])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0733,  0.0051,  0.0008,  ..., -0.0085, -0.0199,  0.0028])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-1.8049, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-1.9803, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:11 | [trpo_pendulum] epoch #18 | Saving snapshot...\n",
      "2022-09-02 00:55:11 | [trpo_pendulum] epoch #18 | Saved\n",
      "2022-09-02 00:55:11 | [trpo_pendulum] epoch #18 | Time 20.56 s\n",
      "2022-09-02 00:55:11 | [trpo_pendulum] epoch #18 | EpochTime 1.15 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -14.0744\n",
      "Evaluation/AverageReturn             -254.579\n",
      "Evaluation/Iteration                   18\n",
      "Evaluation/MaxReturn                 -167.921\n",
      "Evaluation/MinReturn                 -341.236\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   86.6579\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       37387\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.8323,  1.9909,  0.6753,  ...,  0.2573, -0.6008,  1.1319])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.7590,  1.9960,  0.6762,  ...,  0.2488, -0.6207,  1.1347])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(14.6915)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-110.9508,   -2.4141],\n",
      "        [  -2.4141,   -0.1774]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.3008, 0.3994])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.1391, -0.0276,  0.0072,  ..., -0.0080, -0.0778,  0.0841])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(12.6457, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(10.2522, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:12 | [trpo_pendulum] epoch #19 | Saving snapshot...\n",
      "2022-09-02 00:55:12 | [trpo_pendulum] epoch #19 | Saved\n",
      "2022-09-02 00:55:12 | [trpo_pendulum] epoch #19 | Time 21.65 s\n",
      "2022-09-02 00:55:12 | [trpo_pendulum] epoch #19 | EpochTime 1.09 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -20.9129\n",
      "Evaluation/AverageReturn             -366.438\n",
      "Evaluation/Iteration                   19\n",
      "Evaluation/MaxReturn                 -287.524\n",
      "Evaluation/MinReturn                 -445.351\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   78.9138\n",
      "Evaluation/TerminationRate              0.5\n",
      "TotalEnvSteps                       39291\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.7590,  1.9960,  0.6762,  ...,  0.2488, -0.6207,  1.1347])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.8981,  1.9684,  0.6834,  ...,  0.2408, -0.6985,  1.2188])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(10.2354)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-86.2088,  19.7905],\n",
      "        [ 19.7905, -13.5213]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.1767, 0.4677])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.2180, -0.0137, -0.0005,  ...,  0.0157, -0.0580,  0.0310])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(33.8493, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(24.2180, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:13 | [trpo_pendulum] epoch #20 | Saving snapshot...\n",
      "2022-09-02 00:55:13 | [trpo_pendulum] epoch #20 | Saved\n",
      "2022-09-02 00:55:13 | [trpo_pendulum] epoch #20 | Time 22.69 s\n",
      "2022-09-02 00:55:13 | [trpo_pendulum] epoch #20 | EpochTime 1.03 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -67.3551\n",
      "Evaluation/AverageReturn             -687.44\n",
      "Evaluation/Iteration                   20\n",
      "Evaluation/MaxReturn                 -656.649\n",
      "Evaluation/MinReturn                 -718.231\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   30.7911\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       41289\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.8981,  1.9684,  0.6834,  ...,  0.2408, -0.6985,  1.2188])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.1160,  1.9547,  0.6828,  ...,  0.2564, -0.7566,  1.2498])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(43.6740)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[458.2996,  13.9747],\n",
      "        [ 13.9747,  13.3926]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.1339, -0.4817])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0371,  0.0007,  0.0084,  ..., -0.0273,  0.0335, -0.0037])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(111.1738, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(103.0307, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:14 | [trpo_pendulum] epoch #21 | Saving snapshot...\n",
      "2022-09-02 00:55:14 | [trpo_pendulum] epoch #21 | Saved\n",
      "2022-09-02 00:55:14 | [trpo_pendulum] epoch #21 | Time 23.75 s\n",
      "2022-09-02 00:55:14 | [trpo_pendulum] epoch #21 | EpochTime 1.06 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -155.103\n",
      "Evaluation/AverageReturn            -1587.91\n",
      "Evaluation/Iteration                   21\n",
      "Evaluation/MaxReturn                -1584.47\n",
      "Evaluation/MinReturn                -1591.34\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    3.43304\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       43287\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-1.1160,  1.9547,  0.6828,  ...,  0.2564, -0.7566,  1.2498])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0789,  1.9554,  0.6913,  ...,  0.2291, -0.7231,  1.2460])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(24.1287)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2476.9495,    38.5301],\n",
      "        [   38.5301,     7.0053]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0098, -0.4999])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0023,  0.0011, -0.0041,  ...,  0.0177, -0.0673, -0.0013])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(107.4196, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(100.7663, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:16 | [trpo_pendulum] epoch #22 | Saving snapshot...\n",
      "2022-09-02 00:55:16 | [trpo_pendulum] epoch #22 | Saved\n",
      "2022-09-02 00:55:16 | [trpo_pendulum] epoch #22 | Time 24.85 s\n",
      "2022-09-02 00:55:16 | [trpo_pendulum] epoch #22 | EpochTime 1.09 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn   -159.003\n",
      "Evaluation/AverageReturn            -1613.31\n",
      "Evaluation/Iteration                   22\n",
      "Evaluation/MaxReturn                -1612.55\n",
      "Evaluation/MinReturn                -1614.07\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    0.758559\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       45285\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-1.0789,  1.9554,  0.6913,  ...,  0.2291, -0.7231,  1.2460])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0812,  1.9565,  0.6871,  ...,  0.2468, -0.7904,  1.2447])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(40.5865)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3198.7373,  -123.0879],\n",
      "        [ -123.0879,    -6.0145]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0261, -0.4993])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0565,  0.0001,  0.0082,  ..., -0.0049,  0.0219, -0.0128])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(63.7833, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(65.2825, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:17 | [trpo_pendulum] epoch #23 | Saving snapshot...\n",
      "2022-09-02 00:55:17 | [trpo_pendulum] epoch #23 | Saved\n",
      "2022-09-02 00:55:17 | [trpo_pendulum] epoch #23 | Time 25.90 s\n",
      "2022-09-02 00:55:17 | [trpo_pendulum] epoch #23 | EpochTime 1.05 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -114.306\n",
      "Evaluation/AverageReturn            -1166.72\n",
      "Evaluation/Iteration                   23\n",
      "Evaluation/MaxReturn                -1165.37\n",
      "Evaluation/MinReturn                -1168.07\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    1.34825\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       47283\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-1.0812,  1.9565,  0.6871,  ...,  0.2468, -0.7904,  1.2447])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0247,  1.9566,  0.6954,  ...,  0.2419, -0.7685,  1.2319])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(20.5149)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ -4.2241, -18.7705],\n",
      "        [-18.7705,   0.3740]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0026,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 2.4723e-02, -1.0573e-05,  4.1429e-03,  ..., -2.0352e-03,\n",
      "         1.9411e-02, -7.0288e-03])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(64.2661, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(64.0107, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:18 | [trpo_pendulum] epoch #24 | Saving snapshot...\n",
      "2022-09-02 00:55:18 | [trpo_pendulum] epoch #24 | Saved\n",
      "2022-09-02 00:55:18 | [trpo_pendulum] epoch #24 | Time 26.99 s\n",
      "2022-09-02 00:55:18 | [trpo_pendulum] epoch #24 | EpochTime 1.08 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -117.86\n",
      "Evaluation/AverageReturn            -1192.58\n",
      "Evaluation/Iteration                   24\n",
      "Evaluation/MaxReturn                -1190.46\n",
      "Evaluation/MinReturn                -1194.7\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    2.12174\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       49281\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-1.0247,  1.9566,  0.6954,  ...,  0.2419, -0.7685,  1.2319])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0000,  1.9566,  0.6995,  ...,  0.2399, -0.7491,  1.2249])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(10.3926)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[302.4540,   6.5368],\n",
      "        [  6.5368,   0.5024]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0478,  0.4977])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.1446, -0.0010,  0.0057,  ..., -0.0052,  0.0258, -0.0009])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(77.4311, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(76.8541, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:19 | [trpo_pendulum] epoch #25 | Saving snapshot...\n",
      "2022-09-02 00:55:19 | [trpo_pendulum] epoch #25 | Saved\n",
      "2022-09-02 00:55:19 | [trpo_pendulum] epoch #25 | Time 28.12 s\n",
      "2022-09-02 00:55:19 | [trpo_pendulum] epoch #25 | EpochTime 1.13 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -132.174\n",
      "Evaluation/AverageReturn            -1355.69\n",
      "Evaluation/Iteration                   25\n",
      "Evaluation/MaxReturn                -1354.02\n",
      "Evaluation/MinReturn                -1357.35\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    1.66682\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       51279\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-1.0000,  1.9566,  0.6995,  ...,  0.2399, -0.7491,  1.2249])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.1446,  1.9556,  0.7053,  ...,  0.2347, -0.7233,  1.2240])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(31.1928)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-80.8863,   8.4803],\n",
      "        [  8.4803,   2.2802]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0129, -0.4998])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0771,  0.0037, -0.0011,  ...,  0.0023, -0.0840,  0.0004])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(42.2779, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(41.4040, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:20 | [trpo_pendulum] epoch #26 | Saving snapshot...\n",
      "2022-09-02 00:55:20 | [trpo_pendulum] epoch #26 | Saved\n",
      "2022-09-02 00:55:20 | [trpo_pendulum] epoch #26 | Time 29.23 s\n",
      "2022-09-02 00:55:20 | [trpo_pendulum] epoch #26 | EpochTime 1.10 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn    -98.7983\n",
      "Evaluation/AverageReturn             -992.716\n",
      "Evaluation/Iteration                   26\n",
      "Evaluation/MaxReturn                 -989.689\n",
      "Evaluation/MinReturn                 -995.743\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    3.02679\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       53277\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-1.1446,  1.9556,  0.7053,  ...,  0.2347, -0.7233,  1.2240])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0675,  1.9593,  0.7041,  ...,  0.2369, -0.8073,  1.2244])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(8.0622)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-26.2385,   4.0102],\n",
      "        [  4.0102,   0.5684]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0700, -0.4951])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0216, -0.0002,  0.0006,  ...,  0.0180, -0.0314, -0.0215])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(49.8837, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(49.9436, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:21 | [trpo_pendulum] epoch #27 | Saving snapshot...\n",
      "2022-09-02 00:55:21 | [trpo_pendulum] epoch #27 | Saved\n",
      "2022-09-02 00:55:21 | [trpo_pendulum] epoch #27 | Time 30.33 s\n",
      "2022-09-02 00:55:21 | [trpo_pendulum] epoch #27 | EpochTime 1.10 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -109.832\n",
      "Evaluation/AverageReturn            -1091.61\n",
      "Evaluation/Iteration                   27\n",
      "Evaluation/MaxReturn                -1088.59\n",
      "Evaluation/MinReturn                -1094.63\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    3.02207\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       55275\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-1.0675,  1.9593,  0.7041,  ...,  0.2369, -0.8073,  1.2244])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0460,  1.9591,  0.7047,  ...,  0.2549, -0.8387,  1.2028])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(35.9612)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[214.4865,  -7.3370],\n",
      "        [ -7.3370,  -0.7374]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0075,  0.4999])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0121, -0.0005, -0.0004,  ...,  0.0081,  0.0037, -0.0093])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(58.2933, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(58.4772, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:22 | [trpo_pendulum] epoch #28 | Saving snapshot...\n",
      "2022-09-02 00:55:22 | [trpo_pendulum] epoch #28 | Saved\n",
      "2022-09-02 00:55:22 | [trpo_pendulum] epoch #28 | Time 31.33 s\n",
      "2022-09-02 00:55:22 | [trpo_pendulum] epoch #28 | EpochTime 0.99 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -121.671\n",
      "Evaluation/AverageReturn            -1202.11\n",
      "Evaluation/Iteration                   28\n",
      "Evaluation/MaxReturn                -1199.39\n",
      "Evaluation/MinReturn                -1204.83\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    2.72241\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       57273\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-1.0460,  1.9591,  0.7047,  ...,  0.2549, -0.8387,  1.2028])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0338,  1.9586,  0.7043,  ...,  0.2630, -0.8350,  1.1935])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(16.2881)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-312.8285,  -11.3681],\n",
      "        [ -11.3681,   -0.4508]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0242, -0.4994])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0159, -0.0004, -0.0021,  ..., -0.0007,  0.0839,  0.0028])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(60.6602, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(60.7937, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:23 | [trpo_pendulum] epoch #29 | Saving snapshot...\n",
      "2022-09-02 00:55:23 | [trpo_pendulum] epoch #29 | Saved\n",
      "2022-09-02 00:55:23 | [trpo_pendulum] epoch #29 | Time 32.37 s\n",
      "2022-09-02 00:55:23 | [trpo_pendulum] epoch #29 | EpochTime 1.04 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -120.456\n",
      "Evaluation/AverageReturn            -1242.38\n",
      "Evaluation/Iteration                   29\n",
      "Evaluation/MaxReturn                -1238.64\n",
      "Evaluation/MinReturn                -1246.12\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    3.74054\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       59271\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-1.0338,  1.9586,  0.7043,  ...,  0.2630, -0.8350,  1.1935])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0498,  1.9582,  0.7022,  ...,  0.2623, -0.7511,  1.1963])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.9857)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[19.1101,  0.1251],\n",
      "        [ 0.1251, -0.0799]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.1119, -0.4873])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0868, -0.0018, -0.0005,  ..., -0.0038,  0.3135,  0.0059])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(55.7274, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(55.1108, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:24 | [trpo_pendulum] epoch #30 | Saving snapshot...\n",
      "2022-09-02 00:55:24 | [trpo_pendulum] epoch #30 | Saved\n",
      "2022-09-02 00:55:24 | [trpo_pendulum] epoch #30 | Time 33.40 s\n",
      "2022-09-02 00:55:24 | [trpo_pendulum] epoch #30 | EpochTime 1.03 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -118.565\n",
      "Evaluation/AverageReturn            -1209.86\n",
      "Evaluation/Iteration                   30\n",
      "Evaluation/MaxReturn                -1207.65\n",
      "Evaluation/MinReturn                -1212.07\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    2.21467\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       61269\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-1.0498,  1.9582,  0.7022,  ...,  0.2623, -0.7511,  1.1963])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.9629,  1.9564,  0.7017,  ...,  0.2585, -0.4375,  1.2023])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(17.2592)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-491.3892,    9.3262],\n",
      "        [   9.3262,    0.8552]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0097, -0.4999])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0419,  0.0026,  0.0012,  ..., -0.0038, -0.1907,  0.0020])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(58.2426, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(58.1627, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:25 | [trpo_pendulum] epoch #31 | Saving snapshot...\n",
      "2022-09-02 00:55:25 | [trpo_pendulum] epoch #31 | Saved\n",
      "2022-09-02 00:55:25 | [trpo_pendulum] epoch #31 | Time 34.69 s\n",
      "2022-09-02 00:55:25 | [trpo_pendulum] epoch #31 | EpochTime 1.29 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn   -122.592\n",
      "Evaluation/AverageReturn            -1256.22\n",
      "Evaluation/Iteration                   31\n",
      "Evaluation/MaxReturn                -1255.67\n",
      "Evaluation/MinReturn                -1256.78\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    0.556846\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       63267\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.9629,  1.9564,  0.7017,  ...,  0.2585, -0.4375,  1.2023])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0048,  1.9590,  0.7029,  ...,  0.2547, -0.6282,  1.2043])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(9.5151)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 6.0913e+02, -7.8718e+00],\n",
      "        [-7.8718e+00,  3.1868e-01]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0132,  0.4998])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0233, -0.0005, -0.0011,  ...,  0.0006, -0.0502, -0.0008])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(48.2589, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(48.0740, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:26 | [trpo_pendulum] epoch #32 | Saving snapshot...\n",
      "2022-09-02 00:55:26 | [trpo_pendulum] epoch #32 | Saved\n",
      "2022-09-02 00:55:26 | [trpo_pendulum] epoch #32 | Time 35.75 s\n",
      "2022-09-02 00:55:26 | [trpo_pendulum] epoch #32 | EpochTime 1.05 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -114.371\n",
      "Evaluation/AverageReturn            -1166.38\n",
      "Evaluation/Iteration                   32\n",
      "Evaluation/MaxReturn                -1160.27\n",
      "Evaluation/MinReturn                -1172.49\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    6.10915\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       65265\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-1.0048,  1.9590,  0.7029,  ...,  0.2547, -0.6282,  1.2043])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0281,  1.9585,  0.7019,  ...,  0.2553, -0.6784,  1.2035])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(36.7035)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.2552e+02,  3.0416e+00],\n",
      "        [ 3.0416e+00,  7.5957e-02]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0077, -0.4999])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0172,  0.0003,  0.0014,  ..., -0.0013,  0.0089,  0.0008])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(49.5770, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(49.6616, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:27 | [trpo_pendulum] epoch #33 | Saving snapshot...\n",
      "2022-09-02 00:55:28 | [trpo_pendulum] epoch #33 | Saved\n",
      "2022-09-02 00:55:28 | [trpo_pendulum] epoch #33 | Time 36.82 s\n",
      "2022-09-02 00:55:28 | [trpo_pendulum] epoch #33 | EpochTime 1.07 s\n",
      "----------------------------------  ---------\n",
      "Evaluation/AverageDiscountedReturn   -119.567\n",
      "Evaluation/AverageReturn            -1199.99\n",
      "Evaluation/Iteration                   33\n",
      "Evaluation/MaxReturn                -1189.7\n",
      "Evaluation/MinReturn                -1210.29\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   10.293\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       67263\n",
      "----------------------------------  ---------\n",
      "old policy para is\n",
      "tensor([-1.0281,  1.9585,  0.7019,  ...,  0.2553, -0.6784,  1.2035])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0109,  1.9589,  0.7033,  ...,  0.2540, -0.6695,  1.2043])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(16.0900)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.3415e+02,  6.6107e+00],\n",
      "        [ 6.6107e+00, -1.2432e-01]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0091, -0.4999])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0042, -0.0009, -0.0010,  ...,  0.0016,  0.0181, -0.0009])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(45.1556, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(45.1063, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:29 | [trpo_pendulum] epoch #34 | Saving snapshot...\n",
      "2022-09-02 00:55:29 | [trpo_pendulum] epoch #34 | Saved\n",
      "2022-09-02 00:55:29 | [trpo_pendulum] epoch #34 | Time 37.87 s\n",
      "2022-09-02 00:55:29 | [trpo_pendulum] epoch #34 | EpochTime 1.04 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -115.881\n",
      "Evaluation/AverageReturn            -1168.41\n",
      "Evaluation/Iteration                   34\n",
      "Evaluation/MaxReturn                -1156.83\n",
      "Evaluation/MinReturn                -1179.98\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   11.5724\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       69261\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-1.0109,  1.9589,  0.7033,  ...,  0.2540, -0.6695,  1.2043])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0151,  1.9580,  0.7023,  ...,  0.2556, -0.6513,  1.2034])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(8.9350)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[4.4622, 1.6892],\n",
      "        [1.6892, 0.0442]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0105, -0.4999])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0149,  0.0004, -0.0003,  ...,  0.0009,  0.0086, -0.0011])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(54.4308, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(54.5607, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:30 | [trpo_pendulum] epoch #35 | Saving snapshot...\n",
      "2022-09-02 00:55:30 | [trpo_pendulum] epoch #35 | Saved\n",
      "2022-09-02 00:55:30 | [trpo_pendulum] epoch #35 | Time 39.00 s\n",
      "2022-09-02 00:55:30 | [trpo_pendulum] epoch #35 | EpochTime 1.13 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -124.273\n",
      "Evaluation/AverageReturn            -1284.55\n",
      "Evaluation/Iteration                   35\n",
      "Evaluation/MaxReturn                -1282.83\n",
      "Evaluation/MinReturn                -1286.28\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    1.72569\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       71259\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-1.0151,  1.9580,  0.7023,  ...,  0.2556, -0.6513,  1.2034])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0001,  1.9584,  0.7020,  ...,  0.2565, -0.6427,  1.2023])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.8598)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[7.3570e+01, 2.3264e-01],\n",
      "        [2.3264e-01, 1.2136e-02]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0245,  0.4994])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0068,  0.0015,  0.0038,  ..., -0.0026,  0.0241, -0.0007])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(37.7120, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(37.6983, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:31 | [trpo_pendulum] epoch #36 | Saving snapshot...\n",
      "2022-09-02 00:55:31 | [trpo_pendulum] epoch #36 | Saved\n",
      "2022-09-02 00:55:31 | [trpo_pendulum] epoch #36 | Time 39.97 s\n",
      "2022-09-02 00:55:31 | [trpo_pendulum] epoch #36 | EpochTime 0.97 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -112.561\n",
      "Evaluation/AverageReturn            -1124.53\n",
      "Evaluation/Iteration                   36\n",
      "Evaluation/MaxReturn                -1113\n",
      "Evaluation/MinReturn                -1136.07\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   11.5392\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       73257\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-1.0001,  1.9584,  0.7020,  ...,  0.2565, -0.6427,  1.2023])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-1.0069,  1.9599,  0.7058,  ...,  0.2539, -0.6186,  1.2017])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(12.6555)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[54.4126,  2.4750],\n",
      "        [ 2.4750,  0.7756]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.1729, -0.4692])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0528,  0.0072, -0.0086,  ...,  0.0371, -0.0760, -0.0399])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(38.8262, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(36.4389, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:32 | [trpo_pendulum] epoch #37 | Saving snapshot...\n",
      "2022-09-02 00:55:32 | [trpo_pendulum] epoch #37 | Saved\n",
      "2022-09-02 00:55:32 | [trpo_pendulum] epoch #37 | Time 40.96 s\n",
      "2022-09-02 00:55:32 | [trpo_pendulum] epoch #37 | EpochTime 0.98 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -112.718\n",
      "Evaluation/AverageReturn            -1149.13\n",
      "Evaluation/Iteration                   37\n",
      "Evaluation/MaxReturn                -1145.23\n",
      "Evaluation/MinReturn                -1153.03\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    3.89917\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       75255\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-1.0069,  1.9599,  0.7058,  ...,  0.2539, -0.6186,  1.2017])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.9541,  1.9671,  0.6972,  ...,  0.2910, -0.6946,  1.1618])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.6896)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-5.7111, -0.2242],\n",
      "        [-0.2242, 31.0745]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.5000, -0.0021])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.3454, -0.0087,  0.0259,  ..., -0.0399, -0.8364,  0.0376])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-24.5911, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-23.0219, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:33 | [trpo_pendulum] epoch #38 | Saving snapshot...\n",
      "2022-09-02 00:55:33 | [trpo_pendulum] epoch #38 | Saved\n",
      "2022-09-02 00:55:33 | [trpo_pendulum] epoch #38 | Time 42.07 s\n",
      "2022-09-02 00:55:33 | [trpo_pendulum] epoch #38 | EpochTime 1.11 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -50.33\n",
      "Evaluation/AverageReturn             -468.289\n",
      "Evaluation/Iteration                   38\n",
      "Evaluation/MaxReturn                 -458.08\n",
      "Evaluation/MinReturn                 -478.499\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   10.2095\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       77253\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.9541,  1.9671,  0.6972,  ...,  0.2910, -0.6946,  1.1618])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6088,  1.9584,  0.7231,  ...,  0.2511, -1.5309,  1.1994])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(9.5171)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[231.9487,  39.1747],\n",
      "        [ 39.1747,  -3.5063]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0359, -0.4987])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.1894,  0.0168,  0.0034,  ...,  0.0199,  0.3556, -0.0244])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(37.6017, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(40.9079, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:34 | [trpo_pendulum] epoch #39 | Saving snapshot...\n",
      "2022-09-02 00:55:34 | [trpo_pendulum] epoch #39 | Saved\n",
      "2022-09-02 00:55:34 | [trpo_pendulum] epoch #39 | Time 43.07 s\n",
      "2022-09-02 00:55:34 | [trpo_pendulum] epoch #39 | EpochTime 1.00 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -94.2677\n",
      "Evaluation/AverageReturn            -1123.8\n",
      "Evaluation/Iteration                   39\n",
      "Evaluation/MaxReturn                 -931.654\n",
      "Evaluation/MinReturn                -1315.95\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  192.146\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       79251\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6088,  1.9584,  0.7231,  ...,  0.2511, -1.5309,  1.1994])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.7982,  1.9752,  0.7265,  ...,  0.2710, -1.1753,  1.1750])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(9.6587)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[36.8982,  6.6415],\n",
      "        [ 6.6415, -0.2446]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0461, -0.4979])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.1090, -0.0058, -0.0005,  ..., -0.0092, -0.2534,  0.0098])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(19.4307, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(19.8958, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:35 | [trpo_pendulum] epoch #40 | Saving snapshot...\n",
      "2022-09-02 00:55:35 | [trpo_pendulum] epoch #40 | Saved\n",
      "2022-09-02 00:55:35 | [trpo_pendulum] epoch #40 | Time 44.17 s\n",
      "2022-09-02 00:55:35 | [trpo_pendulum] epoch #40 | EpochTime 1.10 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -91.3778\n",
      "Evaluation/AverageReturn             -954.769\n",
      "Evaluation/Iteration                   40\n",
      "Evaluation/MaxReturn                 -943.291\n",
      "Evaluation/MinReturn                 -966.247\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   11.4777\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       81249\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.7982,  1.9752,  0.7265,  ...,  0.2710, -1.1753,  1.1750])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6892,  1.9694,  0.7260,  ...,  0.2618, -1.4288,  1.1848])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(17.0136)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[762.2124, -20.2305],\n",
      "        [-20.2305,  -0.9593]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([3.5665e-04, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.4374e-02, -2.8847e-03, -8.4142e-05,  ..., -4.7129e-03,\n",
      "        -1.2779e-01,  5.0042e-03])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-29.1293, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-29.0883, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:36 | [trpo_pendulum] epoch #41 | Saving snapshot...\n",
      "2022-09-02 00:55:36 | [trpo_pendulum] epoch #41 | Saved\n",
      "2022-09-02 00:55:36 | [trpo_pendulum] epoch #41 | Time 45.26 s\n",
      "2022-09-02 00:55:36 | [trpo_pendulum] epoch #41 | EpochTime 1.08 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -39.8037\n",
      "Evaluation/AverageReturn             -427.499\n",
      "Evaluation/Iteration                   41\n",
      "Evaluation/MaxReturn                 -407.871\n",
      "Evaluation/MinReturn                 -447.128\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   19.6288\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       83247\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6892,  1.9694,  0.7260,  ...,  0.2618, -1.4288,  1.1848])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6349,  1.9665,  0.7259,  ...,  0.2571, -1.5566,  1.1898])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(8.3082)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.2314e+01,  1.3298e+00],\n",
      "        [ 1.3298e+00,  1.5546e-02]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0105, -0.4999])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0304,  0.0015, -0.0017,  ...,  0.0031,  0.0729, -0.0026])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-4.4970, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-4.6687, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:37 | [trpo_pendulum] epoch #42 | Saving snapshot...\n",
      "2022-09-02 00:55:37 | [trpo_pendulum] epoch #42 | Saved\n",
      "2022-09-02 00:55:37 | [trpo_pendulum] epoch #42 | Time 46.30 s\n",
      "2022-09-02 00:55:37 | [trpo_pendulum] epoch #42 | EpochTime 1.05 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -38.1166\n",
      "Evaluation/AverageReturn             -658.351\n",
      "Evaluation/Iteration                   42\n",
      "Evaluation/MaxReturn                 -378.18\n",
      "Evaluation/MinReturn                 -938.522\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  280.171\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       85245\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6349,  1.9665,  0.7259,  ...,  0.2571, -1.5566,  1.1898])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6653,  1.9680,  0.7242,  ...,  0.2603, -1.4836,  1.1872])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(8.5340)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.6801, -0.2615],\n",
      "        [-0.2615,  0.0343]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0665, 0.4956])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0170, -0.0024, -0.0008,  ..., -0.0032,  0.1018,  0.0063])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-4.7243, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-4.3958, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:38 | [trpo_pendulum] epoch #43 | Saving snapshot...\n",
      "2022-09-02 00:55:38 | [trpo_pendulum] epoch #43 | Saved\n",
      "2022-09-02 00:55:38 | [trpo_pendulum] epoch #43 | Time 47.30 s\n",
      "2022-09-02 00:55:38 | [trpo_pendulum] epoch #43 | EpochTime 1.00 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -36.0039\n",
      "Evaluation/AverageReturn             -651.246\n",
      "Evaluation/Iteration                   43\n",
      "Evaluation/MaxReturn                 -524.027\n",
      "Evaluation/MinReturn                 -778.465\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  127.219\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       87243\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6653,  1.9680,  0.7242,  ...,  0.2603, -1.4836,  1.1872])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6483,  1.9655,  0.7234,  ...,  0.2571, -1.3819,  1.1934])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.9327)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 0.2783, -0.3550],\n",
      "        [-0.3550, -0.0460]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0401,  0.4984])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0002, -0.0013,  0.0010,  ..., -0.0017,  0.0437,  0.0037])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-10.3945, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-10.4124, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:39 | [trpo_pendulum] epoch #44 | Saving snapshot...\n",
      "2022-09-02 00:55:39 | [trpo_pendulum] epoch #44 | Saved\n",
      "2022-09-02 00:55:39 | [trpo_pendulum] epoch #44 | Time 48.35 s\n",
      "2022-09-02 00:55:39 | [trpo_pendulum] epoch #44 | EpochTime 1.05 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -45.9673\n",
      "Evaluation/AverageReturn             -601.758\n",
      "Evaluation/Iteration                   44\n",
      "Evaluation/MaxReturn                 -544.78\n",
      "Evaluation/MinReturn                 -658.737\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   56.9785\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       89241\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6483,  1.9655,  0.7234,  ...,  0.2571, -1.3819,  1.1934])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6481,  1.9642,  0.7244,  ...,  0.2553, -1.3381,  1.1972])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.1727)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 3.3602e+00,  3.3805e-01],\n",
      "        [ 3.3805e-01, -1.3918e-03]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0027, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0011,  0.0008, -0.0002,  ...,  0.0007, -0.0235, -0.0018])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(2.4344, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(2.4290, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:40 | [trpo_pendulum] epoch #45 | Saving snapshot...\n",
      "2022-09-02 00:55:40 | [trpo_pendulum] epoch #45 | Saved\n",
      "2022-09-02 00:55:40 | [trpo_pendulum] epoch #45 | Time 49.53 s\n",
      "2022-09-02 00:55:40 | [trpo_pendulum] epoch #45 | EpochTime 1.17 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -53.8985\n",
      "Evaluation/AverageReturn             -730.943\n",
      "Evaluation/Iteration                   45\n",
      "Evaluation/MaxReturn                 -691.832\n",
      "Evaluation/MinReturn                 -770.054\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   39.1112\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       91239\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6481,  1.9642,  0.7244,  ...,  0.2553, -1.3381,  1.1972])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6492,  1.9649,  0.7242,  ...,  0.2560, -1.3617,  1.1954])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.3282)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 3.3554e+01, -1.8189e-01],\n",
      "        [-1.8189e-01, -1.0303e-03]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0022,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0014,  0.0005, -0.0005,  ...,  0.0008, -0.0078, -0.0014])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-19.1329, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-19.1380, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:41 | [trpo_pendulum] epoch #46 | Saving snapshot...\n",
      "2022-09-02 00:55:41 | [trpo_pendulum] epoch #46 | Saved\n",
      "2022-09-02 00:55:41 | [trpo_pendulum] epoch #46 | Time 50.73 s\n",
      "2022-09-02 00:55:41 | [trpo_pendulum] epoch #46 | EpochTime 1.20 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -48.0167\n",
      "Evaluation/AverageReturn             -511.302\n",
      "Evaluation/Iteration                   46\n",
      "Evaluation/MaxReturn                 -472.806\n",
      "Evaluation/MinReturn                 -549.799\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   38.4962\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       93237\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6492,  1.9649,  0.7242,  ...,  0.2560, -1.3617,  1.1954])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6478,  1.9654,  0.7237,  ...,  0.2568, -1.3695,  1.1940])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.3526)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-6.0427e+01,  4.4576e-01],\n",
      "        [ 4.4576e-01, -1.0422e-02]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0201, 0.4996])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0247,  0.0021, -0.0013,  ...,  0.0005,  0.0115,  0.0021])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(38.2620, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(38.5502, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:42 | [trpo_pendulum] epoch #47 | Saving snapshot...\n",
      "2022-09-02 00:55:42 | [trpo_pendulum] epoch #47 | Saved\n",
      "2022-09-02 00:55:42 | [trpo_pendulum] epoch #47 | Time 51.80 s\n",
      "2022-09-02 00:55:42 | [trpo_pendulum] epoch #47 | EpochTime 1.07 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -93.0553\n",
      "Evaluation/AverageReturn            -1113.43\n",
      "Evaluation/Iteration                   47\n",
      "Evaluation/MaxReturn                -1093.3\n",
      "Evaluation/MinReturn                -1133.57\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   20.1335\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       95235\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6478,  1.9654,  0.7237,  ...,  0.2568, -1.3695,  1.1940])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6725,  1.9676,  0.7224,  ...,  0.2573, -1.3580,  1.1961])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.8012)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 7.6998e+01, -8.8056e-01],\n",
      "        [-8.8056e-01,  5.7957e-02]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0413, -0.4983])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0269, -0.0048, -0.0116,  ...,  0.0005, -0.1153, -0.0019])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(19.4973, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(19.2727, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:44 | [trpo_pendulum] epoch #48 | Saving snapshot...\n",
      "2022-09-02 00:55:44 | [trpo_pendulum] epoch #48 | Saved\n",
      "2022-09-02 00:55:44 | [trpo_pendulum] epoch #48 | Time 52.95 s\n",
      "2022-09-02 00:55:44 | [trpo_pendulum] epoch #48 | EpochTime 1.15 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -63.2085\n",
      "Evaluation/AverageReturn             -925.148\n",
      "Evaluation/Iteration                   48\n",
      "Evaluation/MaxReturn                 -833.344\n",
      "Evaluation/MinReturn                -1016.95\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   91.8048\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       97233\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6725,  1.9676,  0.7224,  ...,  0.2573, -1.3580,  1.1961])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6994,  1.9627,  0.7108,  ...,  0.2578, -1.4733,  1.1942])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.8905)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-14.3372,  -0.6133],\n",
      "        [ -0.6133,  -0.0298]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0239, -0.4994])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0184,  0.0028,  0.0011,  ..., -0.0020,  0.0148,  0.0047])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-18.6231, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-18.5724, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:45 | [trpo_pendulum] epoch #49 | Saving snapshot...\n",
      "2022-09-02 00:55:45 | [trpo_pendulum] epoch #49 | Saved\n",
      "2022-09-02 00:55:45 | [trpo_pendulum] epoch #49 | Time 54.03 s\n",
      "2022-09-02 00:55:45 | [trpo_pendulum] epoch #49 | EpochTime 1.08 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -50.6773\n",
      "Evaluation/AverageReturn             -546.25\n",
      "Evaluation/Iteration                   49\n",
      "Evaluation/MaxReturn                 -485.542\n",
      "Evaluation/MinReturn                 -606.959\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   60.7085\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       99231\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6994,  1.9627,  0.7108,  ...,  0.2578, -1.4733,  1.1942])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6810,  1.9655,  0.7119,  ...,  0.2559, -1.4585,  1.1989])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.1981)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 4.3725, -0.5433],\n",
      "        [-0.5433, -0.0065]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0119,  0.4999])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0023,  0.0013,  0.0003,  ..., -0.0005,  0.0184,  0.0017])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-11.7429, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-11.7697, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:46 | [trpo_pendulum] epoch #50 | Saving snapshot...\n",
      "2022-09-02 00:55:46 | [trpo_pendulum] epoch #50 | Saved\n",
      "2022-09-02 00:55:46 | [trpo_pendulum] epoch #50 | Time 55.08 s\n",
      "2022-09-02 00:55:46 | [trpo_pendulum] epoch #50 | EpochTime 1.04 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -43.6034\n",
      "Evaluation/AverageReturn              -594.596\n",
      "Evaluation/Iteration                    50\n",
      "Evaluation/MaxReturn                  -525.296\n",
      "Evaluation/MinReturn                  -663.896\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    69.3001\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       101229\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6810,  1.9655,  0.7119,  ...,  0.2559, -1.4585,  1.1989])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6787,  1.9668,  0.7123,  ...,  0.2554, -1.4401,  1.2006])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.4846)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.2850e+02,  3.7832e+00],\n",
      "        [ 3.7832e+00,  1.4975e-02]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0017, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0025, -0.0009, -0.0010,  ..., -0.0001, -0.0077, -0.0016])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(17.5721, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(17.5955, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:47 | [trpo_pendulum] epoch #51 | Saving snapshot...\n",
      "2022-09-02 00:55:47 | [trpo_pendulum] epoch #51 | Saved\n",
      "2022-09-02 00:55:47 | [trpo_pendulum] epoch #51 | Time 56.01 s\n",
      "2022-09-02 00:55:47 | [trpo_pendulum] epoch #51 | EpochTime 0.93 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -74.8474\n",
      "Evaluation/AverageReturn              -860.507\n",
      "Evaluation/Iteration                    51\n",
      "Evaluation/MaxReturn                  -779.792\n",
      "Evaluation/MinReturn                  -941.222\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    80.7152\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       103113\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6787,  1.9668,  0.7123,  ...,  0.2554, -1.4401,  1.2006])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6812,  1.9658,  0.7113,  ...,  0.2553, -1.4478,  1.1990])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.5077)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 8.7511e+01, -7.0702e-02],\n",
      "        [-7.0702e-02, -6.8057e-04]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0008,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0017, -0.0005, -0.0004,  ..., -0.0002, -0.0055, -0.0006])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(3.1979, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(3.2026, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:48 | [trpo_pendulum] epoch #52 | Saving snapshot...\n",
      "2022-09-02 00:55:48 | [trpo_pendulum] epoch #52 | Saved\n",
      "2022-09-02 00:55:48 | [trpo_pendulum] epoch #52 | Time 57.04 s\n",
      "2022-09-02 00:55:48 | [trpo_pendulum] epoch #52 | EpochTime 1.03 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn     -60.95\n",
      "Evaluation/AverageReturn              -765.206\n",
      "Evaluation/Iteration                    52\n",
      "Evaluation/MaxReturn                  -602\n",
      "Evaluation/MinReturn                  -928.412\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   163.206\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       105111\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6812,  1.9658,  0.7113,  ...,  0.2553, -1.4478,  1.1990])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6828,  1.9653,  0.7108,  ...,  0.2551, -1.4533,  1.1983])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(9.4263)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[1.8211e+01, 7.2756e-03],\n",
      "        [7.2756e-03, 3.0702e-04]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0021,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-6.3505e-04, -2.0792e-04, -4.5755e-04,  ..., -3.3329e-06,\n",
      "        -1.1180e-03, -2.7014e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-26.7646, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-26.7753, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:49 | [trpo_pendulum] epoch #53 | Saving snapshot...\n",
      "2022-09-02 00:55:49 | [trpo_pendulum] epoch #53 | Saved\n",
      "2022-09-02 00:55:49 | [trpo_pendulum] epoch #53 | Time 58.06 s\n",
      "2022-09-02 00:55:49 | [trpo_pendulum] epoch #53 | EpochTime 1.01 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -37.3698\n",
      "Evaluation/AverageReturn              -445.688\n",
      "Evaluation/Iteration                    53\n",
      "Evaluation/MaxReturn                  -365.974\n",
      "Evaluation/MinReturn                  -525.402\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    79.7143\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       107109\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6828,  1.9653,  0.7108,  ...,  0.2551, -1.4533,  1.1983])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6835,  1.9651,  0.7104,  ...,  0.2551, -1.4545,  1.1981])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.7310)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.2847e+01, -5.4986e-02],\n",
      "        [-5.4986e-02,  5.8103e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0008, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-3.7633e-04, -1.3449e-04, -2.3920e-04,  ..., -4.9930e-05,\n",
      "        -7.7775e-04, -5.6118e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-17.2606, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-17.2591, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:50 | [trpo_pendulum] epoch #54 | Saving snapshot...\n",
      "2022-09-02 00:55:50 | [trpo_pendulum] epoch #54 | Saved\n",
      "2022-09-02 00:55:50 | [trpo_pendulum] epoch #54 | Time 59.18 s\n",
      "2022-09-02 00:55:50 | [trpo_pendulum] epoch #54 | EpochTime 1.13 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -39.8599\n",
      "Evaluation/AverageReturn              -519.696\n",
      "Evaluation/Iteration                    54\n",
      "Evaluation/MaxReturn                  -490.76\n",
      "Evaluation/MinReturn                  -548.633\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    28.9366\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       109107\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6835,  1.9651,  0.7104,  ...,  0.2551, -1.4545,  1.1981])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6838,  1.9650,  0.7101,  ...,  0.2550, -1.4552,  1.1980])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(9.5511)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-4.7009e+01, -1.0133e-01],\n",
      "        [-1.0133e-01, -1.2649e-04]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0008,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-4.3568e-05, -5.3784e-05,  9.7321e-05,  ...,  2.2915e-05,\n",
      "         4.4957e-04, -1.7198e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-10.1097, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-10.1133, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:51 | [trpo_pendulum] epoch #55 | Saving snapshot...\n",
      "2022-09-02 00:55:51 | [trpo_pendulum] epoch #55 | Saved\n",
      "2022-09-02 00:55:51 | [trpo_pendulum] epoch #55 | Time 60.40 s\n",
      "2022-09-02 00:55:51 | [trpo_pendulum] epoch #55 | EpochTime 1.21 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -43.5047\n",
      "Evaluation/AverageReturn              -578.823\n",
      "Evaluation/Iteration                    55\n",
      "Evaluation/MaxReturn                  -537.947\n",
      "Evaluation/MinReturn                  -619.699\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    40.8758\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       111105\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6838,  1.9650,  0.7101,  ...,  0.2550, -1.4552,  1.1980])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6839,  1.9650,  0.7102,  ...,  0.2551, -1.4548,  1.1978])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(15.1891)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.3644e+01, -3.1343e-02],\n",
      "        [-3.1343e-02, -4.7119e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0007,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 3.6386e-05, -3.9429e-05, -1.1245e-05,  ...,  1.5144e-04,\n",
      "         6.3673e-04, -1.4401e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(28.7023, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(28.6940, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:52 | [trpo_pendulum] epoch #56 | Saving snapshot...\n",
      "2022-09-02 00:55:52 | [trpo_pendulum] epoch #56 | Saved\n",
      "2022-09-02 00:55:52 | [trpo_pendulum] epoch #56 | Time 61.39 s\n",
      "2022-09-02 00:55:52 | [trpo_pendulum] epoch #56 | EpochTime 0.99 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -95.7179\n",
      "Evaluation/AverageReturn             -1007.92\n",
      "Evaluation/Iteration                    56\n",
      "Evaluation/MaxReturn                  -776.012\n",
      "Evaluation/MinReturn                 -1239.84\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   231.912\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       113103\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6839,  1.9650,  0.7102,  ...,  0.2551, -1.4548,  1.1978])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6838,  1.9649,  0.7102,  ...,  0.2552, -1.4541,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.7477)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 3.2001e+00,  3.6834e-03],\n",
      "        [ 3.6834e-03, -3.0027e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-3.7012e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.1872e-06,  2.2085e-05, -1.7968e-06,  ..., -7.5109e-05,\n",
      "        -3.2732e-04,  7.2989e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-8.5431, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-8.5425, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:53 | [trpo_pendulum] epoch #57 | Saving snapshot...\n",
      "2022-09-02 00:55:53 | [trpo_pendulum] epoch #57 | Saved\n",
      "2022-09-02 00:55:53 | [trpo_pendulum] epoch #57 | Time 62.44 s\n",
      "2022-09-02 00:55:53 | [trpo_pendulum] epoch #57 | EpochTime 1.04 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -53.6458\n",
      "Evaluation/AverageReturn              -620.412\n",
      "Evaluation/Iteration                    57\n",
      "Evaluation/MaxReturn                  -526.032\n",
      "Evaluation/MinReturn                  -714.792\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    94.3802\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       115101\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6838,  1.9649,  0.7102,  ...,  0.2552, -1.4541,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6838,  1.9649,  0.7102,  ...,  0.2551, -1.4545,  1.1978])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(18.3776)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 3.2577e+02, -9.6526e-02],\n",
      "        [-9.6526e-02, -3.9658e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.6501e-04,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.9931e-04,  2.2450e-05,  8.1882e-05,  ..., -1.1611e-04,\n",
      "        -3.6624e-04,  8.9811e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(37.1314, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(37.1251, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:54 | [trpo_pendulum] epoch #58 | Saving snapshot...\n",
      "2022-09-02 00:55:54 | [trpo_pendulum] epoch #58 | Saved\n",
      "2022-09-02 00:55:54 | [trpo_pendulum] epoch #58 | Time 63.61 s\n",
      "2022-09-02 00:55:54 | [trpo_pendulum] epoch #58 | EpochTime 1.17 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -92.0649\n",
      "Evaluation/AverageReturn             -1107.75\n",
      "Evaluation/Iteration                    58\n",
      "Evaluation/MaxReturn                 -1074.25\n",
      "Evaluation/MinReturn                 -1141.24\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    33.4962\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       117099\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6838,  1.9649,  0.7102,  ...,  0.2551, -1.4545,  1.1978])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6836,  1.9650,  0.7103,  ...,  0.2550, -1.4548,  1.1979])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.6992)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.9999e+02, -7.5736e-03],\n",
      "        [-7.5736e-03, -1.0220e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.7501e-04, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 4.0095e-05,  5.4748e-06,  1.2528e-04,  ..., -6.5806e-05,\n",
      "        -3.4653e-04,  1.5350e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-31.7844, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-31.7834, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:55 | [trpo_pendulum] epoch #59 | Saving snapshot...\n",
      "2022-09-02 00:55:55 | [trpo_pendulum] epoch #59 | Saved\n",
      "2022-09-02 00:55:55 | [trpo_pendulum] epoch #59 | Time 64.61 s\n",
      "2022-09-02 00:55:55 | [trpo_pendulum] epoch #59 | EpochTime 0.99 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -41.4181\n",
      "Evaluation/AverageReturn              -399.224\n",
      "Evaluation/Iteration                    59\n",
      "Evaluation/MaxReturn                  -386.799\n",
      "Evaluation/MinReturn                  -411.649\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    12.4252\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       119097\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6836,  1.9650,  0.7103,  ...,  0.2550, -1.4548,  1.1979])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6836,  1.9650,  0.7104,  ...,  0.2550, -1.4552,  1.1979])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.3824)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.6101e+01,  2.0425e-03],\n",
      "        [ 2.0425e-03, -2.7452e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0043,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 2.2454e-04, -1.9654e-04, -9.9327e-05,  ...,  3.8207e-04,\n",
      "         5.3877e-03, -2.8499e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-6.6993, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-6.7206, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:56 | [trpo_pendulum] epoch #60 | Saving snapshot...\n",
      "2022-09-02 00:55:56 | [trpo_pendulum] epoch #60 | Saved\n",
      "2022-09-02 00:55:56 | [trpo_pendulum] epoch #60 | Time 65.72 s\n",
      "2022-09-02 00:55:56 | [trpo_pendulum] epoch #60 | EpochTime 1.11 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -43.7977\n",
      "Evaluation/AverageReturn              -630.289\n",
      "Evaluation/Iteration                    60\n",
      "Evaluation/MaxReturn                  -454.025\n",
      "Evaluation/MinReturn                  -806.553\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   176.264\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       121095\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6836,  1.9650,  0.7104,  ...,  0.2550, -1.4552,  1.1979])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6834,  1.9648,  0.7103,  ...,  0.2553, -1.4498,  1.1976])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(11.3165)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 5.2917e+01, -7.9170e-02],\n",
      "        [-7.9170e-02,  2.1851e-04]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.2453e-04,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.4975e-04, -9.2520e-05, -6.7591e-05,  ...,  2.0857e-04,\n",
      "         2.8403e-03, -1.5613e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-2.0625, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-2.0675, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:57 | [trpo_pendulum] epoch #61 | Saving snapshot...\n",
      "2022-09-02 00:55:57 | [trpo_pendulum] epoch #61 | Saved\n",
      "2022-09-02 00:55:57 | [trpo_pendulum] epoch #61 | Time 66.80 s\n",
      "2022-09-02 00:55:57 | [trpo_pendulum] epoch #61 | EpochTime 1.08 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn     -53.935\n",
      "Evaluation/AverageReturn              -681.232\n",
      "Evaluation/Iteration                    61\n",
      "Evaluation/MaxReturn                  -509.818\n",
      "Evaluation/MinReturn                  -852.646\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   171.414\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       123093\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6834,  1.9648,  0.7103,  ...,  0.2553, -1.4498,  1.1976])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6832,  1.9647,  0.7103,  ...,  0.2556, -1.4470,  1.1974])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.9394)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[1.0237e+00, 1.4658e-02],\n",
      "        [1.4658e-02, 2.7460e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0013, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.4273e-04,  1.6985e-05,  9.1654e-05,  ...,  5.9767e-05,\n",
      "         2.7687e-03, -1.5638e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-20.1164, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-20.1134, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:55:59 | [trpo_pendulum] epoch #62 | Saving snapshot...\n",
      "2022-09-02 00:55:59 | [trpo_pendulum] epoch #62 | Saved\n",
      "2022-09-02 00:55:59 | [trpo_pendulum] epoch #62 | Time 67.87 s\n",
      "2022-09-02 00:55:59 | [trpo_pendulum] epoch #62 | EpochTime 1.07 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -42.9926\n",
      "Evaluation/AverageReturn              -488.238\n",
      "Evaluation/Iteration                    62\n",
      "Evaluation/MaxReturn                  -434.672\n",
      "Evaluation/MinReturn                  -541.805\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    53.5668\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       125091\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6832,  1.9647,  0.7103,  ...,  0.2556, -1.4470,  1.1974])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6834,  1.9647,  0.7104,  ...,  0.2556, -1.4442,  1.1973])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(1.7016)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.3741e+00, -4.1305e-03],\n",
      "        [-4.1305e-03,  3.7592e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0026, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0008,  0.0001,  0.0004,  ..., -0.0005, -0.0072,  0.0004])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-23.4321, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-23.4298, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:00 | [trpo_pendulum] epoch #63 | Saving snapshot...\n",
      "2022-09-02 00:56:00 | [trpo_pendulum] epoch #63 | Saved\n",
      "2022-09-02 00:56:00 | [trpo_pendulum] epoch #63 | Time 68.90 s\n",
      "2022-09-02 00:56:00 | [trpo_pendulum] epoch #63 | EpochTime 1.03 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -39.0494\n",
      "Evaluation/AverageReturn              -433.656\n",
      "Evaluation/Iteration                    63\n",
      "Evaluation/MaxReturn                  -409.869\n",
      "Evaluation/MinReturn                  -457.444\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    23.7878\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       127089\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6834,  1.9647,  0.7104,  ...,  0.2556, -1.4442,  1.1973])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6825,  1.9648,  0.7108,  ...,  0.2552, -1.4514,  1.1976])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.0111)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.3157e+00, -9.3738e-03],\n",
      "        [-9.3738e-03, -7.8163e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-3.0130e-04,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.1278e-04,  7.1731e-05,  1.6583e-04,  ..., -2.3752e-04,\n",
      "        -3.7413e-03,  2.0195e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-18.9617, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-18.9620, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:01 | [trpo_pendulum] epoch #64 | Saving snapshot...\n",
      "2022-09-02 00:56:01 | [trpo_pendulum] epoch #64 | Saved\n",
      "2022-09-02 00:56:01 | [trpo_pendulum] epoch #64 | Time 69.91 s\n",
      "2022-09-02 00:56:01 | [trpo_pendulum] epoch #64 | EpochTime 1.02 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -43.4614\n",
      "Evaluation/AverageReturn              -461.415\n",
      "Evaluation/Iteration                    64\n",
      "Evaluation/MaxReturn                  -410.123\n",
      "Evaluation/MinReturn                  -512.708\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    51.2927\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       129087\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6825,  1.9648,  0.7108,  ...,  0.2552, -1.4514,  1.1976])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6820,  1.9649,  0.7109,  ...,  0.2549, -1.4551,  1.1978])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(15.5097)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-8.0543e+00, -1.0275e-01],\n",
      "        [-1.0275e-01,  1.2845e-04]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0017, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-6.0064e-07,  1.1617e-04,  8.8896e-05,  ..., -6.2234e-04,\n",
      "        -4.3754e-03, -3.8305e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(30.8039, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(30.8403, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:02 | [trpo_pendulum] epoch #65 | Saving snapshot...\n",
      "2022-09-02 00:56:02 | [trpo_pendulum] epoch #65 | Saved\n",
      "2022-09-02 00:56:02 | [trpo_pendulum] epoch #65 | Time 70.95 s\n",
      "2022-09-02 00:56:02 | [trpo_pendulum] epoch #65 | EpochTime 1.04 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -87.1788\n",
      "Evaluation/AverageReturn              -970.337\n",
      "Evaluation/Iteration                    65\n",
      "Evaluation/MaxReturn                  -909.137\n",
      "Evaluation/MinReturn                 -1031.54\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    61.2005\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       131071\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6820,  1.9649,  0.7109,  ...,  0.2549, -1.4551,  1.1978])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6820,  1.9650,  0.7110,  ...,  0.2543, -1.4595,  1.1975])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.3913)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[1.7512e+02, 8.0776e-02],\n",
      "        [8.0776e-02, 3.1009e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 2.0593e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-2.7172e-06, -3.4805e-05,  4.7448e-05,  ...,  2.5449e-04,\n",
      "         1.0711e-03,  2.2502e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-23.8456, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-23.8445, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:03 | [trpo_pendulum] epoch #66 | Saving snapshot...\n",
      "2022-09-02 00:56:03 | [trpo_pendulum] epoch #66 | Saved\n",
      "2022-09-02 00:56:03 | [trpo_pendulum] epoch #66 | Time 71.90 s\n",
      "2022-09-02 00:56:03 | [trpo_pendulum] epoch #66 | EpochTime 0.95 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -43.7948\n",
      "Evaluation/AverageReturn              -421.089\n",
      "Evaluation/Iteration                    66\n",
      "Evaluation/MaxReturn                  -399.492\n",
      "Evaluation/MinReturn                  -442.685\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    21.5963\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       133069\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6820,  1.9650,  0.7110,  ...,  0.2543, -1.4595,  1.1975])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6820,  1.9650,  0.7111,  ...,  0.2545, -1.4584,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.8935)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.1407e+01, -4.1802e-03],\n",
      "        [-4.1802e-03, -5.0317e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0008, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 9.5178e-05,  3.2857e-05,  5.5947e-05,  ...,  2.3996e-04,\n",
      "         2.0441e-03, -4.7538e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-10.5711, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-10.5697, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:04 | [trpo_pendulum] epoch #67 | Saving snapshot...\n",
      "2022-09-02 00:56:04 | [trpo_pendulum] epoch #67 | Saved\n",
      "2022-09-02 00:56:04 | [trpo_pendulum] epoch #67 | Time 73.03 s\n",
      "2022-09-02 00:56:04 | [trpo_pendulum] epoch #67 | EpochTime 1.13 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -47.2367\n",
      "Evaluation/AverageReturn              -539.607\n",
      "Evaluation/Iteration                    67\n",
      "Evaluation/MaxReturn                  -484.055\n",
      "Evaluation/MinReturn                  -595.16\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    55.5524\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       135067\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6820,  1.9650,  0.7111,  ...,  0.2545, -1.4584,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6819,  1.9650,  0.7111,  ...,  0.2548, -1.4564,  1.1976])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(12.9661)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-9.6993e+01,  5.5239e-02],\n",
      "        [ 5.5239e-02, -1.0135e-04]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0008, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.9468e-04, -1.3773e-04,  1.5648e-04,  ..., -1.1231e-05,\n",
      "         1.6631e-03,  6.6517e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(1.2140, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(1.2180, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:05 | [trpo_pendulum] epoch #68 | Saving snapshot...\n",
      "2022-09-02 00:56:05 | [trpo_pendulum] epoch #68 | Saved\n",
      "2022-09-02 00:56:05 | [trpo_pendulum] epoch #68 | Time 74.21 s\n",
      "2022-09-02 00:56:05 | [trpo_pendulum] epoch #68 | EpochTime 1.18 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -59.7341\n",
      "Evaluation/AverageReturn              -660.45\n",
      "Evaluation/Iteration                    68\n",
      "Evaluation/MaxReturn                  -423.64\n",
      "Evaluation/MinReturn                  -897.26\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   236.81\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       137065\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6819,  1.9650,  0.7111,  ...,  0.2548, -1.4564,  1.1976])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4547,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(12.7651)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.4683e+02, -8.8496e-02],\n",
      "        [-8.8496e-02, -8.8024e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0005, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 3.7615e-05,  1.0597e-04,  2.9996e-05,  ..., -6.6066e-05,\n",
      "        -1.4793e-03, -7.0836e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(13.4225, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(13.4246, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:06 | [trpo_pendulum] epoch #69 | Saving snapshot...\n",
      "2022-09-02 00:56:06 | [trpo_pendulum] epoch #69 | Saved\n",
      "2022-09-02 00:56:06 | [trpo_pendulum] epoch #69 | Time 75.32 s\n",
      "2022-09-02 00:56:06 | [trpo_pendulum] epoch #69 | EpochTime 1.11 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -53.5639\n",
      "Evaluation/AverageReturn              -777.314\n",
      "Evaluation/Iteration                    69\n",
      "Evaluation/MaxReturn                  -725.451\n",
      "Evaluation/MinReturn                  -829.177\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    51.8633\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       139063\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4547,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9650,  0.7113,  ...,  0.2547, -1.4562,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.3393)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.4824e+02,  1.7382e-02],\n",
      "        [ 1.7382e-02, -2.0025e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 4.3355e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.3812e-05, -5.6120e-05, -3.6522e-06,  ...,  3.7332e-05,\n",
      "         8.3736e-04, -7.1558e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-21.4949, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-21.4947, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:07 | [trpo_pendulum] epoch #70 | Saving snapshot...\n",
      "2022-09-02 00:56:07 | [trpo_pendulum] epoch #70 | Saved\n",
      "2022-09-02 00:56:07 | [trpo_pendulum] epoch #70 | Time 76.39 s\n",
      "2022-09-02 00:56:07 | [trpo_pendulum] epoch #70 | EpochTime 1.07 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -38.2559\n",
      "Evaluation/AverageReturn              -426.01\n",
      "Evaluation/Iteration                    70\n",
      "Evaluation/MaxReturn                  -402.686\n",
      "Evaluation/MinReturn                  -449.335\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    23.3245\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       141061\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9650,  0.7113,  ...,  0.2547, -1.4562,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4554,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.4721)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.8982e+01,  2.4518e-04],\n",
      "        [ 2.4518e-04,  3.3558e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.3460e-04, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-6.3445e-05, -3.5453e-05,  5.8445e-06,  ...,  2.5002e-05,\n",
      "         5.4777e-04, -1.4753e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-16.3612, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-16.3611, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:08 | [trpo_pendulum] epoch #71 | Saving snapshot...\n",
      "2022-09-02 00:56:08 | [trpo_pendulum] epoch #71 | Saved\n",
      "2022-09-02 00:56:08 | [trpo_pendulum] epoch #71 | Time 77.37 s\n",
      "2022-09-02 00:56:08 | [trpo_pendulum] epoch #71 | EpochTime 0.98 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -40.8632\n",
      "Evaluation/AverageReturn              -459.412\n",
      "Evaluation/Iteration                    71\n",
      "Evaluation/MaxReturn                  -373.091\n",
      "Evaluation/MinReturn                  -545.733\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    86.3211\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       143059\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4554,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4548,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.1957)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 5.7266e+00,  7.1382e-04],\n",
      "        [ 7.1382e-04, -2.7486e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-2.3270e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 2.2988e-05,  9.3807e-07, -9.4594e-06,  ..., -2.4189e-05,\n",
      "        -3.1838e-04,  2.7654e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-7.0634, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-7.0638, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:09 | [trpo_pendulum] epoch #72 | Saving snapshot...\n",
      "2022-09-02 00:56:09 | [trpo_pendulum] epoch #72 | Saved\n",
      "2022-09-02 00:56:09 | [trpo_pendulum] epoch #72 | Time 78.46 s\n",
      "2022-09-02 00:56:09 | [trpo_pendulum] epoch #72 | EpochTime 1.09 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -51.244\n",
      "Evaluation/AverageReturn              -545.714\n",
      "Evaluation/Iteration                    72\n",
      "Evaluation/MaxReturn                  -467.918\n",
      "Evaluation/MinReturn                  -623.511\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    77.7961\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       145057\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4548,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4551,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.7869)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-7.4413e-02, -4.6656e-03],\n",
      "        [-4.6656e-03, -2.2143e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-5.0112e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.6659e-06, -2.2396e-07,  1.5860e-06,  ..., -1.2560e-05,\n",
      "        -1.6162e-04,  1.2217e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-6.1586, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-6.1587, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:10 | [trpo_pendulum] epoch #73 | Saving snapshot...\n",
      "2022-09-02 00:56:10 | [trpo_pendulum] epoch #73 | Saved\n",
      "2022-09-02 00:56:10 | [trpo_pendulum] epoch #73 | Time 79.62 s\n",
      "2022-09-02 00:56:10 | [trpo_pendulum] epoch #73 | EpochTime 1.15 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -46.4878\n",
      "Evaluation/AverageReturn              -541.923\n",
      "Evaluation/Iteration                    73\n",
      "Evaluation/MaxReturn                  -428.845\n",
      "Evaluation/MinReturn                  -655.002\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   113.079\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       147055\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4551,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.3226)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 5.7492e+01,  1.0253e-03],\n",
      "        [ 1.0253e-03, -2.3953e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.7043e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.1649e-05,  2.3131e-06, -3.6155e-06,  ...,  5.1334e-06,\n",
      "         9.5439e-05, -6.5187e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(30.6466, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(30.6468, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:11 | [trpo_pendulum] epoch #74 | Saving snapshot...\n",
      "2022-09-02 00:56:11 | [trpo_pendulum] epoch #74 | Saved\n",
      "2022-09-02 00:56:11 | [trpo_pendulum] epoch #74 | Time 80.74 s\n",
      "2022-09-02 00:56:11 | [trpo_pendulum] epoch #74 | EpochTime 1.12 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -75.2595\n",
      "Evaluation/AverageReturn              -930.252\n",
      "Evaluation/Iteration                    74\n",
      "Evaluation/MaxReturn                  -714.374\n",
      "Evaluation/MinReturn                 -1146.13\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   215.878\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       149053\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.1276)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.2225e+00, -6.5270e-04],\n",
      "        [-6.5270e-04,  1.1574e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([9.9728e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-5.6553e-05,  2.5609e-05,  7.5407e-06,  ...,  3.2350e-05,\n",
      "         3.9594e-05, -4.0789e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(30.2695, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(30.2704, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:13 | [trpo_pendulum] epoch #75 | Saving snapshot...\n",
      "2022-09-02 00:56:13 | [trpo_pendulum] epoch #75 | Saved\n",
      "2022-09-02 00:56:13 | [trpo_pendulum] epoch #75 | Time 81.85 s\n",
      "2022-09-02 00:56:13 | [trpo_pendulum] epoch #75 | EpochTime 1.11 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -70.7361\n",
      "Evaluation/AverageReturn              -954.245\n",
      "Evaluation/Iteration                    75\n",
      "Evaluation/MaxReturn                  -934.605\n",
      "Evaluation/MinReturn                  -973.886\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    19.6408\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       151051\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6822,  1.9649,  0.7113,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(8.8548)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-5.1249e+02, -1.3132e-02],\n",
      "        [-1.3132e-02, -1.7562e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 3.1221e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.2774e-05, -1.5977e-05,  1.2185e-05,  ..., -1.6058e-05,\n",
      "        -2.3947e-05,  1.5016e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-26.0023, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-26.0023, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:14 | [trpo_pendulum] epoch #76 | Saving snapshot...\n",
      "2022-09-02 00:56:14 | [trpo_pendulum] epoch #76 | Saved\n",
      "2022-09-02 00:56:14 | [trpo_pendulum] epoch #76 | Time 82.93 s\n",
      "2022-09-02 00:56:14 | [trpo_pendulum] epoch #76 | EpochTime 1.08 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -41.0443\n",
      "Evaluation/AverageReturn              -388.965\n",
      "Evaluation/Iteration                    76\n",
      "Evaluation/MaxReturn                  -373.663\n",
      "Evaluation/MinReturn                  -404.268\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    15.3028\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       153049\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6822,  1.9649,  0.7113,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.1194)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.0969e+01, -1.3774e-03],\n",
      "        [-1.3774e-03, -5.5259e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.2113e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 6.7273e-06, -7.8966e-06,  5.7340e-06,  ..., -9.5260e-06,\n",
      "        -3.6199e-05,  9.5200e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-11.3137, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-11.3137, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:15 | [trpo_pendulum] epoch #77 | Saving snapshot...\n",
      "2022-09-02 00:56:15 | [trpo_pendulum] epoch #77 | Saved\n",
      "2022-09-02 00:56:15 | [trpo_pendulum] epoch #77 | Time 83.97 s\n",
      "2022-09-02 00:56:15 | [trpo_pendulum] epoch #77 | EpochTime 1.04 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -44.4391\n",
      "Evaluation/AverageReturn              -519.081\n",
      "Evaluation/Iteration                    77\n",
      "Evaluation/MaxReturn                  -383.277\n",
      "Evaluation/MinReturn                  -654.884\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   135.804\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       155047\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.8054)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[4.5653e+01, 1.4442e-03],\n",
      "        [1.4442e-03, 6.3274e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-4.1509e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.5184e-08, -8.9453e-07,  6.4879e-06,  ...,  4.3460e-07,\n",
      "         5.1690e-05, -2.1804e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(14.4469, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(14.4467, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:16 | [trpo_pendulum] epoch #78 | Saving snapshot...\n",
      "2022-09-02 00:56:16 | [trpo_pendulum] epoch #78 | Saved\n",
      "2022-09-02 00:56:16 | [trpo_pendulum] epoch #78 | Time 85.12 s\n",
      "2022-09-02 00:56:16 | [trpo_pendulum] epoch #78 | EpochTime 1.15 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn     -55.823\n",
      "Evaluation/AverageReturn              -777.566\n",
      "Evaluation/Iteration                    78\n",
      "Evaluation/MaxReturn                  -604.64\n",
      "Evaluation/MinReturn                  -950.491\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   172.925\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       157045\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.4939)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.7522e+01,  4.6151e-04],\n",
      "        [ 4.6151e-04, -4.9690e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-6.8511e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 2.7254e-06,  7.3045e-07, -3.6453e-06,  ..., -6.0207e-07,\n",
      "        -3.3149e-05,  1.8147e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-17.4902, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-17.4903, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:17 | [trpo_pendulum] epoch #79 | Saving snapshot...\n",
      "2022-09-02 00:56:17 | [trpo_pendulum] epoch #79 | Saved\n",
      "2022-09-02 00:56:17 | [trpo_pendulum] epoch #79 | Time 86.27 s\n",
      "2022-09-02 00:56:17 | [trpo_pendulum] epoch #79 | EpochTime 1.14 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn     -40.9618\n",
      "Evaluation/AverageReturn              -456.531\n",
      "Evaluation/Iteration                    79\n",
      "Evaluation/MaxReturn                  -451.588\n",
      "Evaluation/MinReturn                  -461.473\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     4.94237\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       159043\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(18.1327)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[1.1541e+02, 3.2483e-04],\n",
      "        [3.2483e-04, 1.7392e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-4.7431e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-6.5296e-06,  1.3668e-05, -3.4758e-07,  ...,  3.4299e-06,\n",
      "         5.7708e-05, -5.4249e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(26.8268, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(26.8263, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:18 | [trpo_pendulum] epoch #80 | Saving snapshot...\n",
      "2022-09-02 00:56:18 | [trpo_pendulum] epoch #80 | Saved\n",
      "2022-09-02 00:56:18 | [trpo_pendulum] epoch #80 | Time 87.35 s\n",
      "2022-09-02 00:56:18 | [trpo_pendulum] epoch #80 | EpochTime 1.08 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -55.8383\n",
      "Evaluation/AverageReturn              -897.576\n",
      "Evaluation/Iteration                    80\n",
      "Evaluation/MaxReturn                  -774.735\n",
      "Evaluation/MinReturn                 -1020.42\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   122.84\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       161041\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2547, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4551,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.1736)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-9.8992e+00,  1.4482e-03],\n",
      "        [ 1.4482e-03,  7.5456e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 4.0732e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-7.9360e-06, -5.8170e-06,  1.9101e-06,  ...,  3.3175e-06,\n",
      "         4.5205e-05, -4.9154e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-8.5360, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-8.5360, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:19 | [trpo_pendulum] epoch #81 | Saving snapshot...\n",
      "2022-09-02 00:56:19 | [trpo_pendulum] epoch #81 | Saved\n",
      "2022-09-02 00:56:19 | [trpo_pendulum] epoch #81 | Time 88.37 s\n",
      "2022-09-02 00:56:19 | [trpo_pendulum] epoch #81 | EpochTime 1.02 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -46.1256\n",
      "Evaluation/AverageReturn              -558.915\n",
      "Evaluation/Iteration                    81\n",
      "Evaluation/MaxReturn                  -428.774\n",
      "Evaluation/MinReturn                  -689.057\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   130.141\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       163039\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4551,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6822,  1.9649,  0.7113,  ...,  0.2548, -1.4551,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.0281)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 3.3858e+01, -1.4221e-03],\n",
      "        [-1.4221e-03, -4.9798e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([2.5511e-07, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-3.9769e-06, -2.9540e-06,  9.3346e-07,  ...,  1.7208e-06,\n",
      "         2.3906e-05, -2.5219e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-22.0600, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-22.0599, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:20 | [trpo_pendulum] epoch #82 | Saving snapshot...\n",
      "2022-09-02 00:56:20 | [trpo_pendulum] epoch #82 | Saved\n",
      "2022-09-02 00:56:20 | [trpo_pendulum] epoch #82 | Time 89.37 s\n",
      "2022-09-02 00:56:20 | [trpo_pendulum] epoch #82 | EpochTime 1.00 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -37.5443\n",
      "Evaluation/AverageReturn              -407.983\n",
      "Evaluation/Iteration                    82\n",
      "Evaluation/MaxReturn                  -384.451\n",
      "Evaluation/MinReturn                  -431.516\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    23.5329\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       165037\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6822,  1.9649,  0.7113,  ...,  0.2548, -1.4551,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6822,  1.9649,  0.7113,  ...,  0.2548, -1.4551,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(14.0713)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.9474e+01, -1.2021e-03],\n",
      "        [-1.2021e-03, -4.3675e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.6592e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 8.6203e-07, -2.6363e-06,  4.9960e-07,  ...,  3.2241e-06,\n",
      "         3.4574e-05, -3.2757e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-14.7189, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-14.7189, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:21 | [trpo_pendulum] epoch #83 | Saving snapshot...\n",
      "2022-09-02 00:56:21 | [trpo_pendulum] epoch #83 | Saved\n",
      "2022-09-02 00:56:21 | [trpo_pendulum] epoch #83 | Time 90.50 s\n",
      "2022-09-02 00:56:21 | [trpo_pendulum] epoch #83 | EpochTime 1.12 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -38.667\n",
      "Evaluation/AverageReturn              -460.272\n",
      "Evaluation/Iteration                    83\n",
      "Evaluation/MaxReturn                  -361.721\n",
      "Evaluation/MinReturn                  -558.822\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    98.5503\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       167035\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6822,  1.9649,  0.7113,  ...,  0.2548, -1.4551,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6822,  1.9649,  0.7113,  ...,  0.2548, -1.4550,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.9070)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 6.4774e+00, -8.9912e-05],\n",
      "        [-8.9912e-05, -1.4550e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-9.4780e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.8589e-05, -1.0281e-05, -3.9489e-06,  ..., -4.1381e-07,\n",
      "         2.9609e-05,  6.4604e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(11.1396, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(11.1393, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:22 | [trpo_pendulum] epoch #84 | Saving snapshot...\n",
      "2022-09-02 00:56:22 | [trpo_pendulum] epoch #84 | Saved\n",
      "2022-09-02 00:56:22 | [trpo_pendulum] epoch #84 | Time 91.73 s\n",
      "2022-09-02 00:56:22 | [trpo_pendulum] epoch #84 | EpochTime 1.24 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -54.3659\n",
      "Evaluation/AverageReturn              -718.052\n",
      "Evaluation/Iteration                    84\n",
      "Evaluation/MaxReturn                  -564.652\n",
      "Evaluation/MinReturn                  -871.452\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   153.4\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       169033\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6822,  1.9649,  0.7113,  ...,  0.2548, -1.4550,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4550,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.9633)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[2.8377e+00, 2.7874e-04],\n",
      "        [2.7874e-04, 5.2763e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-3.7486e-04,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.4510e-05,  4.7012e-06, -2.2491e-06,  ...,  1.6628e-06,\n",
      "        -1.4015e-04,  3.2802e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-8.8382, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-8.8384, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:24 | [trpo_pendulum] epoch #85 | Saving snapshot...\n",
      "2022-09-02 00:56:24 | [trpo_pendulum] epoch #85 | Saved\n",
      "2022-09-02 00:56:24 | [trpo_pendulum] epoch #85 | Time 92.87 s\n",
      "2022-09-02 00:56:24 | [trpo_pendulum] epoch #85 | EpochTime 1.13 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -45.0788\n",
      "Evaluation/AverageReturn              -521.462\n",
      "Evaluation/Iteration                    85\n",
      "Evaluation/MaxReturn                  -413.783\n",
      "Evaluation/MinReturn                  -629.14\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   107.678\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       171031\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4550,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.5523)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 2.8690e+01, -3.7391e-03],\n",
      "        [-3.7391e-03, -6.9979e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.2197e-04,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-3.2425e-05,  2.1671e-05, -3.3753e-05,  ...,  2.8971e-05,\n",
      "        -8.3121e-05, -3.8707e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(6.0845, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(6.0841, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:25 | [trpo_pendulum] epoch #86 | Saving snapshot...\n",
      "2022-09-02 00:56:25 | [trpo_pendulum] epoch #86 | Saved\n",
      "2022-09-02 00:56:25 | [trpo_pendulum] epoch #86 | Time 94.05 s\n",
      "2022-09-02 00:56:25 | [trpo_pendulum] epoch #86 | EpochTime 1.18 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -55.3881\n",
      "Evaluation/AverageReturn              -670.091\n",
      "Evaluation/Iteration                    86\n",
      "Evaluation/MaxReturn                  -483.045\n",
      "Evaluation/MinReturn                  -857.136\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   187.045\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       173029\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4552,  1.1976])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.1324)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-4.1789e+01, -2.6916e-03],\n",
      "        [-2.6916e-03, -6.0409e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.5854e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-3.0674e-05, -9.1682e-06, -4.8786e-05,  ..., -2.0574e-05,\n",
      "         3.7365e-05,  2.4523e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(30.4306, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(30.4321, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:26 | [trpo_pendulum] epoch #87 | Saving snapshot...\n",
      "2022-09-02 00:56:26 | [trpo_pendulum] epoch #87 | Saved\n",
      "2022-09-02 00:56:26 | [trpo_pendulum] epoch #87 | Time 95.25 s\n",
      "2022-09-02 00:56:26 | [trpo_pendulum] epoch #87 | EpochTime 1.19 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -75.6643\n",
      "Evaluation/AverageReturn              -941.585\n",
      "Evaluation/Iteration                    87\n",
      "Evaluation/MaxReturn                  -832.863\n",
      "Evaluation/MinReturn                 -1050.31\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   108.721\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       175027\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7113,  ...,  0.2548, -1.4552,  1.1976])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.6059)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.1143e+02, -5.0756e-04],\n",
      "        [-5.0756e-04, -1.3701e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-5.6981e-06,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.3962e-05, -5.5113e-06, -2.5600e-05,  ..., -1.0206e-05,\n",
      "         2.6782e-05,  1.2229e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(12.3213, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(12.3217, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:27 | [trpo_pendulum] epoch #88 | Saving snapshot...\n",
      "2022-09-02 00:56:27 | [trpo_pendulum] epoch #88 | Saved\n",
      "2022-09-02 00:56:27 | [trpo_pendulum] epoch #88 | Time 96.28 s\n",
      "2022-09-02 00:56:27 | [trpo_pendulum] epoch #88 | EpochTime 1.03 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -48.0729\n",
      "Evaluation/AverageReturn              -768.99\n",
      "Evaluation/Iteration                    88\n",
      "Evaluation/MaxReturn                  -615.108\n",
      "Evaluation/MinReturn                  -922.872\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   153.882\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       177025\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6822,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.0675)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 5.7567e+01,  5.3860e-04],\n",
      "        [ 5.3860e-04, -1.6040e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-7.1190e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-4.6474e-05, -7.5144e-06, -2.9091e-05,  ...,  5.2425e-06,\n",
      "         1.8233e-04,  1.5549e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-3.2367, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-3.2369, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:28 | [trpo_pendulum] epoch #89 | Saving snapshot...\n",
      "2022-09-02 00:56:28 | [trpo_pendulum] epoch #89 | Saved\n",
      "2022-09-02 00:56:28 | [trpo_pendulum] epoch #89 | Time 97.36 s\n",
      "2022-09-02 00:56:28 | [trpo_pendulum] epoch #89 | EpochTime 1.08 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn     -55.401\n",
      "Evaluation/AverageReturn              -634.105\n",
      "Evaluation/Iteration                    89\n",
      "Evaluation/MaxReturn                  -520.947\n",
      "Evaluation/MinReturn                  -747.263\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   113.158\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       179023\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6822,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6822,  1.9649,  0.7112,  ...,  0.2548, -1.4550,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.7053)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-8.1740e+01,  6.9078e-03],\n",
      "        [ 6.9078e-03, -4.4173e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-2.9555e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 3.0244e-05,  4.2415e-06,  1.9913e-06,  ..., -5.4623e-06,\n",
      "        -1.3074e-04,  7.5957e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(2.9223, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(2.9222, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:29 | [trpo_pendulum] epoch #90 | Saving snapshot...\n",
      "2022-09-02 00:56:29 | [trpo_pendulum] epoch #90 | Saved\n",
      "2022-09-02 00:56:29 | [trpo_pendulum] epoch #90 | Time 98.48 s\n",
      "2022-09-02 00:56:29 | [trpo_pendulum] epoch #90 | EpochTime 1.11 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -40.5264\n",
      "Evaluation/AverageReturn              -677.553\n",
      "Evaluation/Iteration                    90\n",
      "Evaluation/MaxReturn                  -557.932\n",
      "Evaluation/MinReturn                  -797.175\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   119.621\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       181021\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6822,  1.9649,  0.7112,  ...,  0.2548, -1.4550,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6822,  1.9649,  0.7112,  ...,  0.2548, -1.4551,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.3768)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.9485e+01, -1.3295e-03],\n",
      "        [-1.3295e-03, -5.0663e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-2.2215e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 3.5983e-05,  1.1308e-06, -2.2357e-06,  ..., -3.4720e-06,\n",
      "        -1.0796e-04,  4.9576e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(6.1279, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(6.1277, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:30 | [trpo_pendulum] epoch #91 | Saving snapshot...\n",
      "2022-09-02 00:56:30 | [trpo_pendulum] epoch #91 | Saved\n",
      "2022-09-02 00:56:30 | [trpo_pendulum] epoch #91 | Time 99.55 s\n",
      "2022-09-02 00:56:30 | [trpo_pendulum] epoch #91 | EpochTime 1.06 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn     -67.498\n",
      "Evaluation/AverageReturn              -740.241\n",
      "Evaluation/Iteration                    91\n",
      "Evaluation/MaxReturn                  -494.251\n",
      "Evaluation/MinReturn                  -986.23\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   245.99\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       183019\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6822,  1.9649,  0.7112,  ...,  0.2548, -1.4551,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.8372)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 9.9476e+01, -2.9791e-03],\n",
      "        [-2.9791e-03, -6.6828e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.0805e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 2.4858e-05, -2.7264e-06,  1.8447e-06,  ...,  2.1034e-06,\n",
      "        -2.4607e-05, -2.7113e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(13.5057, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(13.5056, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:31 | [trpo_pendulum] epoch #92 | Saving snapshot...\n",
      "2022-09-02 00:56:31 | [trpo_pendulum] epoch #92 | Saved\n",
      "2022-09-02 00:56:31 | [trpo_pendulum] epoch #92 | Time 100.56 s\n",
      "2022-09-02 00:56:31 | [trpo_pendulum] epoch #92 | EpochTime 1.01 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -81.5364\n",
      "Evaluation/AverageReturn              -837.344\n",
      "Evaluation/Iteration                    92\n",
      "Evaluation/MaxReturn                  -643.397\n",
      "Evaluation/MinReturn                 -1031.29\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   193.947\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       185017\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.5900)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 6.5351e+01,  4.0845e-04],\n",
      "        [ 4.0845e-04, -3.0363e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.6162e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.3665e-05,  1.0037e-06, -5.5363e-07,  ..., -9.2590e-07,\n",
      "         1.4232e-05,  1.2366e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-24.9866, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-24.9866, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:32 | [trpo_pendulum] epoch #93 | Saving snapshot...\n",
      "2022-09-02 00:56:32 | [trpo_pendulum] epoch #93 | Saved\n",
      "2022-09-02 00:56:32 | [trpo_pendulum] epoch #93 | Time 101.77 s\n",
      "2022-09-02 00:56:32 | [trpo_pendulum] epoch #93 | EpochTime 1.21 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -51.9383\n",
      "Evaluation/AverageReturn              -443.699\n",
      "Evaluation/Iteration                    93\n",
      "Evaluation/MaxReturn                  -396.233\n",
      "Evaluation/MinReturn                  -491.166\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    47.4664\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       187015\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.3044)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-9.1545e+00,  8.5971e-04],\n",
      "        [ 8.5971e-04,  7.6086e-11]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.8378e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 6.6696e-06, -6.3522e-07,  7.8615e-07,  ...,  4.7836e-07,\n",
      "        -5.3913e-06, -7.7812e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-9.4934, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-9.4934, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:34 | [trpo_pendulum] epoch #94 | Saving snapshot...\n",
      "2022-09-02 00:56:34 | [trpo_pendulum] epoch #94 | Saved\n",
      "2022-09-02 00:56:34 | [trpo_pendulum] epoch #94 | Time 102.91 s\n",
      "2022-09-02 00:56:34 | [trpo_pendulum] epoch #94 | EpochTime 1.13 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn     -43.2817\n",
      "Evaluation/AverageReturn              -556.791\n",
      "Evaluation/Iteration                    94\n",
      "Evaluation/MaxReturn                  -547.339\n",
      "Evaluation/MinReturn                  -566.242\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     9.45127\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       189013\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(8.1645)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.5638e+01, -1.9598e-05],\n",
      "        [-1.9598e-05,  5.3270e-10]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-4.0043e-06,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.8139e-06, -5.0833e-07,  4.0419e-07,  ...,  4.4515e-07,\n",
      "         3.1488e-06, -6.1321e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-10.2529, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-10.2529, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:35 | [trpo_pendulum] epoch #95 | Saving snapshot...\n",
      "2022-09-02 00:56:35 | [trpo_pendulum] epoch #95 | Saved\n",
      "2022-09-02 00:56:35 | [trpo_pendulum] epoch #95 | Time 104.07 s\n",
      "2022-09-02 00:56:35 | [trpo_pendulum] epoch #95 | EpochTime 1.16 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn     -40.218\n",
      "Evaluation/AverageReturn              -531.813\n",
      "Evaluation/Iteration                    95\n",
      "Evaluation/MaxReturn                  -375.925\n",
      "Evaluation/MinReturn                  -687.702\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   155.888\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       191011\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(1.1801)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-4.2319e+01, -3.0326e-05],\n",
      "        [-3.0326e-05, -6.3199e-10]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 5.7156e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-4.4862e-06,  2.9143e-07, -2.9519e-06,  ...,  9.1915e-07,\n",
      "         1.7603e-05, -1.4710e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-16.5371, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-16.5371, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:36 | [trpo_pendulum] epoch #96 | Saving snapshot...\n",
      "2022-09-02 00:56:36 | [trpo_pendulum] epoch #96 | Saved\n",
      "2022-09-02 00:56:36 | [trpo_pendulum] epoch #96 | Time 105.14 s\n",
      "2022-09-02 00:56:36 | [trpo_pendulum] epoch #96 | EpochTime 1.08 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -37.8674\n",
      "Evaluation/AverageReturn              -449.041\n",
      "Evaluation/Iteration                    96\n",
      "Evaluation/MaxReturn                  -375.313\n",
      "Evaluation/MinReturn                  -522.77\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    73.7285\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       193009\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(1.4315)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.2361e+00, -8.5671e-05],\n",
      "        [-8.5671e-05,  6.6948e-10]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([5.1725e-06, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-3.0304e-06,  1.2542e-08, -1.1930e-06,  ...,  9.1625e-07,\n",
      "         1.7003e-05, -7.8081e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-12.4445, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-12.4445, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:37 | [trpo_pendulum] epoch #97 | Saving snapshot...\n",
      "2022-09-02 00:56:37 | [trpo_pendulum] epoch #97 | Saved\n",
      "2022-09-02 00:56:37 | [trpo_pendulum] epoch #97 | Time 106.22 s\n",
      "2022-09-02 00:56:37 | [trpo_pendulum] epoch #97 | EpochTime 1.07 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -39.3993\n",
      "Evaluation/AverageReturn              -461.341\n",
      "Evaluation/Iteration                    97\n",
      "Evaluation/MaxReturn                  -384.347\n",
      "Evaluation/MinReturn                  -538.334\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    76.9937\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       195007\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.9543)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.5035e+01,  9.9831e-05],\n",
      "        [ 9.9831e-05,  3.0134e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 6.3946e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.9199e-06,  3.9319e-07,  2.3909e-06,  ..., -1.3484e-07,\n",
      "         4.3335e-07, -9.4960e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(10.1565, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(10.1566, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:38 | [trpo_pendulum] epoch #98 | Saving snapshot...\n",
      "2022-09-02 00:56:38 | [trpo_pendulum] epoch #98 | Saved\n",
      "2022-09-02 00:56:38 | [trpo_pendulum] epoch #98 | Time 107.38 s\n",
      "2022-09-02 00:56:38 | [trpo_pendulum] epoch #98 | EpochTime 1.15 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -61.5417\n",
      "Evaluation/AverageReturn              -683.913\n",
      "Evaluation/Iteration                    98\n",
      "Evaluation/MaxReturn                  -547.37\n",
      "Evaluation/MinReturn                  -820.457\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   136.544\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       197005\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(11.7732)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.7034e+01,  2.5371e-04],\n",
      "        [ 2.5371e-04, -5.2868e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.0315e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 7.9653e-07, -4.6225e-07,  2.1948e-06,  ..., -1.3238e-06,\n",
      "        -3.4883e-06,  7.8793e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(0.5700, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(0.5700, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:39 | [trpo_pendulum] epoch #99 | Saving snapshot...\n",
      "2022-09-02 00:56:39 | [trpo_pendulum] epoch #99 | Saved\n",
      "2022-09-02 00:56:39 | [trpo_pendulum] epoch #99 | Time 108.43 s\n",
      "2022-09-02 00:56:39 | [trpo_pendulum] epoch #99 | EpochTime 1.05 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -41.8879\n",
      "Evaluation/AverageReturn              -589.661\n",
      "Evaluation/Iteration                    99\n",
      "Evaluation/MaxReturn                  -395.207\n",
      "Evaluation/MinReturn                  -784.114\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   194.453\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       199003\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(13.0871)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-6.4601e+01,  4.7522e-04],\n",
      "        [ 4.7522e-04, -6.4510e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([2.8710e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.0708e-05, -1.5885e-07,  1.7977e-06,  ..., -2.4820e-06,\n",
      "        -6.5673e-05, -2.3911e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(46.2705, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(46.2714, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:40 | [trpo_pendulum] epoch #100 | Saving snapshot...\n",
      "2022-09-02 00:56:40 | [trpo_pendulum] epoch #100 | Saved\n",
      "2022-09-02 00:56:40 | [trpo_pendulum] epoch #100 | Time 109.45 s\n",
      "2022-09-02 00:56:40 | [trpo_pendulum] epoch #100 | EpochTime 1.02 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn    -112.798\n",
      "Evaluation/AverageReturn             -1063.55\n",
      "Evaluation/Iteration                   100\n",
      "Evaluation/MaxReturn                 -1000.16\n",
      "Evaluation/MinReturn                 -1126.94\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    63.3912\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       200911\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.2375)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[1.0686e+01, 5.9840e-04],\n",
      "        [5.9840e-04, 2.8041e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 2.1711e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-7.5137e-06, -1.6946e-06,  8.8519e-08,  ...,  1.8133e-06,\n",
      "         5.4630e-05,  8.1603e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-1.1833, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-1.1833, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:41 | [trpo_pendulum] epoch #101 | Saving snapshot...\n",
      "2022-09-02 00:56:41 | [trpo_pendulum] epoch #101 | Saved\n",
      "2022-09-02 00:56:41 | [trpo_pendulum] epoch #101 | Time 110.56 s\n",
      "2022-09-02 00:56:41 | [trpo_pendulum] epoch #101 | EpochTime 1.11 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -42.7767\n",
      "Evaluation/AverageReturn              -628.492\n",
      "Evaluation/Iteration                   101\n",
      "Evaluation/MaxReturn                  -531.306\n",
      "Evaluation/MinReturn                  -725.678\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    97.186\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       202909\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.4117)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-9.4363e+00,  3.4462e-04],\n",
      "        [ 3.4462e-04, -4.8911e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-3.9573e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.6385e-06,  9.4509e-07, -1.5289e-06,  ...,  3.9495e-07,\n",
      "        -1.0054e-05, -1.4587e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-17.9991, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-17.9991, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:42 | [trpo_pendulum] epoch #102 | Saving snapshot...\n",
      "2022-09-02 00:56:42 | [trpo_pendulum] epoch #102 | Saved\n",
      "2022-09-02 00:56:42 | [trpo_pendulum] epoch #102 | Time 111.69 s\n",
      "2022-09-02 00:56:42 | [trpo_pendulum] epoch #102 | EpochTime 1.13 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn     -43.7985\n",
      "Evaluation/AverageReturn              -461.508\n",
      "Evaluation/Iteration                   102\n",
      "Evaluation/MaxReturn                  -452.454\n",
      "Evaluation/MinReturn                  -470.561\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     9.05347\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       204907\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.7251)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-7.6814e+00,  5.5914e-04],\n",
      "        [ 5.5914e-04,  3.5217e-10]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-3.4332e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-4.1814e-06, -3.2621e-07,  3.5852e-07,  ..., -5.9712e-07,\n",
      "        -7.4860e-07,  1.3197e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-19.9896, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-19.9896, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:43 | [trpo_pendulum] epoch #103 | Saving snapshot...\n",
      "2022-09-02 00:56:43 | [trpo_pendulum] epoch #103 | Saved\n",
      "2022-09-02 00:56:43 | [trpo_pendulum] epoch #103 | Time 112.75 s\n",
      "2022-09-02 00:56:43 | [trpo_pendulum] epoch #103 | EpochTime 1.06 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -38.4772\n",
      "Evaluation/AverageReturn              -417.235\n",
      "Evaluation/Iteration                   103\n",
      "Evaluation/MaxReturn                  -396.057\n",
      "Evaluation/MinReturn                  -438.413\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    21.1781\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       206905\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(10.4554)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.5332e+01,  8.6494e-04],\n",
      "        [ 8.6494e-04,  3.7218e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.1262e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 4.6617e-06, -4.5328e-07,  3.7767e-06,  ..., -3.4372e-07,\n",
      "        -2.4506e-05, -2.4372e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(41.8229, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(41.8230, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:44 | [trpo_pendulum] epoch #104 | Saving snapshot...\n",
      "2022-09-02 00:56:45 | [trpo_pendulum] epoch #104 | Saved\n",
      "2022-09-02 00:56:45 | [trpo_pendulum] epoch #104 | Time 113.83 s\n",
      "2022-09-02 00:56:45 | [trpo_pendulum] epoch #104 | EpochTime 1.07 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -86.8025\n",
      "Evaluation/AverageReturn             -1058.67\n",
      "Evaluation/Iteration                   104\n",
      "Evaluation/MaxReturn                  -911.165\n",
      "Evaluation/MinReturn                 -1206.16\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   147.5\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       208903\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.3304)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.2351e+02, -1.8985e-04],\n",
      "        [-1.8985e-04, -1.0046e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-9.6550e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-5.1609e-05, -2.1815e-05, -1.7270e-06,  ..., -1.2985e-05,\n",
      "        -1.1685e-04, -1.8846e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(41.7192, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(41.7179, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:46 | [trpo_pendulum] epoch #105 | Saving snapshot...\n",
      "2022-09-02 00:56:46 | [trpo_pendulum] epoch #105 | Saved\n",
      "2022-09-02 00:56:46 | [trpo_pendulum] epoch #105 | Time 114.98 s\n",
      "2022-09-02 00:56:46 | [trpo_pendulum] epoch #105 | EpochTime 1.15 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -83.8574\n",
      "Evaluation/AverageReturn             -1076.56\n",
      "Evaluation/Iteration                   105\n",
      "Evaluation/MaxReturn                 -1014.41\n",
      "Evaluation/MinReturn                 -1138.71\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    62.147\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       210901\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6822,  1.9649,  0.7112,  ...,  0.2547, -1.4554,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.6362)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.9386e+00,  6.1774e-04],\n",
      "        [ 6.1774e-04,  2.5341e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-2.7547e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 3.5273e-05,  1.1917e-05, -9.1248e-07,  ...,  9.5656e-06,\n",
      "         9.2019e-05, -2.2131e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-11.1340, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-11.1341, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:47 | [trpo_pendulum] epoch #106 | Saving snapshot...\n",
      "2022-09-02 00:56:47 | [trpo_pendulum] epoch #106 | Saved\n",
      "2022-09-02 00:56:47 | [trpo_pendulum] epoch #106 | Time 116.15 s\n",
      "2022-09-02 00:56:47 | [trpo_pendulum] epoch #106 | EpochTime 1.17 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -43.0469\n",
      "Evaluation/AverageReturn              -553.972\n",
      "Evaluation/Iteration                   106\n",
      "Evaluation/MaxReturn                  -509.241\n",
      "Evaluation/MinReturn                  -598.703\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    44.7307\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       212899\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6822,  1.9649,  0.7112,  ...,  0.2547, -1.4554,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.4277)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.9285e+02,  4.8329e-04],\n",
      "        [ 4.8329e-04, -1.4208e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-6.6580e-06,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.2106e-05,  5.3825e-06, -2.4104e-06,  ...,  5.0903e-06,\n",
      "         5.9987e-05, -5.5797e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-26.7806, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-26.7806, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:48 | [trpo_pendulum] epoch #107 | Saving snapshot...\n",
      "2022-09-02 00:56:48 | [trpo_pendulum] epoch #107 | Saved\n",
      "2022-09-02 00:56:48 | [trpo_pendulum] epoch #107 | Time 117.19 s\n",
      "2022-09-02 00:56:48 | [trpo_pendulum] epoch #107 | EpochTime 1.04 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -34.5812\n",
      "Evaluation/AverageReturn              -381.968\n",
      "Evaluation/Iteration                   107\n",
      "Evaluation/MaxReturn                  -370.721\n",
      "Evaluation/MinReturn                  -393.216\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    11.2475\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       214897\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.3348)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.5705e+01,  6.0449e-05],\n",
      "        [ 6.0449e-05,  8.9005e-10]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 3.4869e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-2.6313e-06, -2.6477e-06,  1.5332e-06,  ..., -2.1715e-06,\n",
      "        -2.2956e-05, -3.5140e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-18.9458, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-18.9458, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:49 | [trpo_pendulum] epoch #108 | Saving snapshot...\n",
      "2022-09-02 00:56:49 | [trpo_pendulum] epoch #108 | Saved\n",
      "2022-09-02 00:56:49 | [trpo_pendulum] epoch #108 | Time 118.33 s\n",
      "2022-09-02 00:56:49 | [trpo_pendulum] epoch #108 | EpochTime 1.14 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -40.1619\n",
      "Evaluation/AverageReturn              -443.489\n",
      "Evaluation/Iteration                   108\n",
      "Evaluation/MaxReturn                  -375.479\n",
      "Evaluation/MinReturn                  -511.498\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    68.0092\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       216895\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.9540)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.1034e+02,  2.7032e-04],\n",
      "        [ 2.7032e-04,  2.6365e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([2.3663e-06, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.4045e-06, -1.2434e-06, -5.8408e-07,  ..., -1.2398e-06,\n",
      "        -1.1510e-05,  5.6102e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-20.4225, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-20.4225, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:50 | [trpo_pendulum] epoch #109 | Saving snapshot...\n",
      "2022-09-02 00:56:50 | [trpo_pendulum] epoch #109 | Saved\n",
      "2022-09-02 00:56:50 | [trpo_pendulum] epoch #109 | Time 119.42 s\n",
      "2022-09-02 00:56:50 | [trpo_pendulum] epoch #109 | EpochTime 1.09 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -38.6536\n",
      "Evaluation/AverageReturn              -409.297\n",
      "Evaluation/Iteration                   109\n",
      "Evaluation/MaxReturn                  -396.182\n",
      "Evaluation/MinReturn                  -422.413\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    13.1156\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       218893\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(1.6317)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 5.3934e+01, -7.6861e-05],\n",
      "        [-7.6861e-05, -1.7610e-10]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-3.1269e-07,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-6.4806e-07, -5.8900e-07, -2.3078e-07,  ..., -6.0496e-07,\n",
      "        -5.9799e-06,  2.3792e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-20.9119, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-20.9119, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:51 | [trpo_pendulum] epoch #110 | Saving snapshot...\n",
      "2022-09-02 00:56:51 | [trpo_pendulum] epoch #110 | Saved\n",
      "2022-09-02 00:56:51 | [trpo_pendulum] epoch #110 | Time 120.60 s\n",
      "2022-09-02 00:56:51 | [trpo_pendulum] epoch #110 | EpochTime 1.18 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -38.4836\n",
      "Evaluation/AverageReturn              -382.457\n",
      "Evaluation/Iteration                   110\n",
      "Evaluation/MaxReturn                  -370.749\n",
      "Evaluation/MinReturn                  -394.165\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    11.708\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       220891\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.4483)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.0499e+01, -5.3289e-05],\n",
      "        [-5.3289e-05, -2.2545e-11]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([6.5680e-07, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 6.2936e-09, -2.6909e-07, -2.7440e-07,  ..., -2.1764e-07,\n",
      "        -1.8455e-06,  3.5837e-08])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(9.2066, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(9.2066, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:52 | [trpo_pendulum] epoch #111 | Saving snapshot...\n",
      "2022-09-02 00:56:52 | [trpo_pendulum] epoch #111 | Saved\n",
      "2022-09-02 00:56:52 | [trpo_pendulum] epoch #111 | Time 121.68 s\n",
      "2022-09-02 00:56:52 | [trpo_pendulum] epoch #111 | EpochTime 1.08 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -45.3553\n",
      "Evaluation/AverageReturn              -665.399\n",
      "Evaluation/Iteration                   111\n",
      "Evaluation/MaxReturn                  -485.165\n",
      "Evaluation/MinReturn                  -845.633\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   180.234\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       222889\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.7870)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-6.2383e+01,  1.2961e-04],\n",
      "        [ 1.2961e-04,  5.1749e-11]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.1272e-07, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 6.8803e-08,  1.0525e-07,  1.9135e-07,  ...,  9.6609e-08,\n",
      "         9.5879e-07, -1.2056e-09])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-19.7193, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-19.7193, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:53 | [trpo_pendulum] epoch #112 | Saving snapshot...\n",
      "2022-09-02 00:56:53 | [trpo_pendulum] epoch #112 | Saved\n",
      "2022-09-02 00:56:53 | [trpo_pendulum] epoch #112 | Time 122.78 s\n",
      "2022-09-02 00:56:53 | [trpo_pendulum] epoch #112 | EpochTime 1.10 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn     -36.3815\n",
      "Evaluation/AverageReturn              -378.815\n",
      "Evaluation/Iteration                   112\n",
      "Evaluation/MaxReturn                  -373.403\n",
      "Evaluation/MinReturn                  -384.226\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     5.41136\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       224887\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.5798)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.0106e+01,  1.0526e-05],\n",
      "        [ 1.0526e-05, -8.9726e-11]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.4124e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-6.7561e-06, -9.5203e-07,  2.8432e-07,  ...,  9.3736e-08,\n",
      "         2.2039e-05, -1.2594e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(6.3130, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(6.3131, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:54 | [trpo_pendulum] epoch #113 | Saving snapshot...\n",
      "2022-09-02 00:56:54 | [trpo_pendulum] epoch #113 | Saved\n",
      "2022-09-02 00:56:54 | [trpo_pendulum] epoch #113 | Time 123.55 s\n",
      "2022-09-02 00:56:54 | [trpo_pendulum] epoch #113 | EpochTime 0.77 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -80.9888\n",
      "Evaluation/AverageReturn              -492.762\n",
      "Evaluation/Iteration                   113\n",
      "Evaluation/MaxReturn                  -394.434\n",
      "Evaluation/MinReturn                  -591.09\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    98.3282\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       226344\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4553,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.5018)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-4.1973e+01, -2.7964e-04],\n",
      "        [-2.7964e-04,  3.0866e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.8472e-06, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-2.2388e-06, -1.9765e-07,  5.5291e-07,  ...,  2.7415e-08,\n",
      "         9.5043e-07,  2.8899e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(27.3388, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(27.3388, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:55 | [trpo_pendulum] epoch #114 | Saving snapshot...\n",
      "2022-09-02 00:56:55 | [trpo_pendulum] epoch #114 | Saved\n",
      "2022-09-02 00:56:55 | [trpo_pendulum] epoch #114 | Time 124.43 s\n",
      "2022-09-02 00:56:55 | [trpo_pendulum] epoch #114 | EpochTime 0.87 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -102.834\n",
      "Evaluation/AverageReturn              -705.241\n",
      "Evaluation/Iteration                   114\n",
      "Evaluation/MaxReturn                  -551.258\n",
      "Evaluation/MinReturn                  -859.224\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   153.983\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       227909\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(47.4942)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 4.1721e+02, -1.8289e-03],\n",
      "        [-1.8289e-03,  2.1072e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.4592e-07,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.0444e-06, -8.1559e-08,  2.1375e-07,  ...,  3.4741e-08,\n",
      "         1.0431e-06,  1.7733e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(54.7544, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(54.7544, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:56 | [trpo_pendulum] epoch #115 | Saving snapshot...\n",
      "2022-09-02 00:56:56 | [trpo_pendulum] epoch #115 | Saved\n",
      "2022-09-02 00:56:56 | [trpo_pendulum] epoch #115 | Time 125.57 s\n",
      "2022-09-02 00:56:56 | [trpo_pendulum] epoch #115 | EpochTime 1.14 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn    -104.379\n",
      "Evaluation/AverageReturn             -1165.94\n",
      "Evaluation/Iteration                   115\n",
      "Evaluation/MaxReturn                 -1107.29\n",
      "Evaluation/MinReturn                 -1224.59\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    58.6479\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       229907\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.2177)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 5.5048e+00, -8.9141e-06],\n",
      "        [-8.9141e-06, -3.6435e-11]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.2740e-06,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-6.9527e-07,  1.4581e-08,  2.0006e-07,  ...,  7.6977e-09,\n",
      "         6.2706e-07,  1.1904e-08])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-5.5042, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-5.5042, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:57 | [trpo_pendulum] epoch #116 | Saving snapshot...\n",
      "2022-09-02 00:56:57 | [trpo_pendulum] epoch #116 | Saved\n",
      "2022-09-02 00:56:57 | [trpo_pendulum] epoch #116 | Time 126.63 s\n",
      "2022-09-02 00:56:57 | [trpo_pendulum] epoch #116 | EpochTime 1.06 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn     -52.2475\n",
      "Evaluation/AverageReturn              -586.537\n",
      "Evaluation/Iteration                   116\n",
      "Evaluation/MaxReturn                  -578.352\n",
      "Evaluation/MinReturn                  -594.722\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     8.18504\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       231905\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.4253)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.7939e+00, -1.6126e-05],\n",
      "        [-1.6126e-05,  7.4835e-11]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([8.8858e-07, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.9442e-07, -2.5959e-08, -4.4665e-08,  ..., -3.6121e-08,\n",
      "        -5.1460e-07,  1.1606e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(1.3216, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(1.3216, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:56:59 | [trpo_pendulum] epoch #117 | Saving snapshot...\n",
      "2022-09-02 00:56:59 | [trpo_pendulum] epoch #117 | Saved\n",
      "2022-09-02 00:56:59 | [trpo_pendulum] epoch #117 | Time 127.88 s\n",
      "2022-09-02 00:56:59 | [trpo_pendulum] epoch #117 | EpochTime 1.25 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -44.3581\n",
      "Evaluation/AverageReturn              -643.798\n",
      "Evaluation/Iteration                   117\n",
      "Evaluation/MaxReturn                  -592.558\n",
      "Evaluation/MinReturn                  -695.038\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    51.2395\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       233903\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.0349)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-5.8059e+00, -2.9444e-06],\n",
      "        [-2.9444e-06,  5.4564e-12]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.8579e-06, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-6.5721e-08, -6.2560e-08,  8.6312e-08,  ..., -2.4648e-08,\n",
      "         1.6286e-06,  5.5726e-08])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-19.3275, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-19.3275, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:00 | [trpo_pendulum] epoch #118 | Saving snapshot...\n",
      "2022-09-02 00:57:00 | [trpo_pendulum] epoch #118 | Saved\n",
      "2022-09-02 00:57:00 | [trpo_pendulum] epoch #118 | Time 129.01 s\n",
      "2022-09-02 00:57:00 | [trpo_pendulum] epoch #118 | EpochTime 1.12 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -39.8489\n",
      "Evaluation/AverageReturn              -434.136\n",
      "Evaluation/Iteration                   118\n",
      "Evaluation/MaxReturn                  -382.654\n",
      "Evaluation/MinReturn                  -485.619\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    51.4825\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       235901\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.1793)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[1.7173e+02, 2.8814e-06],\n",
      "        [2.8814e-06, 5.9262e-11]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-4.1204e-06,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 6.7582e-07, -3.2977e-07, -8.9289e-07,  ...,  2.6555e-07,\n",
      "         7.1174e-06,  2.2972e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-18.2335, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-18.2335, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:01 | [trpo_pendulum] epoch #119 | Saving snapshot...\n",
      "2022-09-02 00:57:01 | [trpo_pendulum] epoch #119 | Saved\n",
      "2022-09-02 00:57:01 | [trpo_pendulum] epoch #119 | Time 130.23 s\n",
      "2022-09-02 00:57:01 | [trpo_pendulum] epoch #119 | EpochTime 1.22 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -37.1765\n",
      "Evaluation/AverageReturn              -421.4\n",
      "Evaluation/Iteration                   119\n",
      "Evaluation/MaxReturn                  -397.905\n",
      "Evaluation/MinReturn                  -444.894\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    23.4948\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       237899\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.8660)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.9000e+01,  1.3787e-04],\n",
      "        [ 1.3787e-04,  5.1181e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.2575e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.9300e-06, -8.4593e-07,  6.1366e-07,  ...,  2.3414e-07,\n",
      "         2.6479e-05, -3.9991e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(20.7936, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(20.7937, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:02 | [trpo_pendulum] epoch #120 | Saving snapshot...\n",
      "2022-09-02 00:57:02 | [trpo_pendulum] epoch #120 | Saved\n",
      "2022-09-02 00:57:02 | [trpo_pendulum] epoch #120 | Time 131.30 s\n",
      "2022-09-02 00:57:02 | [trpo_pendulum] epoch #120 | EpochTime 1.07 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -91.8485\n",
      "Evaluation/AverageReturn              -842.505\n",
      "Evaluation/Iteration                   120\n",
      "Evaluation/MaxReturn                  -612.795\n",
      "Evaluation/MinReturn                 -1072.22\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   229.71\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       239897\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.5797)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.0222e+02, -4.9478e-04],\n",
      "        [-4.9478e-04, -9.3393e-10]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-7.5705e-07,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.1532e-06, -4.3526e-07, -1.5352e-08,  ...,  1.8405e-07,\n",
      "         1.3747e-05, -1.3353e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-20.8421, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-20.8421, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:03 | [trpo_pendulum] epoch #121 | Saving snapshot...\n",
      "2022-09-02 00:57:03 | [trpo_pendulum] epoch #121 | Saved\n",
      "2022-09-02 00:57:03 | [trpo_pendulum] epoch #121 | Time 132.35 s\n",
      "2022-09-02 00:57:03 | [trpo_pendulum] epoch #121 | EpochTime 1.04 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn     -36.4423\n",
      "Evaluation/AverageReturn              -396.442\n",
      "Evaluation/Iteration                   121\n",
      "Evaluation/MaxReturn                  -390.156\n",
      "Evaluation/MinReturn                  -402.728\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     6.28637\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       241895\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.2429)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.7055e+01, -8.0260e-05],\n",
      "        [-8.0260e-05,  1.6461e-10]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([3.4637e-07, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 6.4310e-07, -2.2196e-07, -9.3140e-08,  ...,  9.3493e-08,\n",
      "         6.9032e-06, -3.8392e-08])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-17.5374, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-17.5374, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:04 | [trpo_pendulum] epoch #122 | Saving snapshot...\n",
      "2022-09-02 00:57:04 | [trpo_pendulum] epoch #122 | Saved\n",
      "2022-09-02 00:57:04 | [trpo_pendulum] epoch #122 | Time 133.49 s\n",
      "2022-09-02 00:57:04 | [trpo_pendulum] epoch #122 | EpochTime 1.14 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn     -40.684\n",
      "Evaluation/AverageReturn              -409.277\n",
      "Evaluation/Iteration                   122\n",
      "Evaluation/MaxReturn                  -402.678\n",
      "Evaluation/MinReturn                  -415.876\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     6.59899\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       243893\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(30.5522)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 9.5897e-01,  3.8209e-04],\n",
      "        [ 3.8209e-04, -1.0529e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.4626e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.6729e-05,  1.9586e-06, -7.6660e-07,  ...,  1.7950e-06,\n",
      "        -4.4776e-05,  4.1109e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(57.0378, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(57.0374, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:05 | [trpo_pendulum] epoch #123 | Saving snapshot...\n",
      "2022-09-02 00:57:05 | [trpo_pendulum] epoch #123 | Saved\n",
      "2022-09-02 00:57:05 | [trpo_pendulum] epoch #123 | Time 134.58 s\n",
      "2022-09-02 00:57:05 | [trpo_pendulum] epoch #123 | EpochTime 1.08 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -113.6\n",
      "Evaluation/AverageReturn             -1135.71\n",
      "Evaluation/Iteration                   123\n",
      "Evaluation/MaxReturn                  -991.175\n",
      "Evaluation/MinReturn                 -1280.24\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   144.531\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       245778\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(9.1191)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.5157e+02,  9.4410e-04],\n",
      "        [ 9.4410e-04, -9.2950e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.6452e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 9.1357e-06, -9.9017e-07, -3.1328e-08,  ..., -8.3434e-07,\n",
      "         2.4622e-05, -1.8490e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(3.3213, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(3.3213, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:06 | [trpo_pendulum] epoch #124 | Saving snapshot...\n",
      "2022-09-02 00:57:06 | [trpo_pendulum] epoch #124 | Saved\n",
      "2022-09-02 00:57:06 | [trpo_pendulum] epoch #124 | Time 135.61 s\n",
      "2022-09-02 00:57:06 | [trpo_pendulum] epoch #124 | EpochTime 1.03 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -45.5242\n",
      "Evaluation/AverageReturn              -645.38\n",
      "Evaluation/Iteration                   124\n",
      "Evaluation/MaxReturn                  -486.422\n",
      "Evaluation/MinReturn                  -804.338\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   158.958\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       247776\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(15.5008)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.3905e+02, -5.5005e-04],\n",
      "        [-5.5005e-04, -3.6499e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-7.0850e-07,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.2464e-06, -6.7276e-07,  1.4603e-07,  ..., -4.4677e-07,\n",
      "         1.0568e-05, -1.0921e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(10.9926, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(10.9926, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:07 | [trpo_pendulum] epoch #125 | Saving snapshot...\n",
      "2022-09-02 00:57:07 | [trpo_pendulum] epoch #125 | Saved\n",
      "2022-09-02 00:57:07 | [trpo_pendulum] epoch #125 | Time 136.70 s\n",
      "2022-09-02 00:57:07 | [trpo_pendulum] epoch #125 | EpochTime 1.08 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -43.6119\n",
      "Evaluation/AverageReturn              -723.535\n",
      "Evaluation/Iteration                   125\n",
      "Evaluation/MaxReturn                  -522.052\n",
      "Evaluation/MinReturn                  -925.018\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   201.483\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       249774\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.6968)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-4.6111e+01, -2.7978e-05],\n",
      "        [-2.7978e-05, -5.2886e-10]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.3513e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-2.2978e-06,  1.6627e-07,  9.8889e-08,  ...,  1.6025e-07,\n",
      "        -6.9934e-06,  6.0785e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-20.3085, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-20.3085, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:08 | [trpo_pendulum] epoch #126 | Saving snapshot...\n",
      "2022-09-02 00:57:09 | [trpo_pendulum] epoch #126 | Saved\n",
      "2022-09-02 00:57:09 | [trpo_pendulum] epoch #126 | Time 137.81 s\n",
      "2022-09-02 00:57:09 | [trpo_pendulum] epoch #126 | EpochTime 1.11 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -39.739\n",
      "Evaluation/AverageReturn              -416.243\n",
      "Evaluation/Iteration                   126\n",
      "Evaluation/MaxReturn                  -377.988\n",
      "Evaluation/MinReturn                  -454.499\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    38.2558\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       251772\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(9.0191)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.5615e+01, -5.4287e-05],\n",
      "        [-5.4287e-05, -5.1479e-11]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([2.3568e-06, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-9.7054e-07, -2.0387e-07,  8.9429e-07,  ...,  3.2639e-08,\n",
      "         5.4677e-07,  2.8878e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-2.2704, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-2.2704, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:10 | [trpo_pendulum] epoch #127 | Saving snapshot...\n",
      "2022-09-02 00:57:10 | [trpo_pendulum] epoch #127 | Saved\n",
      "2022-09-02 00:57:10 | [trpo_pendulum] epoch #127 | Time 138.96 s\n",
      "2022-09-02 00:57:10 | [trpo_pendulum] epoch #127 | EpochTime 1.15 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -51.0311\n",
      "Evaluation/AverageReturn              -588.878\n",
      "Evaluation/Iteration                   127\n",
      "Evaluation/MaxReturn                  -406.049\n",
      "Evaluation/MinReturn                  -771.707\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   182.829\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       253770\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(9.5468)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 2.1337e+01, -2.1050e-04],\n",
      "        [-2.1050e-04,  7.6982e-10]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.8835e-06, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.0325e-06,  4.9638e-08,  4.3826e-07,  ...,  7.3722e-08,\n",
      "        -2.4943e-06, -1.8513e-08])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(17.8378, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(17.8378, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:11 | [trpo_pendulum] epoch #128 | Saving snapshot...\n",
      "2022-09-02 00:57:11 | [trpo_pendulum] epoch #128 | Saved\n",
      "2022-09-02 00:57:11 | [trpo_pendulum] epoch #128 | Time 140.06 s\n",
      "2022-09-02 00:57:11 | [trpo_pendulum] epoch #128 | EpochTime 1.09 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -76.6616\n",
      "Evaluation/AverageReturn              -812.083\n",
      "Evaluation/Iteration                   128\n",
      "Evaluation/MaxReturn                  -686.877\n",
      "Evaluation/MinReturn                  -937.29\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   125.207\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       255768\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(23.9523)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-9.4119e+00,  1.1704e-05],\n",
      "        [ 1.1704e-05, -1.6704e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([3.0482e-04, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-2.4846e-04, -2.5220e-06, -1.1823e-06,  ..., -1.0317e-05,\n",
      "         6.5513e-04,  2.2438e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(33.6112, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(33.6148, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:12 | [trpo_pendulum] epoch #129 | Saving snapshot...\n",
      "2022-09-02 00:57:12 | [trpo_pendulum] epoch #129 | Saved\n",
      "2022-09-02 00:57:12 | [trpo_pendulum] epoch #129 | Time 141.20 s\n",
      "2022-09-02 00:57:12 | [trpo_pendulum] epoch #129 | EpochTime 1.14 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -81.3225\n",
      "Evaluation/AverageReturn              -996.368\n",
      "Evaluation/Iteration                   129\n",
      "Evaluation/MaxReturn                  -859.309\n",
      "Evaluation/MinReturn                 -1133.43\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   137.059\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       257766\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6821,  1.9649,  0.7112,  ...,  0.2548, -1.4552,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6824,  1.9649,  0.7112,  ...,  0.2547, -1.4545,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.6039)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.2059e+01, -2.6083e-03],\n",
      "        [-2.6083e-03, -2.7210e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.0451e-04, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.6161e-04, -5.1166e-07,  1.2003e-05,  ..., -6.2769e-06,\n",
      "         3.0328e-04,  6.4613e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-4.7420, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-4.7417, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:13 | [trpo_pendulum] epoch #130 | Saving snapshot...\n",
      "2022-09-02 00:57:13 | [trpo_pendulum] epoch #130 | Saved\n",
      "2022-09-02 00:57:13 | [trpo_pendulum] epoch #130 | Time 142.24 s\n",
      "2022-09-02 00:57:13 | [trpo_pendulum] epoch #130 | EpochTime 1.04 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -53.0012\n",
      "Evaluation/AverageReturn              -622.104\n",
      "Evaluation/Iteration                   130\n",
      "Evaluation/MaxReturn                  -576.808\n",
      "Evaluation/MinReturn                  -667.399\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    45.2953\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       259764\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6824,  1.9649,  0.7112,  ...,  0.2547, -1.4545,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6825,  1.9649,  0.7112,  ...,  0.2547, -1.4542,  1.1977])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.8045)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-7.9983e+01, -5.7853e-03],\n",
      "        [-5.7853e-03, -9.8220e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0014, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.4208e-03,  1.1826e-05,  4.4804e-04,  ..., -2.9336e-05,\n",
      "        -1.3703e-04, -1.4457e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-18.3087, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-18.3040, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:14 | [trpo_pendulum] epoch #131 | Saving snapshot...\n",
      "2022-09-02 00:57:14 | [trpo_pendulum] epoch #131 | Saved\n",
      "2022-09-02 00:57:14 | [trpo_pendulum] epoch #131 | Time 143.31 s\n",
      "2022-09-02 00:57:14 | [trpo_pendulum] epoch #131 | EpochTime 1.07 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -36.7907\n",
      "Evaluation/AverageReturn              -466.187\n",
      "Evaluation/Iteration                   131\n",
      "Evaluation/MaxReturn                  -444.082\n",
      "Evaluation/MinReturn                  -488.293\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    22.1055\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       261762\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6825,  1.9649,  0.7112,  ...,  0.2547, -1.4542,  1.1977])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6840,  1.9649,  0.7116,  ...,  0.2547, -1.4544,  1.1976])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(1.9607)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[5.3976e+01, 3.5670e-02],\n",
      "        [3.5670e-02, 9.8389e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0013,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-8.5102e-04,  7.5638e-05,  1.1172e-04,  ..., -3.0839e-04,\n",
      "        -5.9433e-03,  3.2700e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-18.2956, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-18.2971, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:15 | [trpo_pendulum] epoch #132 | Saving snapshot...\n",
      "2022-09-02 00:57:15 | [trpo_pendulum] epoch #132 | Saved\n",
      "2022-09-02 00:57:15 | [trpo_pendulum] epoch #132 | Time 144.29 s\n",
      "2022-09-02 00:57:15 | [trpo_pendulum] epoch #132 | EpochTime 0.97 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -50.7726\n",
      "Evaluation/AverageReturn              -456.312\n",
      "Evaluation/Iteration                   132\n",
      "Evaluation/MaxReturn                  -430.723\n",
      "Evaluation/MinReturn                  -481.9\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    25.5884\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       263760\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6840,  1.9649,  0.7116,  ...,  0.2547, -1.4544,  1.1976])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6848,  1.9650,  0.7117,  ...,  0.2544, -1.4603,  1.1979])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.1955)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.7467e+02, -9.0701e-03],\n",
      "        [-9.0701e-03, -9.5907e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0007,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0004,  0.0002, -0.0002,  ..., -0.0003, -0.0068,  0.0004])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-19.8569, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-19.8578, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:16 | [trpo_pendulum] epoch #133 | Saving snapshot...\n",
      "2022-09-02 00:57:16 | [trpo_pendulum] epoch #133 | Saved\n",
      "2022-09-02 00:57:16 | [trpo_pendulum] epoch #133 | Time 145.32 s\n",
      "2022-09-02 00:57:16 | [trpo_pendulum] epoch #133 | EpochTime 1.03 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -39.4334\n",
      "Evaluation/AverageReturn              -402.698\n",
      "Evaluation/Iteration                   133\n",
      "Evaluation/MaxReturn                  -390.932\n",
      "Evaluation/MinReturn                  -414.464\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    11.7658\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       265758\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6848,  1.9650,  0.7117,  ...,  0.2544, -1.4603,  1.1979])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6852,  1.9652,  0.7115,  ...,  0.2541, -1.4671,  1.1983])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.3803)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-5.6088e+00,  2.0488e-02],\n",
      "        [ 2.0488e-02, -1.0120e-04]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0024, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-4.2518e-04, -2.4433e-05,  1.7351e-04,  ..., -1.7067e-04,\n",
      "        -3.0247e-03,  1.2932e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-9.2310, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-9.2298, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:17 | [trpo_pendulum] epoch #134 | Saving snapshot...\n",
      "2022-09-02 00:57:17 | [trpo_pendulum] epoch #134 | Saved\n",
      "2022-09-02 00:57:17 | [trpo_pendulum] epoch #134 | Time 146.41 s\n",
      "2022-09-02 00:57:17 | [trpo_pendulum] epoch #134 | EpochTime 1.09 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn     -47.7306\n",
      "Evaluation/AverageReturn              -486.204\n",
      "Evaluation/Iteration                   134\n",
      "Evaluation/MaxReturn                  -484.122\n",
      "Evaluation/MinReturn                  -488.286\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     2.08162\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       267756\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.6852,  1.9652,  0.7115,  ...,  0.2541, -1.4671,  1.1983])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6857,  1.9651,  0.7117,  ...,  0.2539, -1.4701,  1.1985])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(8.7425)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-4.1549e+02, -3.5892e-01],\n",
      "        [-3.5892e-01, -2.0427e-03]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0006, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-3.8651e-04,  1.4656e-05, -1.0713e-04,  ...,  1.1041e-05,\n",
      "        -3.6572e-04,  7.2913e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(22.6624, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(22.6789, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:18 | [trpo_pendulum] epoch #135 | Saving snapshot...\n",
      "2022-09-02 00:57:18 | [trpo_pendulum] epoch #135 | Saved\n",
      "2022-09-02 00:57:18 | [trpo_pendulum] epoch #135 | Time 147.56 s\n",
      "2022-09-02 00:57:18 | [trpo_pendulum] epoch #135 | EpochTime 1.14 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -75.8757\n",
      "Evaluation/AverageReturn              -815.734\n",
      "Evaluation/Iteration                   135\n",
      "Evaluation/MaxReturn                  -499.708\n",
      "Evaluation/MinReturn                 -1131.76\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   316.026\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       269754\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6857,  1.9651,  0.7117,  ...,  0.2539, -1.4701,  1.1985])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6860,  1.9651,  0.7116,  ...,  0.2539, -1.4705,  1.1985])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.8618)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-4.7488e+00, -1.9694e-02],\n",
      "        [-1.9694e-02, -8.7849e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0023, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-4.2186e-04, -4.5215e-05, -2.4626e-04,  ...,  2.5147e-04,\n",
      "         2.5324e-03, -2.3162e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(4.1425, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(4.1435, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:19 | [trpo_pendulum] epoch #136 | Saving snapshot...\n",
      "2022-09-02 00:57:19 | [trpo_pendulum] epoch #136 | Saved\n",
      "2022-09-02 00:57:19 | [trpo_pendulum] epoch #136 | Time 148.58 s\n",
      "2022-09-02 00:57:19 | [trpo_pendulum] epoch #136 | EpochTime 1.02 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -50.0316\n",
      "Evaluation/AverageReturn              -639.346\n",
      "Evaluation/Iteration                   136\n",
      "Evaluation/MaxReturn                  -615.125\n",
      "Evaluation/MinReturn                  -663.567\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    24.2211\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       271752\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6860,  1.9651,  0.7116,  ...,  0.2539, -1.4705,  1.1985])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6865,  1.9651,  0.7114,  ...,  0.2542, -1.4680,  1.1983])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.9489)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.1686e+02,  1.9455e-02],\n",
      "        [ 1.9455e-02,  9.3983e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([2.0275e-04, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.2684e-04, -3.7358e-05, -6.7387e-05,  ...,  1.2572e-04,\n",
      "         1.3283e-03, -1.3380e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-19.1089, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-19.1091, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:20 | [trpo_pendulum] epoch #137 | Saving snapshot...\n",
      "2022-09-02 00:57:20 | [trpo_pendulum] epoch #137 | Saved\n",
      "2022-09-02 00:57:20 | [trpo_pendulum] epoch #137 | Time 149.70 s\n",
      "2022-09-02 00:57:20 | [trpo_pendulum] epoch #137 | EpochTime 1.12 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -41.6465\n",
      "Evaluation/AverageReturn              -405.429\n",
      "Evaluation/Iteration                   137\n",
      "Evaluation/MaxReturn                  -360.667\n",
      "Evaluation/MinReturn                  -450.192\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    44.7626\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       273750\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6865,  1.9651,  0.7114,  ...,  0.2542, -1.4680,  1.1983])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6866,  1.9651,  0.7113,  ...,  0.2543, -1.4666,  1.1982])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.7960)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 3.6011e+01,  3.0385e-03],\n",
      "        [ 3.0385e-03, -6.0935e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.0110e-04,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-6.1708e-05, -1.5983e-05, -4.3072e-05,  ...,  5.2817e-05,\n",
      "         5.1875e-04, -5.0237e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-11.6761, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-11.6760, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:21 | [trpo_pendulum] epoch #138 | Saving snapshot...\n",
      "2022-09-02 00:57:21 | [trpo_pendulum] epoch #138 | Saved\n",
      "2022-09-02 00:57:21 | [trpo_pendulum] epoch #138 | Time 150.75 s\n",
      "2022-09-02 00:57:21 | [trpo_pendulum] epoch #138 | EpochTime 1.05 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn     -58.2833\n",
      "Evaluation/AverageReturn              -465.678\n",
      "Evaluation/Iteration                   138\n",
      "Evaluation/MaxReturn                  -462.865\n",
      "Evaluation/MinReturn                  -468.49\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     2.81239\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       275748\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.6866,  1.9651,  0.7113,  ...,  0.2543, -1.4666,  1.1982])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6866,  1.9650,  0.7112,  ...,  0.2544, -1.4661,  1.1981])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.8505)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.4916e+01,  1.5431e-03],\n",
      "        [ 1.5431e-03, -1.4532e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([6.0862e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-9.6511e-06, -4.8952e-06, -5.9687e-06,  ...,  1.6042e-05,\n",
      "         9.3672e-05, -1.8163e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-4.4229, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-4.4229, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:23 | [trpo_pendulum] epoch #139 | Saving snapshot...\n",
      "2022-09-02 00:57:23 | [trpo_pendulum] epoch #139 | Saved\n",
      "2022-09-02 00:57:23 | [trpo_pendulum] epoch #139 | Time 152.03 s\n",
      "2022-09-02 00:57:23 | [trpo_pendulum] epoch #139 | EpochTime 1.28 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -39.6433\n",
      "Evaluation/AverageReturn              -498.762\n",
      "Evaluation/Iteration                   139\n",
      "Evaluation/MaxReturn                  -387.212\n",
      "Evaluation/MinReturn                  -610.312\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   111.55\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       277746\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6866,  1.9650,  0.7112,  ...,  0.2544, -1.4661,  1.1981])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6867,  1.9650,  0.7112,  ...,  0.2544, -1.4660,  1.1981])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.9058)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.0531e+00, -9.8818e-05],\n",
      "        [-9.8818e-05, -2.2546e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.6953e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-4.4602e-05,  8.6243e-06, -7.3223e-06,  ..., -6.2886e-06,\n",
      "        -1.1464e-04,  9.8764e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(2.2996, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(2.2998, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:24 | [trpo_pendulum] epoch #140 | Saving snapshot...\n",
      "2022-09-02 00:57:24 | [trpo_pendulum] epoch #140 | Saved\n",
      "2022-09-02 00:57:24 | [trpo_pendulum] epoch #140 | Time 153.04 s\n",
      "2022-09-02 00:57:24 | [trpo_pendulum] epoch #140 | EpochTime 1.00 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -42.6253\n",
      "Evaluation/AverageReturn              -559.165\n",
      "Evaluation/Iteration                   140\n",
      "Evaluation/MaxReturn                  -531.79\n",
      "Evaluation/MinReturn                  -586.541\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    27.3759\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       279744\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6867,  1.9650,  0.7112,  ...,  0.2544, -1.4660,  1.1981])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6867,  1.9651,  0.7112,  ...,  0.2544, -1.4661,  1.1981])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(10.7903)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.8972e+01,  5.0386e-03],\n",
      "        [ 5.0386e-03, -2.4434e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-2.4654e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 3.2923e-05, -6.0456e-07,  7.4539e-07,  ...,  1.8726e-06,\n",
      "         4.8763e-05, -2.1403e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(17.4276, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(17.4275, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:25 | [trpo_pendulum] epoch #141 | Saving snapshot...\n",
      "2022-09-02 00:57:25 | [trpo_pendulum] epoch #141 | Saved\n",
      "2022-09-02 00:57:25 | [trpo_pendulum] epoch #141 | Time 154.08 s\n",
      "2022-09-02 00:57:25 | [trpo_pendulum] epoch #141 | EpochTime 1.04 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -56.2238\n",
      "Evaluation/AverageReturn              -729.602\n",
      "Evaluation/Iteration                   141\n",
      "Evaluation/MaxReturn                  -483.099\n",
      "Evaluation/MinReturn                  -976.104\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   246.503\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       281742\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6867,  1.9651,  0.7112,  ...,  0.2544, -1.4661,  1.1981])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6867,  1.9651,  0.7112,  ...,  0.2544, -1.4661,  1.1981])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.4263)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.7620e+01, -2.6006e-04],\n",
      "        [-2.6006e-04, -8.0978e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.3469e-04,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 7.5670e-05, -1.0991e-05, -2.2205e-06,  ...,  1.0268e-05,\n",
      "         1.1089e-04, -1.0233e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-5.2766, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-5.2768, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:26 | [trpo_pendulum] epoch #142 | Saving snapshot...\n",
      "2022-09-02 00:57:26 | [trpo_pendulum] epoch #142 | Saved\n",
      "2022-09-02 00:57:26 | [trpo_pendulum] epoch #142 | Time 155.25 s\n",
      "2022-09-02 00:57:26 | [trpo_pendulum] epoch #142 | EpochTime 1.16 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -49.6793\n",
      "Evaluation/AverageReturn              -536.399\n",
      "Evaluation/Iteration                   142\n",
      "Evaluation/MaxReturn                  -469.118\n",
      "Evaluation/MinReturn                  -603.68\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    67.2811\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       283740\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6867,  1.9651,  0.7112,  ...,  0.2544, -1.4661,  1.1981])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6866,  1.9650,  0.7112,  ...,  0.2544, -1.4660,  1.1981])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(8.4876)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 9.0776e+00,  1.2531e-03],\n",
      "        [ 1.2531e-03, -2.0233e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-2.6297e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.8773e-05,  4.9197e-06, -2.5120e-06,  ..., -4.8823e-06,\n",
      "        -5.2309e-05,  7.2165e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(15.3489, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(15.3490, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:27 | [trpo_pendulum] epoch #143 | Saving snapshot...\n",
      "2022-09-02 00:57:27 | [trpo_pendulum] epoch #143 | Saved\n",
      "2022-09-02 00:57:27 | [trpo_pendulum] epoch #143 | Time 156.42 s\n",
      "2022-09-02 00:57:27 | [trpo_pendulum] epoch #143 | EpochTime 1.17 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -77.8975\n",
      "Evaluation/AverageReturn              -759.659\n",
      "Evaluation/Iteration                   143\n",
      "Evaluation/MaxReturn                  -519.489\n",
      "Evaluation/MinReturn                  -999.829\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   240.17\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       285738\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6866,  1.9650,  0.7112,  ...,  0.2544, -1.4660,  1.1981])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6866,  1.9650,  0.7112,  ...,  0.2544, -1.4660,  1.1981])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(10.9348)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 3.7062e+00,  1.8758e-03],\n",
      "        [ 1.8758e-03, -2.0499e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-5.2156e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 2.2528e-05,  3.4657e-08, -4.2293e-06,  ...,  5.6212e-06,\n",
      "         1.6301e-05, -7.2730e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(11.6062, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(11.6061, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:28 | [trpo_pendulum] epoch #144 | Saving snapshot...\n",
      "2022-09-02 00:57:28 | [trpo_pendulum] epoch #144 | Saved\n",
      "2022-09-02 00:57:28 | [trpo_pendulum] epoch #144 | Time 157.50 s\n",
      "2022-09-02 00:57:28 | [trpo_pendulum] epoch #144 | EpochTime 1.08 s\n",
      "----------------------------------  -------------\n",
      "Evaluation/AverageDiscountedReturn     -91.0854\n",
      "Evaluation/AverageReturn              -766.781\n",
      "Evaluation/Iteration                   144\n",
      "Evaluation/MaxReturn                  -766.224\n",
      "Evaluation/MinReturn                  -767.338\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     0.557244\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       287736\n",
      "----------------------------------  -------------\n",
      "old policy para is\n",
      "tensor([-0.6866,  1.9650,  0.7112,  ...,  0.2544, -1.4660,  1.1981])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6866,  1.9650,  0.7112,  ...,  0.2544, -1.4660,  1.1981])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.0934)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.3588e+01, -1.8365e-03],\n",
      "        [-1.8365e-03,  4.4035e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0008, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.3740e-04, -3.8915e-06,  1.0209e-04,  ...,  1.4705e-04,\n",
      "        -1.8586e-04, -2.5288e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(29.8535, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(29.8598, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:29 | [trpo_pendulum] epoch #145 | Saving snapshot...\n",
      "2022-09-02 00:57:29 | [trpo_pendulum] epoch #145 | Saved\n",
      "2022-09-02 00:57:29 | [trpo_pendulum] epoch #145 | Time 158.56 s\n",
      "2022-09-02 00:57:29 | [trpo_pendulum] epoch #145 | EpochTime 1.06 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -80.8697\n",
      "Evaluation/AverageReturn              -960.688\n",
      "Evaluation/Iteration                   145\n",
      "Evaluation/MaxReturn                  -676.676\n",
      "Evaluation/MinReturn                 -1244.7\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   284.012\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       289734\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6866,  1.9650,  0.7112,  ...,  0.2544, -1.4660,  1.1981])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6861,  1.9650,  0.7113,  ...,  0.2545, -1.4662,  1.1978])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.5786)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.0194e+02,  1.2931e-02],\n",
      "        [ 1.2931e-02,  5.5462e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([4.7718e-04, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 8.1863e-05, -8.2846e-05, -9.0501e-05,  ...,  1.3253e-04,\n",
      "        -1.7614e-05,  5.0723e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(27.7368, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(27.7378, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:30 | [trpo_pendulum] epoch #146 | Saving snapshot...\n",
      "2022-09-02 00:57:30 | [trpo_pendulum] epoch #146 | Saved\n",
      "2022-09-02 00:57:30 | [trpo_pendulum] epoch #146 | Time 159.56 s\n",
      "2022-09-02 00:57:30 | [trpo_pendulum] epoch #146 | EpochTime 0.99 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -104.585\n",
      "Evaluation/AverageReturn              -886.052\n",
      "Evaluation/Iteration                   146\n",
      "Evaluation/MaxReturn                  -702.279\n",
      "Evaluation/MinReturn                 -1069.82\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   183.773\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       291482\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6861,  1.9650,  0.7113,  ...,  0.2545, -1.4662,  1.1978])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6860,  1.9650,  0.7112,  ...,  0.2547, -1.4662,  1.1979])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.8291)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[1.1573e+02, 2.6886e-02],\n",
      "        [2.6886e-02, 1.1567e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-2.0333e-04,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-8.5339e-06, -4.0244e-05, -1.2789e-04,  ...,  2.9781e-05,\n",
      "        -7.2516e-04,  1.0579e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-17.8298, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-17.8306, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:31 | [trpo_pendulum] epoch #147 | Saving snapshot...\n",
      "2022-09-02 00:57:31 | [trpo_pendulum] epoch #147 | Saved\n",
      "2022-09-02 00:57:31 | [trpo_pendulum] epoch #147 | Time 160.63 s\n",
      "2022-09-02 00:57:31 | [trpo_pendulum] epoch #147 | EpochTime 1.07 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -39.0602\n",
      "Evaluation/AverageReturn              -512.989\n",
      "Evaluation/Iteration                   147\n",
      "Evaluation/MaxReturn                  -412.314\n",
      "Evaluation/MinReturn                  -613.664\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   100.675\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       293480\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6860,  1.9650,  0.7112,  ...,  0.2547, -1.4662,  1.1979])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6860,  1.9649,  0.7111,  ...,  0.2547, -1.4669,  1.1980])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(11.6255)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 4.4091e+01,  2.2404e-02],\n",
      "        [ 2.2404e-02, -5.5059e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0013, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.5508e-04,  1.1427e-04, -2.4586e-04,  ...,  7.4102e-05,\n",
      "         1.9976e-03, -1.0895e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-5.1614, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-5.1655, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:32 | [trpo_pendulum] epoch #148 | Saving snapshot...\n",
      "2022-09-02 00:57:32 | [trpo_pendulum] epoch #148 | Saved\n",
      "2022-09-02 00:57:32 | [trpo_pendulum] epoch #148 | Time 161.75 s\n",
      "2022-09-02 00:57:32 | [trpo_pendulum] epoch #148 | EpochTime 1.11 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -40.6672\n",
      "Evaluation/AverageReturn              -623.895\n",
      "Evaluation/Iteration                   148\n",
      "Evaluation/MaxReturn                  -599.196\n",
      "Evaluation/MinReturn                  -648.593\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    24.6984\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       295478\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6860,  1.9649,  0.7111,  ...,  0.2547, -1.4669,  1.1980])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6854,  1.9650,  0.7109,  ...,  0.2548, -1.4649,  1.1979])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(1.3023)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.2077e+00,  7.3163e-03],\n",
      "        [ 7.3163e-03,  1.9675e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0013, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.1960e-04, -3.8826e-05,  3.2382e-04,  ...,  1.6300e-04,\n",
      "         2.4921e-03, -2.8788e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-16.2292, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-16.2278, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:33 | [trpo_pendulum] epoch #149 | Saving snapshot...\n",
      "2022-09-02 00:57:33 | [trpo_pendulum] epoch #149 | Saved\n",
      "2022-09-02 00:57:33 | [trpo_pendulum] epoch #149 | Time 162.77 s\n",
      "2022-09-02 00:57:33 | [trpo_pendulum] epoch #149 | EpochTime 1.02 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -47.0974\n",
      "Evaluation/AverageReturn              -512.907\n",
      "Evaluation/Iteration                   149\n",
      "Evaluation/MaxReturn                  -415.525\n",
      "Evaluation/MinReturn                  -610.29\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    97.3824\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       297476\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6854,  1.9650,  0.7109,  ...,  0.2548, -1.4649,  1.1979])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6853,  1.9650,  0.7112,  ...,  0.2549, -1.4624,  1.1976])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(11.1700)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.3124e+02, -6.1695e-01],\n",
      "        [-6.1695e-01,  1.8661e-03]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0016, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.4502e-03, -5.8836e-04,  5.8080e-05,  ..., -1.1741e-04,\n",
      "         1.8580e-03, -3.1608e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(27.6812, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(27.7236, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:34 | [trpo_pendulum] epoch #150 | Saving snapshot...\n",
      "2022-09-02 00:57:34 | [trpo_pendulum] epoch #150 | Saved\n",
      "2022-09-02 00:57:34 | [trpo_pendulum] epoch #150 | Time 163.43 s\n",
      "2022-09-02 00:57:34 | [trpo_pendulum] epoch #150 | EpochTime 0.66 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -103.927\n",
      "Evaluation/AverageReturn              -515.308\n",
      "Evaluation/Iteration                   150\n",
      "Evaluation/MaxReturn                  -403.148\n",
      "Evaluation/MinReturn                  -627.468\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   112.16\n",
      "Evaluation/TerminationRate               1\n",
      "TotalEnvSteps                       298539\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6853,  1.9650,  0.7112,  ...,  0.2549, -1.4624,  1.1976])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6839,  1.9644,  0.7112,  ...,  0.2548, -1.4606,  1.1973])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(1.9729)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-7.8098e+01,  6.6483e-03],\n",
      "        [ 6.6483e-03, -1.6454e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([4.9306e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 7.4698e-04, -2.9616e-04,  4.9097e-05,  ..., -6.1879e-05,\n",
      "         9.3751e-04, -1.6245e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-27.2829, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-27.2814, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:35 | [trpo_pendulum] epoch #151 | Saving snapshot...\n",
      "2022-09-02 00:57:35 | [trpo_pendulum] epoch #151 | Saved\n",
      "2022-09-02 00:57:35 | [trpo_pendulum] epoch #151 | Time 164.47 s\n",
      "2022-09-02 00:57:35 | [trpo_pendulum] epoch #151 | EpochTime 1.03 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -34.8728\n",
      "Evaluation/AverageReturn              -384.998\n",
      "Evaluation/Iteration                   151\n",
      "Evaluation/MaxReturn                  -365.773\n",
      "Evaluation/MinReturn                  -404.223\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    19.2249\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       300537\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6839,  1.9644,  0.7112,  ...,  0.2548, -1.4606,  1.1973])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6831,  1.9641,  0.7113,  ...,  0.2547, -1.4596,  1.1971])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(8.7928)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-9.0385e+01, -3.2903e-02],\n",
      "        [-3.2903e-02,  7.4336e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-6.0544e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 3.5903e-04, -1.5004e-04,  4.6144e-05,  ..., -2.8098e-05,\n",
      "         5.4737e-04, -9.2092e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-0.0984, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-0.0993, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:36 | [trpo_pendulum] epoch #152 | Saving snapshot...\n",
      "2022-09-02 00:57:36 | [trpo_pendulum] epoch #152 | Saved\n",
      "2022-09-02 00:57:36 | [trpo_pendulum] epoch #152 | Time 165.55 s\n",
      "2022-09-02 00:57:36 | [trpo_pendulum] epoch #152 | EpochTime 1.08 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn     -56.12\n",
      "Evaluation/AverageReturn              -646.634\n",
      "Evaluation/Iteration                   152\n",
      "Evaluation/MaxReturn                  -510.568\n",
      "Evaluation/MinReturn                  -782.701\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   136.066\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       302535\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6831,  1.9641,  0.7113,  ...,  0.2547, -1.4596,  1.1971])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6827,  1.9640,  0.7113,  ...,  0.2547, -1.4591,  1.1970])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.3357)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-5.9425e+01,  1.8575e-02],\n",
      "        [ 1.8575e-02,  2.8136e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.4737e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-6.1769e-05,  6.4480e-05, -7.8378e-05,  ...,  3.9089e-05,\n",
      "        -3.9734e-04,  4.0286e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(11.6781, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(11.6789, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:37 | [trpo_pendulum] epoch #153 | Saving snapshot...\n",
      "2022-09-02 00:57:37 | [trpo_pendulum] epoch #153 | Saved\n",
      "2022-09-02 00:57:37 | [trpo_pendulum] epoch #153 | Time 166.66 s\n",
      "2022-09-02 00:57:37 | [trpo_pendulum] epoch #153 | EpochTime 1.11 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -68.3419\n",
      "Evaluation/AverageReturn              -775.409\n",
      "Evaluation/Iteration                   153\n",
      "Evaluation/MaxReturn                  -634.73\n",
      "Evaluation/MinReturn                  -916.089\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   140.679\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       304533\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6827,  1.9640,  0.7113,  ...,  0.2547, -1.4591,  1.1970])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6828,  1.9640,  0.7113,  ...,  0.2548, -1.4595,  1.1971])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(0.9789)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 5.4107e+00, -1.8593e-03],\n",
      "        [-1.8593e-03,  9.1940e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([8.0508e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-3.8899e-05,  2.9179e-05, -4.2588e-05,  ...,  2.7681e-05,\n",
      "        -1.0648e-04,  1.0479e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-10.9950, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-10.9948, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:38 | [trpo_pendulum] epoch #154 | Saving snapshot...\n",
      "2022-09-02 00:57:39 | [trpo_pendulum] epoch #154 | Saved\n",
      "2022-09-02 00:57:39 | [trpo_pendulum] epoch #154 | Time 167.83 s\n",
      "2022-09-02 00:57:39 | [trpo_pendulum] epoch #154 | EpochTime 1.16 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -56.7282\n",
      "Evaluation/AverageReturn              -555.472\n",
      "Evaluation/Iteration                   154\n",
      "Evaluation/MaxReturn                  -530.368\n",
      "Evaluation/MinReturn                  -580.576\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    25.1042\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       306531\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6828,  1.9640,  0.7113,  ...,  0.2548, -1.4595,  1.1971])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6828,  1.9641,  0.7112,  ...,  0.2548, -1.4596,  1.1971])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.6232)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.7518e+02,  3.9755e-03],\n",
      "        [ 3.9755e-03, -1.6175e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-3.6226e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.7412e-05, -1.4256e-05,  2.1180e-05,  ..., -1.2309e-05,\n",
      "         5.4638e-05, -7.2496e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(20.5637, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(20.5638, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:40 | [trpo_pendulum] epoch #155 | Saving snapshot...\n",
      "2022-09-02 00:57:40 | [trpo_pendulum] epoch #155 | Saved\n",
      "2022-09-02 00:57:40 | [trpo_pendulum] epoch #155 | Time 168.88 s\n",
      "2022-09-02 00:57:40 | [trpo_pendulum] epoch #155 | EpochTime 1.05 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn     -55.0027\n",
      "Evaluation/AverageReturn              -849.995\n",
      "Evaluation/Iteration                   155\n",
      "Evaluation/MaxReturn                  -845.026\n",
      "Evaluation/MinReturn                  -854.963\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     4.96856\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       308529\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.6828,  1.9641,  0.7112,  ...,  0.2548, -1.4596,  1.1971])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6828,  1.9640,  0.7112,  ...,  0.2548, -1.4595,  1.1971])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.2433)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 2.0531e+01, -8.8408e-04],\n",
      "        [-8.8408e-04,  8.2750e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-3.4397e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 7.2416e-06,  1.4129e-05, -7.5004e-06,  ...,  1.2558e-05,\n",
      "        -1.4368e-04, -3.8925e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(14.2813, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(14.2811, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:41 | [trpo_pendulum] epoch #156 | Saving snapshot...\n",
      "2022-09-02 00:57:41 | [trpo_pendulum] epoch #156 | Saved\n",
      "2022-09-02 00:57:41 | [trpo_pendulum] epoch #156 | Time 170.01 s\n",
      "2022-09-02 00:57:41 | [trpo_pendulum] epoch #156 | EpochTime 1.12 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn     -69.431\n",
      "Evaluation/AverageReturn              -835.797\n",
      "Evaluation/Iteration                   156\n",
      "Evaluation/MaxReturn                  -691.438\n",
      "Evaluation/MinReturn                  -980.155\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   144.358\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       310527\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6828,  1.9640,  0.7112,  ...,  0.2548, -1.4595,  1.1971])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6828,  1.9641,  0.7112,  ...,  0.2548, -1.4597,  1.1971])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.7349)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 2.8312e-01, -6.5114e-05],\n",
      "        [-6.5114e-05,  6.7911e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0007, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.1986e-05,  4.6083e-06,  3.5116e-05,  ...,  3.6395e-06,\n",
      "        -2.7637e-04, -1.8262e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-17.4618, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-17.4625, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:42 | [trpo_pendulum] epoch #157 | Saving snapshot...\n",
      "2022-09-02 00:57:42 | [trpo_pendulum] epoch #157 | Saved\n",
      "2022-09-02 00:57:42 | [trpo_pendulum] epoch #157 | Time 171.08 s\n",
      "2022-09-02 00:57:42 | [trpo_pendulum] epoch #157 | EpochTime 1.07 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn     -36.614\n",
      "Evaluation/AverageReturn              -508.786\n",
      "Evaluation/Iteration                   157\n",
      "Evaluation/MaxReturn                  -373.285\n",
      "Evaluation/MinReturn                  -644.288\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   135.501\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       312525\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6828,  1.9641,  0.7112,  ...,  0.2548, -1.4597,  1.1971])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6828,  1.9641,  0.7113,  ...,  0.2548, -1.4600,  1.1971])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.3965)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 6.7503e-01,  2.0751e-03],\n",
      "        [ 2.0751e-03, -3.4893e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-4.5483e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.5978e-05, -6.4962e-06, -2.1563e-05,  ...,  2.4245e-06,\n",
      "         2.5229e-04,  6.3666e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-27.7036, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-27.7036, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:43 | [trpo_pendulum] epoch #158 | Saving snapshot...\n",
      "2022-09-02 00:57:43 | [trpo_pendulum] epoch #158 | Saved\n",
      "2022-09-02 00:57:43 | [trpo_pendulum] epoch #158 | Time 172.14 s\n",
      "2022-09-02 00:57:43 | [trpo_pendulum] epoch #158 | EpochTime 1.06 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -37.995\n",
      "Evaluation/AverageReturn              -382.507\n",
      "Evaluation/Iteration                   158\n",
      "Evaluation/MaxReturn                  -365.666\n",
      "Evaluation/MinReturn                  -399.348\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    16.8409\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       314523\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6828,  1.9641,  0.7113,  ...,  0.2548, -1.4600,  1.1971])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6828,  1.9641,  0.7112,  ...,  0.2548, -1.4597,  1.1971])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.0762)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.1505e+01, -1.2635e-04],\n",
      "        [-1.2635e-04, -4.1135e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.3374e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 6.6714e-05,  7.1070e-06,  4.5592e-05,  ..., -1.5674e-05,\n",
      "        -3.1613e-04, -1.6663e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(1.3450, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(1.3453, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:44 | [trpo_pendulum] epoch #159 | Saving snapshot...\n",
      "2022-09-02 00:57:44 | [trpo_pendulum] epoch #159 | Saved\n",
      "2022-09-02 00:57:44 | [trpo_pendulum] epoch #159 | Time 173.25 s\n",
      "2022-09-02 00:57:44 | [trpo_pendulum] epoch #159 | EpochTime 1.10 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -81.3264\n",
      "Evaluation/AverageReturn              -677.354\n",
      "Evaluation/Iteration                   159\n",
      "Evaluation/MaxReturn                  -665.68\n",
      "Evaluation/MinReturn                  -689.028\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    11.6737\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       316521\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6828,  1.9641,  0.7112,  ...,  0.2548, -1.4597,  1.1971])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6827,  1.9641,  0.7113,  ...,  0.2548, -1.4600,  1.1971])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(17.0546)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.8899e+03,  1.7665e-01],\n",
      "        [ 1.7665e-01, -9.8242e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-2.8138e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-5.6867e-05, -7.4420e-06,  3.8408e-06,  ...,  1.7512e-05,\n",
      "         3.7999e-04, -2.1042e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-20.5718, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-20.5721, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:45 | [trpo_pendulum] epoch #160 | Saving snapshot...\n",
      "2022-09-02 00:57:45 | [trpo_pendulum] epoch #160 | Saved\n",
      "2022-09-02 00:57:45 | [trpo_pendulum] epoch #160 | Time 174.28 s\n",
      "2022-09-02 00:57:45 | [trpo_pendulum] epoch #160 | EpochTime 1.03 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -36.7536\n",
      "Evaluation/AverageReturn              -415.525\n",
      "Evaluation/Iteration                   160\n",
      "Evaluation/MaxReturn                  -364.923\n",
      "Evaluation/MinReturn                  -466.126\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    50.6019\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       318519\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6827,  1.9641,  0.7113,  ...,  0.2548, -1.4600,  1.1971])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6828,  1.9641,  0.7113,  ...,  0.2548, -1.4597,  1.1970])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.3985)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[1.6146e+02, 2.1436e-02],\n",
      "        [2.1436e-02, 1.9946e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.0265e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 2.6996e-05,  6.7311e-06,  5.3570e-07,  ..., -2.3084e-06,\n",
      "        -1.3560e-04,  4.8164e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-0.7162, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-0.7167, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:46 | [trpo_pendulum] epoch #161 | Saving snapshot...\n",
      "2022-09-02 00:57:46 | [trpo_pendulum] epoch #161 | Saved\n",
      "2022-09-02 00:57:46 | [trpo_pendulum] epoch #161 | Time 175.41 s\n",
      "2022-09-02 00:57:46 | [trpo_pendulum] epoch #161 | EpochTime 1.13 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -39.1589\n",
      "Evaluation/AverageReturn              -582.071\n",
      "Evaluation/Iteration                   161\n",
      "Evaluation/MaxReturn                  -361.245\n",
      "Evaluation/MinReturn                  -802.898\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   220.827\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       320517\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6828,  1.9641,  0.7113,  ...,  0.2548, -1.4597,  1.1970])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6828,  1.9641,  0.7113,  ...,  0.2548, -1.4598,  1.1970])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.5381)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.5153e+01, -4.0148e-03],\n",
      "        [-4.0148e-03, -1.7923e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-2.0171e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 2.8559e-05,  1.0634e-05,  5.6701e-06,  ...,  5.2197e-06,\n",
      "        -5.4710e-05,  8.4409e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(2.0141, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(2.0140, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:47 | [trpo_pendulum] epoch #162 | Saving snapshot...\n",
      "2022-09-02 00:57:47 | [trpo_pendulum] epoch #162 | Saved\n",
      "2022-09-02 00:57:47 | [trpo_pendulum] epoch #162 | Time 176.36 s\n",
      "2022-09-02 00:57:47 | [trpo_pendulum] epoch #162 | EpochTime 0.95 s\n",
      "----------------------------------  -------------\n",
      "Evaluation/AverageDiscountedReturn     -84.0013\n",
      "Evaluation/AverageReturn              -396.146\n",
      "Evaluation/Iteration                   162\n",
      "Evaluation/MaxReturn                  -368.074\n",
      "Evaluation/MinReturn                  -429.276\n",
      "Evaluation/NumEpisodes                   3\n",
      "Evaluation/StdReturn                    25.2403\n",
      "Evaluation/TerminationRate               0.666667\n",
      "TotalEnvSteps                       322321\n",
      "----------------------------------  -------------\n",
      "old policy para is\n",
      "tensor([-0.6828,  1.9641,  0.7113,  ...,  0.2548, -1.4598,  1.1970])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6827,  1.9641,  0.7113,  ...,  0.2548, -1.4598,  1.1971])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(1.4032)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[5.0446e+01, 5.9526e-04],\n",
      "        [5.9526e-04, 3.6458e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 8.6011e-07, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.4142e-05, -5.3576e-06, -2.4606e-06,  ..., -2.6301e-06,\n",
      "         2.6833e-05, -4.3373e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-19.1692, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-19.1692, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:48 | [trpo_pendulum] epoch #163 | Saving snapshot...\n",
      "2022-09-02 00:57:48 | [trpo_pendulum] epoch #163 | Saved\n",
      "2022-09-02 00:57:48 | [trpo_pendulum] epoch #163 | Time 177.52 s\n",
      "2022-09-02 00:57:48 | [trpo_pendulum] epoch #163 | EpochTime 1.16 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -40.1234\n",
      "Evaluation/AverageReturn              -402.442\n",
      "Evaluation/Iteration                   163\n",
      "Evaluation/MaxReturn                  -375.27\n",
      "Evaluation/MinReturn                  -429.614\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    27.172\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       324319\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6827,  1.9641,  0.7113,  ...,  0.2548, -1.4598,  1.1971])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6827,  1.9641,  0.7113,  ...,  0.2548, -1.4598,  1.1970])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.4984)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[5.9786e+00, 2.5558e-04],\n",
      "        [2.5558e-04, 1.1291e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-2.3031e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.3577e-05, -3.3140e-06,  1.3085e-06,  ..., -4.0547e-06,\n",
      "        -1.0595e-05,  5.3635e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(26.3126, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(26.3126, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:49 | [trpo_pendulum] epoch #164 | Saving snapshot...\n",
      "2022-09-02 00:57:49 | [trpo_pendulum] epoch #164 | Saved\n",
      "2022-09-02 00:57:49 | [trpo_pendulum] epoch #164 | Time 178.57 s\n",
      "2022-09-02 00:57:49 | [trpo_pendulum] epoch #164 | EpochTime 1.04 s\n",
      "----------------------------------  -------------\n",
      "Evaluation/AverageDiscountedReturn     -62.0797\n",
      "Evaluation/AverageReturn              -847.315\n",
      "Evaluation/Iteration                   164\n",
      "Evaluation/MaxReturn                  -846.926\n",
      "Evaluation/MinReturn                  -847.703\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     0.388188\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       326317\n",
      "----------------------------------  -------------\n",
      "old policy para is\n",
      "tensor([-0.6827,  1.9641,  0.7113,  ...,  0.2548, -1.4598,  1.1970])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6828,  1.9641,  0.7113,  ...,  0.2548, -1.4598,  1.1970])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(21.2014)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-8.8574e+01, -9.0523e-04],\n",
      "        [-9.0523e-04, -8.7411e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 9.5932e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-2.2420e-04,  6.2841e-07, -1.4254e-05,  ..., -1.0970e-05,\n",
      "        -1.3587e-04,  6.6450e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(38.0909, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(38.0931, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:50 | [trpo_pendulum] epoch #165 | Saving snapshot...\n",
      "2022-09-02 00:57:50 | [trpo_pendulum] epoch #165 | Saved\n",
      "2022-09-02 00:57:50 | [trpo_pendulum] epoch #165 | Time 179.52 s\n",
      "2022-09-02 00:57:50 | [trpo_pendulum] epoch #165 | EpochTime 0.95 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -99.1193\n",
      "Evaluation/AverageReturn              -825.538\n",
      "Evaluation/Iteration                   165\n",
      "Evaluation/MaxReturn                  -626.959\n",
      "Evaluation/MinReturn                 -1024.12\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   198.579\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       327909\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6828,  1.9641,  0.7113,  ...,  0.2548, -1.4598,  1.1970])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6830,  1.9641,  0.7113,  ...,  0.2548, -1.4600,  1.1970])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.5500)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-4.5536e+01,  8.2934e-04],\n",
      "        [ 8.2934e-04,  2.4631e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.3894e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([1.1030e-04, 2.1414e-06, 8.4523e-06,  ..., 2.8223e-06, 2.3687e-05,\n",
      "        1.4785e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-22.4432, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-22.4433, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:51 | [trpo_pendulum] epoch #166 | Saving snapshot...\n",
      "2022-09-02 00:57:51 | [trpo_pendulum] epoch #166 | Saved\n",
      "2022-09-02 00:57:51 | [trpo_pendulum] epoch #166 | Time 180.51 s\n",
      "2022-09-02 00:57:51 | [trpo_pendulum] epoch #166 | EpochTime 0.98 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -37.275\n",
      "Evaluation/AverageReturn              -403.381\n",
      "Evaluation/Iteration                   166\n",
      "Evaluation/MaxReturn                  -361.471\n",
      "Evaluation/MinReturn                  -445.292\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    41.9108\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       329907\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6830,  1.9641,  0.7113,  ...,  0.2548, -1.4600,  1.1970])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6829,  1.9641,  0.7113,  ...,  0.2548, -1.4599,  1.1970])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.2247)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.2737e+02, -5.5112e-04],\n",
      "        [-5.5112e-04, -8.0095e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 6.1994e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-5.8997e-05, -7.0408e-07, -1.7747e-06,  ..., -1.8864e-06,\n",
      "        -1.9427e-05, -1.4832e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-20.4497, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-20.4496, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:52 | [trpo_pendulum] epoch #167 | Saving snapshot...\n",
      "2022-09-02 00:57:52 | [trpo_pendulum] epoch #167 | Saved\n",
      "2022-09-02 00:57:52 | [trpo_pendulum] epoch #167 | Time 181.67 s\n",
      "2022-09-02 00:57:52 | [trpo_pendulum] epoch #167 | EpochTime 1.16 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -39.9897\n",
      "Evaluation/AverageReturn              -406.735\n",
      "Evaluation/Iteration                   167\n",
      "Evaluation/MaxReturn                  -396.134\n",
      "Evaluation/MinReturn                  -417.337\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    10.6018\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       331905\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6829,  1.9641,  0.7113,  ...,  0.2548, -1.4599,  1.1970])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6829,  1.9641,  0.7113,  ...,  0.2548, -1.4600,  1.1970])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(9.0984)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.6341e+01, -9.6825e-05],\n",
      "        [-9.6825e-05,  6.5406e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.6968e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 2.8334e-05,  5.3527e-07,  2.6459e-06,  ...,  1.8448e-06,\n",
      "         3.1495e-05, -1.1626e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-9.6197, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-9.6197, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:53 | [trpo_pendulum] epoch #168 | Saving snapshot...\n",
      "2022-09-02 00:57:53 | [trpo_pendulum] epoch #168 | Saved\n",
      "2022-09-02 00:57:53 | [trpo_pendulum] epoch #168 | Time 182.78 s\n",
      "2022-09-02 00:57:53 | [trpo_pendulum] epoch #168 | EpochTime 1.10 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -38.4661\n",
      "Evaluation/AverageReturn              -491.693\n",
      "Evaluation/Iteration                   168\n",
      "Evaluation/MaxReturn                  -438.305\n",
      "Evaluation/MinReturn                  -545.081\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    53.3879\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       333903\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6829,  1.9641,  0.7113,  ...,  0.2548, -1.4600,  1.1970])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6829,  1.9641,  0.7113,  ...,  0.2548, -1.4599,  1.1970])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.4367)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.8996e+02, -6.4209e-04],\n",
      "        [-6.4209e-04, -1.3224e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.1201e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.1158e-05, -5.6053e-07,  3.3915e-06,  ..., -8.8647e-07,\n",
      "         1.2930e-05,  1.4045e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(37.3816, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(37.3819, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:54 | [trpo_pendulum] epoch #169 | Saving snapshot...\n",
      "2022-09-02 00:57:54 | [trpo_pendulum] epoch #169 | Saved\n",
      "2022-09-02 00:57:54 | [trpo_pendulum] epoch #169 | Time 183.74 s\n",
      "2022-09-02 00:57:54 | [trpo_pendulum] epoch #169 | EpochTime 0.96 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -83.7008\n",
      "Evaluation/AverageReturn              -993.075\n",
      "Evaluation/Iteration                   169\n",
      "Evaluation/MaxReturn                  -944.814\n",
      "Evaluation/MinReturn                 -1041.33\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    48.2603\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       335901\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6829,  1.9641,  0.7113,  ...,  0.2548, -1.4599,  1.1970])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6829,  1.9641,  0.7113,  ...,  0.2548, -1.4599,  1.1970])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.3060)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-5.9950e+00,  2.6638e-05],\n",
      "        [ 2.6638e-05,  2.4354e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.3787e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.5829e-06, -2.8568e-09,  3.2463e-06,  ..., -2.0940e-07,\n",
      "         1.1160e-05, -3.1720e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-11.5466, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-11.5466, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:56 | [trpo_pendulum] epoch #170 | Saving snapshot...\n",
      "2022-09-02 00:57:56 | [trpo_pendulum] epoch #170 | Saved\n",
      "2022-09-02 00:57:56 | [trpo_pendulum] epoch #170 | Time 184.87 s\n",
      "2022-09-02 00:57:56 | [trpo_pendulum] epoch #170 | EpochTime 1.12 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn     -39.6715\n",
      "Evaluation/AverageReturn              -500.957\n",
      "Evaluation/Iteration                   170\n",
      "Evaluation/MaxReturn                  -499.625\n",
      "Evaluation/MinReturn                  -502.288\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     1.33156\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       337899\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.6829,  1.9641,  0.7113,  ...,  0.2548, -1.4599,  1.1970])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6829,  1.9641,  0.7113,  ...,  0.2548, -1.4599,  1.1970])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.7654)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 4.1274e+00, -2.7116e-05],\n",
      "        [-2.7116e-05, -9.7557e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.9562e-04,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.0152e-04, -5.9932e-06,  8.3116e-06,  ...,  1.8869e-06,\n",
      "         1.1040e-04, -2.6154e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-11.0429, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-11.0433, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:57 | [trpo_pendulum] epoch #171 | Saving snapshot...\n",
      "2022-09-02 00:57:57 | [trpo_pendulum] epoch #171 | Saved\n",
      "2022-09-02 00:57:57 | [trpo_pendulum] epoch #171 | Time 186.00 s\n",
      "2022-09-02 00:57:57 | [trpo_pendulum] epoch #171 | EpochTime 1.13 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -42.4464\n",
      "Evaluation/AverageReturn              -498.164\n",
      "Evaluation/Iteration                   171\n",
      "Evaluation/MaxReturn                  -397.135\n",
      "Evaluation/MinReturn                  -599.193\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   101.029\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       339897\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6829,  1.9641,  0.7113,  ...,  0.2548, -1.4599,  1.1970])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6830,  1.9641,  0.7113,  ...,  0.2548, -1.4598,  1.1970])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(12.7345)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.8489e+00,  3.9995e-04],\n",
      "        [ 3.9995e-04,  5.7635e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0016, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.7832e-04, -2.5903e-05, -4.7958e-05,  ..., -8.8043e-05,\n",
      "        -1.2353e-03,  1.1482e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-11.2904, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-11.2859, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:58 | [trpo_pendulum] epoch #172 | Saving snapshot...\n",
      "2022-09-02 00:57:58 | [trpo_pendulum] epoch #172 | Saved\n",
      "2022-09-02 00:57:58 | [trpo_pendulum] epoch #172 | Time 187.16 s\n",
      "2022-09-02 00:57:58 | [trpo_pendulum] epoch #172 | EpochTime 1.16 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -35.3202\n",
      "Evaluation/AverageReturn              -474.826\n",
      "Evaluation/Iteration                   172\n",
      "Evaluation/MaxReturn                  -347.565\n",
      "Evaluation/MinReturn                  -602.086\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   127.26\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       341895\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6830,  1.9641,  0.7113,  ...,  0.2548, -1.4598,  1.1970])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6828,  1.9640,  0.7113,  ...,  0.2547, -1.4610,  1.1972])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(8.4174)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.5214e+01, -5.2722e-02],\n",
      "        [-5.2722e-02, -3.8097e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-2.6885e-04,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 2.2476e-04, -1.5692e-05, -3.7323e-05,  ..., -9.4487e-06,\n",
      "        -3.1835e-04,  3.9200e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(27.1577, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(27.1571, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:57:59 | [trpo_pendulum] epoch #173 | Saving snapshot...\n",
      "2022-09-02 00:57:59 | [trpo_pendulum] epoch #173 | Saved\n",
      "2022-09-02 00:57:59 | [trpo_pendulum] epoch #173 | Time 188.17 s\n",
      "2022-09-02 00:57:59 | [trpo_pendulum] epoch #173 | EpochTime 1.01 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -77.2076\n",
      "Evaluation/AverageReturn              -883.741\n",
      "Evaluation/Iteration                   173\n",
      "Evaluation/MaxReturn                  -877.708\n",
      "Evaluation/MinReturn                  -889.774\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     6.0329\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       343893\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6828,  1.9640,  0.7113,  ...,  0.2547, -1.4610,  1.1972])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6826,  1.9640,  0.7112,  ...,  0.2547, -1.4613,  1.1972])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.5821)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 7.0402e+00,  5.8711e-03],\n",
      "        [ 5.8711e-03, -5.8801e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0007, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-4.6246e-04, -2.4220e-05,  1.5778e-05,  ...,  1.7504e-05,\n",
      "        -1.4772e-04, -2.4731e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(14.9328, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(14.9307, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:00 | [trpo_pendulum] epoch #174 | Saving snapshot...\n",
      "2022-09-02 00:58:00 | [trpo_pendulum] epoch #174 | Saved\n",
      "2022-09-02 00:58:00 | [trpo_pendulum] epoch #174 | Time 189.24 s\n",
      "2022-09-02 00:58:00 | [trpo_pendulum] epoch #174 | EpochTime 1.06 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -69.0136\n",
      "Evaluation/AverageReturn              -789.527\n",
      "Evaluation/Iteration                   174\n",
      "Evaluation/MaxReturn                  -728.1\n",
      "Evaluation/MinReturn                  -850.953\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    61.4267\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       345891\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6826,  1.9640,  0.7112,  ...,  0.2547, -1.4613,  1.1972])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6830,  1.9640,  0.7112,  ...,  0.2547, -1.4615,  1.1972])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(1.2449)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.0523e+00, -2.9296e-03],\n",
      "        [-2.9296e-03, -4.8955e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0008, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.8550e-04,  7.4573e-05, -2.0316e-04,  ..., -2.0636e-05,\n",
      "        -6.6365e-04,  8.6103e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-4.3521, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-4.3504, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:01 | [trpo_pendulum] epoch #175 | Saving snapshot...\n",
      "2022-09-02 00:58:01 | [trpo_pendulum] epoch #175 | Saved\n",
      "2022-09-02 00:58:01 | [trpo_pendulum] epoch #175 | Time 190.33 s\n",
      "2022-09-02 00:58:01 | [trpo_pendulum] epoch #175 | EpochTime 1.09 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -44.9792\n",
      "Evaluation/AverageReturn              -592.578\n",
      "Evaluation/Iteration                   175\n",
      "Evaluation/MaxReturn                  -475.46\n",
      "Evaluation/MinReturn                  -709.695\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   117.117\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       347889\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6830,  1.9640,  0.7112,  ...,  0.2547, -1.4615,  1.1972])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6825,  1.9641,  0.7110,  ...,  0.2547, -1.4622,  1.1973])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.5374)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.8338e+00, -2.1657e-02],\n",
      "        [-2.1657e-02, -4.7909e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-2.3522e-04,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 4.5407e-04,  1.5795e-04, -1.5753e-04,  ..., -3.2325e-05,\n",
      "        -6.7124e-04, -3.3440e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(37.8377, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(37.8313, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:02 | [trpo_pendulum] epoch #176 | Saving snapshot...\n",
      "2022-09-02 00:58:02 | [trpo_pendulum] epoch #176 | Saved\n",
      "2022-09-02 00:58:02 | [trpo_pendulum] epoch #176 | Time 191.31 s\n",
      "2022-09-02 00:58:02 | [trpo_pendulum] epoch #176 | EpochTime 0.97 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -93.0726\n",
      "Evaluation/AverageReturn             -1032.13\n",
      "Evaluation/Iteration                   176\n",
      "Evaluation/MaxReturn                  -995.398\n",
      "Evaluation/MinReturn                 -1068.86\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    36.7324\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       349880\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6825,  1.9641,  0.7110,  ...,  0.2547, -1.4622,  1.1973])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6820,  1.9642,  0.7109,  ...,  0.2546, -1.4628,  1.1973])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.4945)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-8.5052e+01,  1.4485e-03],\n",
      "        [ 1.4485e-03,  6.9535e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0014, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0008, -0.0002,  0.0001,  ..., -0.0001,  0.0023,  0.0004])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(21.1637, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(21.1827, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:03 | [trpo_pendulum] epoch #177 | Saving snapshot...\n",
      "2022-09-02 00:58:03 | [trpo_pendulum] epoch #177 | Saved\n",
      "2022-09-02 00:58:03 | [trpo_pendulum] epoch #177 | Time 192.31 s\n",
      "2022-09-02 00:58:03 | [trpo_pendulum] epoch #177 | EpochTime 1.00 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -85.9348\n",
      "Evaluation/AverageReturn              -815.903\n",
      "Evaluation/Iteration                   177\n",
      "Evaluation/MaxReturn                  -751.383\n",
      "Evaluation/MinReturn                  -880.422\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    64.52\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       351641\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6820,  1.9642,  0.7109,  ...,  0.2546, -1.4628,  1.1973])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6828,  1.9640,  0.7110,  ...,  0.2545, -1.4605,  1.1976])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.3613)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 4.9813e+01,  7.5387e-02],\n",
      "        [ 7.5387e-02, -6.3602e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 2.8211e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 3.7559e-04,  1.0029e-04, -6.1872e-05,  ...,  9.5307e-05,\n",
      "        -8.0954e-04, -2.4466e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-12.9841, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-12.9811, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:04 | [trpo_pendulum] epoch #178 | Saving snapshot...\n",
      "2022-09-02 00:58:04 | [trpo_pendulum] epoch #178 | Saved\n",
      "2022-09-02 00:58:04 | [trpo_pendulum] epoch #178 | Time 193.34 s\n",
      "2022-09-02 00:58:04 | [trpo_pendulum] epoch #178 | EpochTime 1.03 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -51.5657\n",
      "Evaluation/AverageReturn              -567.649\n",
      "Evaluation/Iteration                   178\n",
      "Evaluation/MaxReturn                  -510.866\n",
      "Evaluation/MinReturn                  -624.432\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    56.7829\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       353639\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6828,  1.9640,  0.7110,  ...,  0.2545, -1.4605,  1.1976])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6824,  1.9641,  0.7109,  ...,  0.2546, -1.4613,  1.1974])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.1383)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.0807e+00, -1.6431e-03],\n",
      "        [-1.6431e-03, -7.5195e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0018, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-7.4095e-04, -5.5694e-06, -2.1997e-06,  ..., -3.6335e-04,\n",
      "        -4.2708e-03,  5.2169e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-22.5906, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-22.5878, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:05 | [trpo_pendulum] epoch #179 | Saving snapshot...\n",
      "2022-09-02 00:58:05 | [trpo_pendulum] epoch #179 | Saved\n",
      "2022-09-02 00:58:05 | [trpo_pendulum] epoch #179 | Time 194.39 s\n",
      "2022-09-02 00:58:05 | [trpo_pendulum] epoch #179 | EpochTime 1.04 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -41.4515\n",
      "Evaluation/AverageReturn              -449.617\n",
      "Evaluation/Iteration                   179\n",
      "Evaluation/MaxReturn                  -403.509\n",
      "Evaluation/MinReturn                  -495.725\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    46.1081\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       355637\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6824,  1.9641,  0.7109,  ...,  0.2546, -1.4613,  1.1974])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6831,  1.9641,  0.7109,  ...,  0.2543, -1.4656,  1.1979])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.7656)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[4.0201e+00, 1.5622e-02],\n",
      "        [1.5622e-02, 1.1527e-03]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0293,  0.4991])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0072, -0.0030,  0.0050,  ..., -0.0039,  0.0354, -0.0073])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(21.2175, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(21.0476, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:06 | [trpo_pendulum] epoch #180 | Saving snapshot...\n",
      "2022-09-02 00:58:06 | [trpo_pendulum] epoch #180 | Saved\n",
      "2022-09-02 00:58:06 | [trpo_pendulum] epoch #180 | Time 195.14 s\n",
      "2022-09-02 00:58:06 | [trpo_pendulum] epoch #180 | EpochTime 0.75 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -93.5461\n",
      "Evaluation/AverageReturn              -640.278\n",
      "Evaluation/Iteration                   180\n",
      "Evaluation/MaxReturn                  -299.194\n",
      "Evaluation/MinReturn                  -981.361\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   341.084\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       357013\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6831,  1.9641,  0.7109,  ...,  0.2543, -1.4656,  1.1979])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6903,  1.9611,  0.7159,  ...,  0.2504, -1.4302,  1.1906])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(12.9853)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 3.8127e+01, -9.1048e-01],\n",
      "        [-9.1048e-01,  2.3862e-02]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0130, -0.4998])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0032,  0.0028,  0.0007,  ...,  0.0022, -0.0182,  0.0029])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(27.9591, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(27.8760, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:07 | [trpo_pendulum] epoch #181 | Saving snapshot...\n",
      "2022-09-02 00:58:07 | [trpo_pendulum] epoch #181 | Saved\n",
      "2022-09-02 00:58:07 | [trpo_pendulum] epoch #181 | Time 196.08 s\n",
      "2022-09-02 00:58:07 | [trpo_pendulum] epoch #181 | EpochTime 0.94 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -79.8877\n",
      "Evaluation/AverageReturn              -856.491\n",
      "Evaluation/Iteration                   181\n",
      "Evaluation/MaxReturn                  -666.364\n",
      "Evaluation/MinReturn                 -1046.62\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   190.127\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       358766\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6903,  1.9611,  0.7159,  ...,  0.2504, -1.4302,  1.1906])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6871,  1.9639,  0.7166,  ...,  0.2526, -1.4484,  1.1935])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.3123)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[3.8706e+02, 5.5420e-01],\n",
      "        [5.5420e-01, 2.8809e-03]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0016,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0008,  0.0015, -0.0001,  ...,  0.0011, -0.0148,  0.0018])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-28.0325, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-28.0374, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:08 | [trpo_pendulum] epoch #182 | Saving snapshot...\n",
      "2022-09-02 00:58:08 | [trpo_pendulum] epoch #182 | Saved\n",
      "2022-09-02 00:58:08 | [trpo_pendulum] epoch #182 | Time 197.17 s\n",
      "2022-09-02 00:58:08 | [trpo_pendulum] epoch #182 | EpochTime 1.09 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -36.4312\n",
      "Evaluation/AverageReturn              -405.657\n",
      "Evaluation/Iteration                   182\n",
      "Evaluation/MaxReturn                  -372.228\n",
      "Evaluation/MinReturn                  -439.085\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    33.4287\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       360764\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6871,  1.9639,  0.7166,  ...,  0.2526, -1.4484,  1.1935])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6862,  1.9654,  0.7165,  ...,  0.2537, -1.4632,  1.1953])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.6083)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-7.2773e+00, -8.8114e-02],\n",
      "        [-8.8114e-02, -4.6799e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0009,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.1024e-04,  7.7530e-04,  2.9018e-05,  ...,  6.0726e-04,\n",
      "        -6.5304e-03,  7.7684e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-6.2996, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-6.3019, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:09 | [trpo_pendulum] epoch #183 | Saving snapshot...\n",
      "2022-09-02 00:58:09 | [trpo_pendulum] epoch #183 | Saved\n",
      "2022-09-02 00:58:09 | [trpo_pendulum] epoch #183 | Time 198.22 s\n",
      "2022-09-02 00:58:09 | [trpo_pendulum] epoch #183 | EpochTime 1.05 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -69.3148\n",
      "Evaluation/AverageReturn              -629.61\n",
      "Evaluation/Iteration                   183\n",
      "Evaluation/MaxReturn                  -519.495\n",
      "Evaluation/MinReturn                  -739.726\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   110.115\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       362762\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6862,  1.9654,  0.7165,  ...,  0.2537, -1.4632,  1.1953])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6861,  1.9661,  0.7165,  ...,  0.2543, -1.4698,  1.1961])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.9002)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-4.2270e+01,  1.1380e-01],\n",
      "        [ 1.1380e-01, -2.6777e-04]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0011, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0005, -0.0005, -0.0002,  ..., -0.0002,  0.0044, -0.0004])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-8.3172, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-8.3189, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:10 | [trpo_pendulum] epoch #184 | Saving snapshot...\n",
      "2022-09-02 00:58:10 | [trpo_pendulum] epoch #184 | Saved\n",
      "2022-09-02 00:58:10 | [trpo_pendulum] epoch #184 | Time 199.23 s\n",
      "2022-09-02 00:58:10 | [trpo_pendulum] epoch #184 | EpochTime 1.01 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -40.1932\n",
      "Evaluation/AverageReturn              -573.063\n",
      "Evaluation/Iteration                   184\n",
      "Evaluation/MaxReturn                  -423.816\n",
      "Evaluation/MinReturn                  -722.311\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   149.247\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       364760\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6861,  1.9661,  0.7165,  ...,  0.2543, -1.4698,  1.1961])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6866,  1.9656,  0.7163,  ...,  0.2541, -1.4654,  1.1957])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(3.7574)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-8.1973e+01,  8.7571e-03],\n",
      "        [ 8.7571e-03,  4.9119e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 7.2017e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 2.3318e-04,  2.4362e-04,  7.8313e-05,  ...,  1.0031e-04,\n",
      "        -2.2698e-03,  2.1642e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-9.4870, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-9.4882, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:11 | [trpo_pendulum] epoch #185 | Saving snapshot...\n",
      "2022-09-02 00:58:11 | [trpo_pendulum] epoch #185 | Saved\n",
      "2022-09-02 00:58:11 | [trpo_pendulum] epoch #185 | Time 200.38 s\n",
      "2022-09-02 00:58:11 | [trpo_pendulum] epoch #185 | EpochTime 1.14 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -44.9009\n",
      "Evaluation/AverageReturn              -554.266\n",
      "Evaluation/Iteration                   185\n",
      "Evaluation/MaxReturn                  -534.422\n",
      "Evaluation/MinReturn                  -574.109\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    19.8433\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       366758\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6866,  1.9656,  0.7163,  ...,  0.2541, -1.4654,  1.1957])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6864,  1.9659,  0.7164,  ...,  0.2542, -1.4677,  1.1959])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(4.2941)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.0410e+01,  1.5155e-02],\n",
      "        [ 1.5155e-02, -3.9034e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0011, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-8.8830e-04, -6.8037e-05, -1.2264e-04,  ..., -3.7550e-05,\n",
      "        -7.6326e-04,  2.4768e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(40.3172, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(40.3278, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:12 | [trpo_pendulum] epoch #186 | Saving snapshot...\n",
      "2022-09-02 00:58:12 | [trpo_pendulum] epoch #186 | Saved\n",
      "2022-09-02 00:58:12 | [trpo_pendulum] epoch #186 | Time 201.47 s\n",
      "2022-09-02 00:58:12 | [trpo_pendulum] epoch #186 | EpochTime 1.09 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -93.5754\n",
      "Evaluation/AverageReturn             -1083.24\n",
      "Evaluation/Iteration                   186\n",
      "Evaluation/MaxReturn                  -973.829\n",
      "Evaluation/MinReturn                 -1192.66\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   109.415\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       368756\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6864,  1.9659,  0.7164,  ...,  0.2542, -1.4677,  1.1959])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6872,  1.9658,  0.7163,  ...,  0.2541, -1.4684,  1.1962])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.1295)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.1918e+01, -4.5629e-03],\n",
      "        [-4.5629e-03, -4.3763e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-3.7270e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 3.2830e-04, -1.4439e-05,  8.1590e-05,  ...,  2.1477e-05,\n",
      "         5.0197e-04, -1.2758e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(27.8217, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(27.8233, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:13 | [trpo_pendulum] epoch #187 | Saving snapshot...\n",
      "2022-09-02 00:58:13 | [trpo_pendulum] epoch #187 | Saved\n",
      "2022-09-02 00:58:13 | [trpo_pendulum] epoch #187 | Time 202.51 s\n",
      "2022-09-02 00:58:13 | [trpo_pendulum] epoch #187 | EpochTime 1.04 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -65.6606\n",
      "Evaluation/AverageReturn              -979.465\n",
      "Evaluation/Iteration                   187\n",
      "Evaluation/MaxReturn                  -948.552\n",
      "Evaluation/MinReturn                 -1010.38\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    30.9127\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       370754\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6872,  1.9658,  0.7163,  ...,  0.2541, -1.4684,  1.1962])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6869,  1.9658,  0.7164,  ...,  0.2542, -1.4679,  1.1960])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(15.1712)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.9680e+01, -5.1590e-02],\n",
      "        [-5.1590e-02, -5.2599e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0005,  0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.3959e-03, -1.5761e-04,  2.0174e-05,  ..., -1.0875e-04,\n",
      "         1.4910e-03, -1.4580e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(24.4419, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(24.4183, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:14 | [trpo_pendulum] epoch #188 | Saving snapshot...\n",
      "2022-09-02 00:58:14 | [trpo_pendulum] epoch #188 | Saved\n",
      "2022-09-02 00:58:14 | [trpo_pendulum] epoch #188 | Time 203.15 s\n",
      "2022-09-02 00:58:14 | [trpo_pendulum] epoch #188 | EpochTime 0.64 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn    -108.961\n",
      "Evaluation/AverageReturn              -560.481\n",
      "Evaluation/Iteration                   188\n",
      "Evaluation/MaxReturn                  -338.422\n",
      "Evaluation/MinReturn                  -782.541\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   222.06\n",
      "Evaluation/TerminationRate               1\n",
      "TotalEnvSteps                       371837\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([-0.6869,  1.9658,  0.7164,  ...,  0.2542, -1.4679,  1.1960])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6855,  1.9656,  0.7164,  ...,  0.2541, -1.4664,  1.1959])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.7733)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 8.4371e+01, -4.8800e-02],\n",
      "        [-4.8800e-02,  8.5341e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.0883e-04, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 6.8314e-04, -7.8721e-05, -4.0196e-05,  ..., -4.4636e-05,\n",
      "         7.0299e-04, -6.3734e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-4.0484, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-4.0479, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:15 | [trpo_pendulum] epoch #189 | Saving snapshot...\n",
      "2022-09-02 00:58:15 | [trpo_pendulum] epoch #189 | Saved\n",
      "2022-09-02 00:58:15 | [trpo_pendulum] epoch #189 | Time 204.17 s\n",
      "2022-09-02 00:58:15 | [trpo_pendulum] epoch #189 | EpochTime 1.02 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -45.2398\n",
      "Evaluation/AverageReturn              -678.047\n",
      "Evaluation/Iteration                   189\n",
      "Evaluation/MaxReturn                  -594.635\n",
      "Evaluation/MinReturn                  -761.46\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    83.4126\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       373835\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6855,  1.9656,  0.7164,  ...,  0.2541, -1.4664,  1.1959])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6848,  1.9656,  0.7163,  ...,  0.2540, -1.4657,  1.1958])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(10.6875)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.8539e+01,  7.0374e-03],\n",
      "        [ 7.0374e-03,  1.1688e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 3.7020e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-2.1927e-04,  4.7214e-05,  7.7814e-05,  ...,  3.3291e-05,\n",
      "        -7.1193e-05, -1.1578e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-4.5629, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-4.5624, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:16 | [trpo_pendulum] epoch #190 | Saving snapshot...\n",
      "2022-09-02 00:58:16 | [trpo_pendulum] epoch #190 | Saved\n",
      "2022-09-02 00:58:16 | [trpo_pendulum] epoch #190 | Time 205.21 s\n",
      "2022-09-02 00:58:16 | [trpo_pendulum] epoch #190 | EpochTime 1.03 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -38.805\n",
      "Evaluation/AverageReturn              -663.496\n",
      "Evaluation/Iteration                   190\n",
      "Evaluation/MaxReturn                  -591.643\n",
      "Evaluation/MinReturn                  -735.349\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    71.8529\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       375833\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6848,  1.9656,  0.7163,  ...,  0.2540, -1.4657,  1.1958])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6851,  1.9656,  0.7164,  ...,  0.2540, -1.4658,  1.1958])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(10.4346)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.0364e+01,  7.3280e-03],\n",
      "        [ 7.3280e-03, -8.7576e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([3.6604e-04, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-5.7133e-04,  2.6162e-05,  2.2141e-05,  ..., -4.6561e-05,\n",
      "        -1.3490e-03,  8.2684e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-30.0871, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-30.0839, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:17 | [trpo_pendulum] epoch #191 | Saving snapshot...\n",
      "2022-09-02 00:58:17 | [trpo_pendulum] epoch #191 | Saved\n",
      "2022-09-02 00:58:17 | [trpo_pendulum] epoch #191 | Time 206.27 s\n",
      "2022-09-02 00:58:17 | [trpo_pendulum] epoch #191 | EpochTime 1.06 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -35.1179\n",
      "Evaluation/AverageReturn              -399.623\n",
      "Evaluation/Iteration                   191\n",
      "Evaluation/MaxReturn                  -371.332\n",
      "Evaluation/MinReturn                  -427.914\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    28.2911\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       377831\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6851,  1.9656,  0.7164,  ...,  0.2540, -1.4658,  1.1958])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6856,  1.9656,  0.7164,  ...,  0.2540, -1.4671,  1.1959])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(11.8186)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.1810e+02, -1.9826e-02],\n",
      "        [-1.9826e-02, -4.1882e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 3.4031e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 2.4683e-04,  1.2585e-04,  2.5826e-05,  ..., -2.1538e-05,\n",
      "         9.5252e-04,  5.7532e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(26.3394, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(26.3441, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:18 | [trpo_pendulum] epoch #192 | Saving snapshot...\n",
      "2022-09-02 00:58:18 | [trpo_pendulum] epoch #192 | Saved\n",
      "2022-09-02 00:58:18 | [trpo_pendulum] epoch #192 | Time 207.37 s\n",
      "2022-09-02 00:58:18 | [trpo_pendulum] epoch #192 | EpochTime 1.09 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -65.3027\n",
      "Evaluation/AverageReturn              -956.952\n",
      "Evaluation/Iteration                   192\n",
      "Evaluation/MaxReturn                  -750.609\n",
      "Evaluation/MinReturn                 -1163.3\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   206.343\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       379829\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6856,  1.9656,  0.7164,  ...,  0.2540, -1.4671,  1.1959])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6854,  1.9658,  0.7165,  ...,  0.2540, -1.4662,  1.1959])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.2184)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 3.9625e+01,  1.1886e-02],\n",
      "        [ 1.1886e-02, -2.7463e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-3.4471e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-3.0988e-04, -2.2312e-05,  9.6767e-06,  ...,  2.5346e-05,\n",
      "        -8.2952e-04,  5.1389e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(20.4360, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(20.4329, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:19 | [trpo_pendulum] epoch #193 | Saving snapshot...\n",
      "2022-09-02 00:58:19 | [trpo_pendulum] epoch #193 | Saved\n",
      "2022-09-02 00:58:19 | [trpo_pendulum] epoch #193 | Time 208.61 s\n",
      "2022-09-02 00:58:19 | [trpo_pendulum] epoch #193 | EpochTime 1.24 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -78.8391\n",
      "Evaluation/AverageReturn              -884.804\n",
      "Evaluation/Iteration                   193\n",
      "Evaluation/MaxReturn                  -858.23\n",
      "Evaluation/MinReturn                  -911.378\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    26.5742\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       381745\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6854,  1.9658,  0.7165,  ...,  0.2540, -1.4662,  1.1959])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6857,  1.9657,  0.7165,  ...,  0.2540, -1.4670,  1.1960])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.5702)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 4.6485e+01, -1.2549e-02],\n",
      "        [-1.2549e-02,  4.0239e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.8925e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.9593e-04,  5.8561e-06, -2.8066e-05,  ..., -2.2787e-05,\n",
      "         1.7391e-04, -3.7116e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-11.3370, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-11.3372, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:20 | [trpo_pendulum] epoch #194 | Saving snapshot...\n",
      "2022-09-02 00:58:20 | [trpo_pendulum] epoch #194 | Saved\n",
      "2022-09-02 00:58:20 | [trpo_pendulum] epoch #194 | Time 209.75 s\n",
      "2022-09-02 00:58:20 | [trpo_pendulum] epoch #194 | EpochTime 1.13 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -52.9908\n",
      "Evaluation/AverageReturn              -616.558\n",
      "Evaluation/Iteration                   194\n",
      "Evaluation/MaxReturn                  -486.783\n",
      "Evaluation/MinReturn                  -746.333\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   129.775\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       383743\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6857,  1.9657,  0.7165,  ...,  0.2540, -1.4670,  1.1960])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6855,  1.9657,  0.7164,  ...,  0.2540, -1.4668,  1.1960])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(5.2083)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-8.1927e+01, -1.6517e-03],\n",
      "        [-1.6517e-03,  4.0392e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.3155e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 9.1699e-05,  9.9281e-06, -8.4016e-06,  ..., -2.2259e-05,\n",
      "        -7.8027e-05,  8.0774e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-29.3979, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-29.3982, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:22 | [trpo_pendulum] epoch #195 | Saving snapshot...\n",
      "2022-09-02 00:58:22 | [trpo_pendulum] epoch #195 | Saved\n",
      "2022-09-02 00:58:22 | [trpo_pendulum] epoch #195 | Time 210.87 s\n",
      "2022-09-02 00:58:22 | [trpo_pendulum] epoch #195 | EpochTime 1.12 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -35.7753\n",
      "Evaluation/AverageReturn              -406.77\n",
      "Evaluation/Iteration                   195\n",
      "Evaluation/MaxReturn                  -372.483\n",
      "Evaluation/MinReturn                  -441.057\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    34.2866\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       385741\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6855,  1.9657,  0.7164,  ...,  0.2540, -1.4668,  1.1960])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6854,  1.9658,  0.7164,  ...,  0.2540, -1.4669,  1.1960])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.3349)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-8.6156e+01, -9.0710e-04],\n",
      "        [-9.0710e-04,  1.7606e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([9.7195e-06, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 4.6085e-05,  5.0285e-06, -4.2034e-06,  ..., -1.1463e-05,\n",
      "        -4.2086e-05,  4.6397e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-23.5411, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-23.5411, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:23 | [trpo_pendulum] epoch #196 | Saving snapshot...\n",
      "2022-09-02 00:58:23 | [trpo_pendulum] epoch #196 | Saved\n",
      "2022-09-02 00:58:23 | [trpo_pendulum] epoch #196 | Time 211.89 s\n",
      "2022-09-02 00:58:23 | [trpo_pendulum] epoch #196 | EpochTime 1.02 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -38.438\n",
      "Evaluation/AverageReturn              -432.855\n",
      "Evaluation/Iteration                   196\n",
      "Evaluation/MaxReturn                  -410.376\n",
      "Evaluation/MinReturn                  -455.334\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                    22.4788\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       387739\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6854,  1.9658,  0.7164,  ...,  0.2540, -1.4669,  1.1960])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6854,  1.9658,  0.7164,  ...,  0.2540, -1.4670,  1.1960])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(2.6203)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.6284e+00, -4.9149e-06],\n",
      "        [-4.9149e-06, -2.8743e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([3.0358e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.9548e-05,  3.8776e-06, -1.1754e-06,  ..., -6.8790e-06,\n",
      "        -4.9013e-05,  2.5683e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(-0.1304, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-0.1303, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:24 | [trpo_pendulum] epoch #197 | Saving snapshot...\n",
      "2022-09-02 00:58:24 | [trpo_pendulum] epoch #197 | Saved\n",
      "2022-09-02 00:58:24 | [trpo_pendulum] epoch #197 | Time 213.01 s\n",
      "2022-09-02 00:58:24 | [trpo_pendulum] epoch #197 | EpochTime 1.12 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -52.5235\n",
      "Evaluation/AverageReturn              -652.072\n",
      "Evaluation/Iteration                   197\n",
      "Evaluation/MaxReturn                  -518.217\n",
      "Evaluation/MinReturn                  -785.928\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   133.855\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       389737\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([-0.6854,  1.9658,  0.7164,  ...,  0.2540, -1.4670,  1.1960])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6853,  1.9658,  0.7164,  ...,  0.2539, -1.4670,  1.1960])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(6.8562)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.1435e+01, -2.2908e-03],\n",
      "        [-2.2908e-03,  2.2651e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([4.9359e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.3053e-05, -1.0002e-05,  1.6790e-05,  ...,  2.0318e-05,\n",
      "         7.1510e-05,  3.1493e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(3.3655, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(3.3660, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:25 | [trpo_pendulum] epoch #198 | Saving snapshot...\n",
      "2022-09-02 00:58:25 | [trpo_pendulum] epoch #198 | Saved\n",
      "2022-09-02 00:58:25 | [trpo_pendulum] epoch #198 | Time 214.02 s\n",
      "2022-09-02 00:58:25 | [trpo_pendulum] epoch #198 | EpochTime 1.01 s\n",
      "----------------------------------  ------------\n",
      "Evaluation/AverageDiscountedReturn     -39.0228\n",
      "Evaluation/AverageReturn              -623.405\n",
      "Evaluation/Iteration                   198\n",
      "Evaluation/MaxReturn                  -615.193\n",
      "Evaluation/MinReturn                  -631.617\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                     8.21208\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       391608\n",
      "----------------------------------  ------------\n",
      "old policy para is\n",
      "tensor([-0.6853,  1.9658,  0.7164,  ...,  0.2539, -1.4670,  1.1960])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([-0.6853,  1.9658,  0.7164,  ...,  0.2540, -1.4669,  1.1960])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(25.8848)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 5.5116e+01, -3.1388e-03],\n",
      "        [-3.1388e-03,  8.8763e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.2248e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-3.7493e-05,  6.3446e-06, -3.8983e-05,  ..., -3.8629e-05,\n",
      "        -2.7309e-04, -2.9191e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(25.0031, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(25.0006, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:26 | [trpo_pendulum] epoch #199 | Saving snapshot...\n",
      "2022-09-02 00:58:26 | [trpo_pendulum] epoch #199 | Saved\n",
      "2022-09-02 00:58:26 | [trpo_pendulum] epoch #199 | Time 214.86 s\n",
      "2022-09-02 00:58:26 | [trpo_pendulum] epoch #199 | EpochTime 0.84 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn     -73.4497\n",
      "Evaluation/AverageReturn              -774.171\n",
      "Evaluation/Iteration                   199\n",
      "Evaluation/MaxReturn                  -568.756\n",
      "Evaluation/MinReturn                  -979.586\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   205.415\n",
      "Evaluation/TerminationRate               0.5\n",
      "TotalEnvSteps                       393270\n",
      "----------------------------------  -----------\n",
      "2022-09-02 00:58:26 | [trpo_pendulum] Logging to drsom_test/momentum-opt-NPG_1\n",
      "2022-09-02 00:58:26 | [trpo_pendulum] Obtaining samples...\n",
      "old policy para is\n",
      "tensor([ 0.0000, -0.0881,  0.1847,  ..., -0.2844,  0.1172,  0.0000])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0000, -0.0881,  0.1847,  ..., -0.2844,  0.1172,  0.0000])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(0.6961)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 1.8410e+01, -2.7360e-08],\n",
      "        [-2.7360e-08,  1.1250e-15]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-5.0000e-01,  7.4308e-10])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0366, -0.3762,  0.0946,  ..., -0.2153,  0.0270,  0.4458])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(8.6532, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(5.7874, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:27 | [trpo_pendulum] epoch #0 | Saving snapshot...\n",
      "2022-09-02 00:58:27 | [trpo_pendulum] epoch #0 | Saved\n",
      "2022-09-02 00:58:27 | [trpo_pendulum] epoch #0 | Time 1.24 s\n",
      "2022-09-02 00:58:27 | [trpo_pendulum] epoch #0 | EpochTime 1.24 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -10.3497\n",
      "Evaluation/AverageReturn             -96.0911\n",
      "Evaluation/Iteration                   0\n",
      "Evaluation/MaxReturn                 -92.4846\n",
      "Evaluation/MinReturn                 -99.6977\n",
      "Evaluation/NumEpisodes                 2\n",
      "Evaluation/StdReturn                   3.60653\n",
      "Evaluation/TerminationRate             0\n",
      "TotalEnvSteps                       1998\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0000, -0.0881,  0.1847,  ..., -0.2844,  0.1172,  0.0000])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0366, -0.4643,  0.2793,  ..., -0.4997,  0.1442,  0.4458])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(18.3195)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.7195e+05,  6.7656e+02],\n",
      "        [ 6.7656e+02, -1.1888e+01]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0020, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0178, -0.1952, -0.0156,  ..., -0.0503, -0.0321,  0.2320])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(151.0381, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(116.9182, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:28 | [trpo_pendulum] epoch #1 | Saving snapshot...\n",
      "2022-09-02 00:58:28 | [trpo_pendulum] epoch #1 | Saved\n",
      "2022-09-02 00:58:28 | [trpo_pendulum] epoch #1 | Time 2.39 s\n",
      "2022-09-02 00:58:28 | [trpo_pendulum] epoch #1 | EpochTime 1.14 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -171.957\n",
      "Evaluation/AverageReturn            -1719.44\n",
      "Evaluation/Iteration                    1\n",
      "Evaluation/MaxReturn                -1691.48\n",
      "Evaluation/MinReturn                -1747.39\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   27.9554\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                        3996\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0366, -0.4643,  0.2793,  ..., -0.4997,  0.1442,  0.4458])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0544, -0.6595,  0.2637,  ..., -0.5500,  0.1121,  0.6778])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(103.2314)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-9.8012e+06,  4.9036e+03],\n",
      "        [ 4.9036e+03, -2.5722e+00]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([2.5024e-04, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0088, -0.0860,  0.0030,  ..., -0.0339, -0.0114,  0.1143])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(371.0933, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(381.4888, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:30 | [trpo_pendulum] epoch #2 | Saving snapshot...\n",
      "2022-09-02 00:58:30 | [trpo_pendulum] epoch #2 | Saved\n",
      "2022-09-02 00:58:30 | [trpo_pendulum] epoch #2 | Time 3.46 s\n",
      "2022-09-02 00:58:30 | [trpo_pendulum] epoch #2 | EpochTime 1.08 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -429.814\n",
      "Evaluation/AverageReturn            -4265.89\n",
      "Evaluation/Iteration                    2\n",
      "Evaluation/MaxReturn                -4248.02\n",
      "Evaluation/MinReturn                -4283.77\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   17.8716\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                        5994\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0544, -0.6595,  0.2637,  ..., -0.5500,  0.1121,  0.6778])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0633, -0.7456,  0.2667,  ..., -0.5838,  0.1007,  0.7921])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(27.4176)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.2295e+07,  4.2351e+02],\n",
      "        [ 4.2351e+02,  6.8244e+00]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([0.0005, 0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0057, -0.0758, -0.0185,  ...,  0.0144, -0.0174,  0.0720])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(506.1671, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(506.3063, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:31 | [trpo_pendulum] epoch #3 | Saving snapshot...\n",
      "2022-09-02 00:58:31 | [trpo_pendulum] epoch #3 | Saved\n",
      "2022-09-02 00:58:31 | [trpo_pendulum] epoch #3 | Time 4.53 s\n",
      "2022-09-02 00:58:31 | [trpo_pendulum] epoch #3 | EpochTime 1.06 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -582.144\n",
      "Evaluation/AverageReturn            -5822.79\n",
      "Evaluation/Iteration                    3\n",
      "Evaluation/MaxReturn                -5814.6\n",
      "Evaluation/MinReturn                -5830.98\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    8.18902\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                        7992\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([ 0.0633, -0.7456,  0.2667,  ..., -0.5838,  0.1007,  0.7921])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0689, -0.8214,  0.2482,  ..., -0.5694,  0.0833,  0.8642])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(326.5001)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[5.8816e+06, 6.7608e+03],\n",
      "        [6.7608e+03, 4.7886e+00]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 0.0006, -0.5000])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0026,  0.1288,  0.0415,  ..., -0.0101,  0.0140, -0.0382])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(587.6356, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(595.5900, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:32 | [trpo_pendulum] epoch #4 | Saving snapshot...\n",
      "2022-09-02 00:58:32 | [trpo_pendulum] epoch #4 | Saved\n",
      "2022-09-02 00:58:32 | [trpo_pendulum] epoch #4 | Time 5.57 s\n",
      "2022-09-02 00:58:32 | [trpo_pendulum] epoch #4 | EpochTime 1.04 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -676.033\n",
      "Evaluation/AverageReturn            -6764.94\n",
      "Evaluation/Iteration                    4\n",
      "Evaluation/MaxReturn                -6735.74\n",
      "Evaluation/MinReturn                -6794.14\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   29.1976\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                        9990\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0689, -0.8214,  0.2482,  ..., -0.5694,  0.0833,  0.8642])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0716, -0.6926,  0.2898,  ..., -0.5795,  0.0973,  0.8260])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(44.4460)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 2.5830e+06, -1.7461e+02],\n",
      "        [-1.7461e+02,  2.3203e+00]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.8458e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0019, -0.0664, -0.0212,  ...,  0.0057, -0.0066,  0.0196])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(534.9540, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(534.6619, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:33 | [trpo_pendulum] epoch #5 | Saving snapshot...\n",
      "2022-09-02 00:58:33 | [trpo_pendulum] epoch #5 | Saved\n",
      "2022-09-02 00:58:33 | [trpo_pendulum] epoch #5 | Time 6.64 s\n",
      "2022-09-02 00:58:33 | [trpo_pendulum] epoch #5 | EpochTime 1.07 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -616.219\n",
      "Evaluation/AverageReturn            -6209.37\n",
      "Evaluation/Iteration                    5\n",
      "Evaluation/MaxReturn                -6143.24\n",
      "Evaluation/MinReturn                -6275.5\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   66.1293\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       11988\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0716, -0.6926,  0.2898,  ..., -0.5795,  0.0973,  0.8260])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0696, -0.7590,  0.2686,  ..., -0.5738,  0.0906,  0.8456])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(47.7656)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.5679e+07, -3.8915e+02],\n",
      "        [-3.8915e+02,  2.0714e-01]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 4.2624e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.0010,  0.0379,  0.0130,  ..., -0.0030,  0.0048, -0.0089])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(557.0984, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(557.0101, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:34 | [trpo_pendulum] epoch #6 | Saving snapshot...\n",
      "2022-09-02 00:58:34 | [trpo_pendulum] epoch #6 | Saved\n",
      "2022-09-02 00:58:34 | [trpo_pendulum] epoch #6 | Time 7.81 s\n",
      "2022-09-02 00:58:34 | [trpo_pendulum] epoch #6 | EpochTime 1.17 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -644.873\n",
      "Evaluation/AverageReturn            -6481.93\n",
      "Evaluation/Iteration                    6\n",
      "Evaluation/MaxReturn                -6399.08\n",
      "Evaluation/MinReturn                -6564.78\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   82.8545\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       13986\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0696, -0.7590,  0.2686,  ..., -0.5738,  0.0906,  0.8456])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0707, -0.7211,  0.2816,  ..., -0.5768,  0.0954,  0.8367])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(106.7683)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.7680e+07, -2.5518e+03],\n",
      "        [-2.5518e+03, -1.1631e-01]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-4.2722e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0002,  0.0184,  0.0060,  ..., -0.0010,  0.0022, -0.0044])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(544.3719, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(544.0015, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:35 | [trpo_pendulum] epoch #7 | Saving snapshot...\n",
      "2022-09-02 00:58:35 | [trpo_pendulum] epoch #7 | Saved\n",
      "2022-09-02 00:58:35 | [trpo_pendulum] epoch #7 | Time 8.96 s\n",
      "2022-09-02 00:58:35 | [trpo_pendulum] epoch #7 | EpochTime 1.15 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -631.763\n",
      "Evaluation/AverageReturn            -6363.46\n",
      "Evaluation/Iteration                    7\n",
      "Evaluation/MaxReturn                -6332.9\n",
      "Evaluation/MinReturn                -6394.02\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   30.5628\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       15984\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0707, -0.7211,  0.2816,  ..., -0.5768,  0.0954,  0.8367])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0704, -0.7028,  0.2876,  ..., -0.5778,  0.0976,  0.8323])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(86.8656)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.3347e+06,  2.1706e+02],\n",
      "        [ 2.1706e+02, -1.2314e-02]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([4.8518e-05, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-2.4690e-05,  1.0059e-02,  4.2087e-03,  ..., -1.5511e-04,\n",
      "         1.9923e-03, -1.6685e-03])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(535.4747, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(535.3007, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:36 | [trpo_pendulum] epoch #8 | Saving snapshot...\n",
      "2022-09-02 00:58:36 | [trpo_pendulum] epoch #8 | Saved\n",
      "2022-09-02 00:58:36 | [trpo_pendulum] epoch #8 | Time 10.07 s\n",
      "2022-09-02 00:58:36 | [trpo_pendulum] epoch #8 | EpochTime 1.11 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -635.87\n",
      "Evaluation/AverageReturn            -6298.7\n",
      "Evaluation/Iteration                    8\n",
      "Evaluation/MaxReturn                -6287.56\n",
      "Evaluation/MinReturn                -6309.84\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   11.1378\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       17982\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0704, -0.7028,  0.2876,  ..., -0.5778,  0.0976,  0.8323])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0704, -0.6927,  0.2918,  ..., -0.5780,  0.0996,  0.8307])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(84.2761)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.3110e+07, -5.2068e+02],\n",
      "        [-5.2068e+02, -5.2147e-03]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.8510e-05,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.8521e-05,  5.1559e-03,  2.0483e-03,  ..., -1.1426e-04,\n",
      "         8.2739e-04, -9.5715e-04])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(528.3746, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(528.2759, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:37 | [trpo_pendulum] epoch #9 | Saving snapshot...\n",
      "2022-09-02 00:58:37 | [trpo_pendulum] epoch #9 | Saved\n",
      "2022-09-02 00:58:37 | [trpo_pendulum] epoch #9 | Time 11.20 s\n",
      "2022-09-02 00:58:37 | [trpo_pendulum] epoch #9 | EpochTime 1.12 s\n",
      "----------------------------------  ---------\n",
      "Evaluation/AverageDiscountedReturn   -628.263\n",
      "Evaluation/AverageReturn            -6239.07\n",
      "Evaluation/Iteration                    9\n",
      "Evaluation/MaxReturn                -6176.9\n",
      "Evaluation/MinReturn                -6301.25\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   62.175\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       19980\n",
      "----------------------------------  ---------\n",
      "old policy para is\n",
      "tensor([ 0.0704, -0.6927,  0.2918,  ..., -0.5780,  0.0996,  0.8307])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0704, -0.6875,  0.2939,  ..., -0.5781,  0.1004,  0.8297])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(25.3653)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.8074e+04, -5.8273e+00],\n",
      "        [-5.8273e+00, -5.0585e-03]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 4.2944e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0044, -0.0043, -0.0043,  ..., -0.0015, -0.0032, -0.0013])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(523.4263, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(523.6165, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:38 | [trpo_pendulum] epoch #10 | Saving snapshot...\n",
      "2022-09-02 00:58:38 | [trpo_pendulum] epoch #10 | Saved\n",
      "2022-09-02 00:58:38 | [trpo_pendulum] epoch #10 | Time 12.23 s\n",
      "2022-09-02 00:58:38 | [trpo_pendulum] epoch #10 | EpochTime 1.02 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -621.714\n",
      "Evaluation/AverageReturn            -6201.26\n",
      "Evaluation/Iteration                   10\n",
      "Evaluation/MaxReturn                -6186.47\n",
      "Evaluation/MinReturn                -6216.04\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   14.7896\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       21978\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0704, -0.6875,  0.2939,  ..., -0.5781,  0.1004,  0.8297])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0660, -0.6919,  0.2896,  ..., -0.5796,  0.0972,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(49.5324)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-7.2017e+05, -9.4381e+01],\n",
      "        [-9.4381e+01, -3.9498e-02]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.2756e-04, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([0.0037, 0.0016, 0.0013,  ..., 0.0003, 0.0011, 0.0003])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(520.9967, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(521.0957, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:39 | [trpo_pendulum] epoch #11 | Saving snapshot...\n",
      "2022-09-02 00:58:39 | [trpo_pendulum] epoch #11 | Saved\n",
      "2022-09-02 00:58:39 | [trpo_pendulum] epoch #11 | Time 13.23 s\n",
      "2022-09-02 00:58:39 | [trpo_pendulum] epoch #11 | EpochTime 1.00 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -620.916\n",
      "Evaluation/AverageReturn            -6193.82\n",
      "Evaluation/Iteration                   11\n",
      "Evaluation/MaxReturn                -6133.09\n",
      "Evaluation/MinReturn                -6254.55\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   60.7274\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       23976\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0660, -0.6919,  0.2896,  ..., -0.5796,  0.0972,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0697, -0.6902,  0.2909,  ..., -0.5793,  0.0983,  0.8287])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(99.7583)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 2.9033e+07, -7.9900e+02],\n",
      "        [-7.9900e+02,  2.7217e-02]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.4659e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0022, -0.0006, -0.0007,  ..., -0.0004, -0.0005, -0.0002])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(517.4554, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(517.3336, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:40 | [trpo_pendulum] epoch #12 | Saving snapshot...\n",
      "2022-09-02 00:58:40 | [trpo_pendulum] epoch #12 | Saved\n",
      "2022-09-02 00:58:40 | [trpo_pendulum] epoch #12 | Time 14.29 s\n",
      "2022-09-02 00:58:40 | [trpo_pendulum] epoch #12 | EpochTime 1.06 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -606.726\n",
      "Evaluation/AverageReturn            -6161.73\n",
      "Evaluation/Iteration                   12\n",
      "Evaluation/MaxReturn                -6135.75\n",
      "Evaluation/MinReturn                -6187.7\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   25.9773\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       25974\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0697, -0.6902,  0.2909,  ..., -0.5793,  0.0983,  0.8287])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0675, -0.6909,  0.2901,  ..., -0.5797,  0.0978,  0.8285])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(95.2310)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.0547e+07,  1.8296e+02],\n",
      "        [ 1.8296e+02, -3.4209e-03]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([8.8031e-06, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-0.0013, -0.0002, -0.0003,  ..., -0.0003, -0.0002, -0.0002])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(520.1454, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(520.1932, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:41 | [trpo_pendulum] epoch #13 | Saving snapshot...\n",
      "2022-09-02 00:58:42 | [trpo_pendulum] epoch #13 | Saved\n",
      "2022-09-02 00:58:42 | [trpo_pendulum] epoch #13 | Time 15.41 s\n",
      "2022-09-02 00:58:42 | [trpo_pendulum] epoch #13 | EpochTime 1.12 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -606.336\n",
      "Evaluation/AverageReturn            -6205.34\n",
      "Evaluation/Iteration                   13\n",
      "Evaluation/MaxReturn                -6172.66\n",
      "Evaluation/MinReturn                -6238.01\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   32.6777\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       27972\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0675, -0.6909,  0.2901,  ..., -0.5797,  0.0978,  0.8285])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0663, -0.6910,  0.2899,  ..., -0.5801,  0.0975,  0.8283])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(38.9474)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.7563e+06, -5.7292e+01],\n",
      "        [-5.7292e+01, -1.4491e-03]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 3.0077e-05, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.1597e-04,  7.8839e-04,  2.6918e-04,  ..., -1.7345e-04,\n",
      "         2.5870e-04,  3.6613e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(512.7775, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(512.7864, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:43 | [trpo_pendulum] epoch #14 | Saving snapshot...\n",
      "2022-09-02 00:58:43 | [trpo_pendulum] epoch #14 | Saved\n",
      "2022-09-02 00:58:43 | [trpo_pendulum] epoch #14 | Time 16.50 s\n",
      "2022-09-02 00:58:43 | [trpo_pendulum] epoch #14 | EpochTime 1.09 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -622.392\n",
      "Evaluation/AverageReturn            -6164.68\n",
      "Evaluation/Iteration                   14\n",
      "Evaluation/MaxReturn                -6132.15\n",
      "Evaluation/MinReturn                -6197.21\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   32.5262\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       29970\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0663, -0.6910,  0.2899,  ..., -0.5801,  0.0975,  0.8283])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0664, -0.6902,  0.2901,  ..., -0.5802,  0.0978,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(226.1858)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 3.6418e+07, -9.2889e+02],\n",
      "        [-9.2889e+02, -3.7275e-04]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([5.0413e-07, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 6.4354e-05,  5.4451e-04,  2.0528e-04,  ..., -1.1815e-04,\n",
      "         1.8950e-04,  2.8865e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(510.2899, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(510.2910, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:44 | [trpo_pendulum] epoch #15 | Saving snapshot...\n",
      "2022-09-02 00:58:44 | [trpo_pendulum] epoch #15 | Saved\n",
      "2022-09-02 00:58:44 | [trpo_pendulum] epoch #15 | Time 17.59 s\n",
      "2022-09-02 00:58:44 | [trpo_pendulum] epoch #15 | EpochTime 1.08 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -607.341\n",
      "Evaluation/AverageReturn            -6141.72\n",
      "Evaluation/Iteration                   15\n",
      "Evaluation/MaxReturn                -6116.23\n",
      "Evaluation/MinReturn                -6167.2\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   25.4837\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       31968\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0664, -0.6902,  0.2901,  ..., -0.5802,  0.0978,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0665, -0.6897,  0.2903,  ..., -0.5803,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(305.0264)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.8817e+07,  6.1087e+01],\n",
      "        [ 6.1087e+01,  1.0773e-03]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 2.5376e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-2.6299e-05,  1.1756e-04,  9.5455e-05,  ...,  3.4902e-07,\n",
      "         8.6787e-05,  4.8066e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(517.2197, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(517.2203, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:45 | [trpo_pendulum] epoch #16 | Saving snapshot...\n",
      "2022-09-02 00:58:45 | [trpo_pendulum] epoch #16 | Saved\n",
      "2022-09-02 00:58:45 | [trpo_pendulum] epoch #16 | Time 18.69 s\n",
      "2022-09-02 00:58:45 | [trpo_pendulum] epoch #16 | EpochTime 1.10 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -628\n",
      "Evaluation/AverageReturn            -6247.99\n",
      "Evaluation/Iteration                   16\n",
      "Evaluation/MaxReturn                -6173.55\n",
      "Evaluation/MinReturn                -6322.43\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   74.4383\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       33966\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0665, -0.6897,  0.2903,  ..., -0.5803,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0664, -0.6896,  0.2904,  ..., -0.5803,  0.0981,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(46.7079)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 5.4682e+06, -3.3436e+00],\n",
      "        [-3.3436e+00, -1.1738e-04]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-6.1204e-06,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.2043e-04, -2.0753e-05,  8.9095e-05,  ...,  1.3092e-04,\n",
      "         7.6635e-05,  7.9776e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(509.4172, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(509.4060, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:46 | [trpo_pendulum] epoch #17 | Saving snapshot...\n",
      "2022-09-02 00:58:46 | [trpo_pendulum] epoch #17 | Saved\n",
      "2022-09-02 00:58:46 | [trpo_pendulum] epoch #17 | Time 19.74 s\n",
      "2022-09-02 00:58:46 | [trpo_pendulum] epoch #17 | EpochTime 1.05 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -617.004\n",
      "Evaluation/AverageReturn            -6175.97\n",
      "Evaluation/Iteration                   17\n",
      "Evaluation/MaxReturn                -6162.51\n",
      "Evaluation/MinReturn                -6189.44\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   13.4644\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       35964\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0664, -0.6896,  0.2904,  ..., -0.5803,  0.0981,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0665, -0.6896,  0.2905,  ..., -0.5802,  0.0982,  0.8285])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(280.3989)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[1.0836e+07, 2.6143e+02],\n",
      "        [2.6143e+02, 4.4306e-03]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 4.1727e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-3.2691e-05, -2.7198e-04, -1.3768e-04,  ..., -3.8469e-05,\n",
      "        -1.0996e-04, -7.0380e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(513.5457, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(513.5435, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:47 | [trpo_pendulum] epoch #18 | Saving snapshot...\n",
      "2022-09-02 00:58:47 | [trpo_pendulum] epoch #18 | Saved\n",
      "2022-09-02 00:58:47 | [trpo_pendulum] epoch #18 | Time 20.83 s\n",
      "2022-09-02 00:58:47 | [trpo_pendulum] epoch #18 | EpochTime 1.09 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -622.21\n",
      "Evaluation/AverageReturn            -6238.86\n",
      "Evaluation/Iteration                   18\n",
      "Evaluation/MaxReturn                -6235.56\n",
      "Evaluation/MinReturn                -6242.15\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    3.29757\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       37962\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([ 0.0665, -0.6896,  0.2905,  ..., -0.5802,  0.0982,  0.8285])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0665, -0.6899,  0.2904,  ..., -0.5802,  0.0980,  0.8285])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(31.4883)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.9901e+06, -2.3874e+01],\n",
      "        [-2.3874e+01, -8.5809e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.0633e-06,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.8826e-05, -1.4807e-04, -7.8137e-05,  ..., -9.9662e-06,\n",
      "        -6.5063e-05, -3.5997e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(506.1104, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(506.1104, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:48 | [trpo_pendulum] epoch #19 | Saving snapshot...\n",
      "2022-09-02 00:58:48 | [trpo_pendulum] epoch #19 | Saved\n",
      "2022-09-02 00:58:48 | [trpo_pendulum] epoch #19 | Time 21.93 s\n",
      "2022-09-02 00:58:48 | [trpo_pendulum] epoch #19 | EpochTime 1.10 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -611.701\n",
      "Evaluation/AverageReturn            -6170.76\n",
      "Evaluation/Iteration                   19\n",
      "Evaluation/MaxReturn                -6117.18\n",
      "Evaluation/MinReturn                -6224.33\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   53.5758\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       39960\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0665, -0.6899,  0.2904,  ..., -0.5802,  0.0980,  0.8285])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0665, -0.6900,  0.2903,  ..., -0.5803,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(101.1903)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-8.0379e+05, -4.7685e+00],\n",
      "        [-4.7685e+00,  2.7113e-05]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([1.5905e-06, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-2.4239e-05, -8.0495e-05, -4.6627e-05,  ...,  2.9031e-06,\n",
      "        -4.0415e-05, -1.7864e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(502.7672, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(502.7679, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:49 | [trpo_pendulum] epoch #20 | Saving snapshot...\n",
      "2022-09-02 00:58:49 | [trpo_pendulum] epoch #20 | Saved\n",
      "2022-09-02 00:58:49 | [trpo_pendulum] epoch #20 | Time 23.06 s\n",
      "2022-09-02 00:58:49 | [trpo_pendulum] epoch #20 | EpochTime 1.13 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -609.012\n",
      "Evaluation/AverageReturn            -6151.04\n",
      "Evaluation/Iteration                   20\n",
      "Evaluation/MaxReturn                -6091.32\n",
      "Evaluation/MinReturn                -6210.76\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   59.7208\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       41958\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0665, -0.6900,  0.2903,  ..., -0.5803,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0665, -0.6901,  0.2903,  ..., -0.5803,  0.0979,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(222.9846)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-5.7951e+07, -2.8069e+01],\n",
      "        [-2.8069e+01, -2.3307e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.8472e-07,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.3515e-05, -4.1310e-05, -2.0792e-05,  ...,  4.1716e-06,\n",
      "        -1.6396e-05, -5.8673e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(509.9116, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(509.9110, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:50 | [trpo_pendulum] epoch #21 | Saving snapshot...\n",
      "2022-09-02 00:58:50 | [trpo_pendulum] epoch #21 | Saved\n",
      "2022-09-02 00:58:50 | [trpo_pendulum] epoch #21 | Time 24.08 s\n",
      "2022-09-02 00:58:50 | [trpo_pendulum] epoch #21 | EpochTime 1.01 s\n",
      "----------------------------------  ---------\n",
      "Evaluation/AverageDiscountedReturn   -633.085\n",
      "Evaluation/AverageReturn            -6262.64\n",
      "Evaluation/Iteration                   21\n",
      "Evaluation/MaxReturn                -6235.17\n",
      "Evaluation/MinReturn                -6290.12\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   27.477\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       43956\n",
      "----------------------------------  ---------\n",
      "old policy para is\n",
      "tensor([ 0.0665, -0.6901,  0.2903,  ..., -0.5803,  0.0979,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0665, -0.6901,  0.2902,  ..., -0.5803,  0.0979,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(160.6476)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.2830e+08,  4.8508e+01],\n",
      "        [ 4.8508e+01, -1.9450e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-4.8533e-08, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([6.6463e-06, 1.7498e-05, 9.4765e-06,  ..., 4.7763e-07, 6.6504e-06,\n",
      "        2.8279e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(499.2333, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(499.2333, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:51 | [trpo_pendulum] epoch #22 | Saving snapshot...\n",
      "2022-09-02 00:58:51 | [trpo_pendulum] epoch #22 | Saved\n",
      "2022-09-02 00:58:51 | [trpo_pendulum] epoch #22 | Time 25.13 s\n",
      "2022-09-02 00:58:51 | [trpo_pendulum] epoch #22 | EpochTime 1.05 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -624.511\n",
      "Evaluation/AverageReturn            -6164.02\n",
      "Evaluation/Iteration                   22\n",
      "Evaluation/MaxReturn                -6097.18\n",
      "Evaluation/MinReturn                -6230.86\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   66.8429\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       45954\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0665, -0.6901,  0.2902,  ..., -0.5803,  0.0979,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0665, -0.6901,  0.2902,  ..., -0.5803,  0.0979,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(140.9671)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-2.6962e+06, -4.0671e-01],\n",
      "        [-4.0671e-01,  1.1526e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([9.0959e-06, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([2.2068e-04, 9.4252e-05, 9.7137e-05,  ..., 5.1509e-05, 8.1585e-05,\n",
      "        4.6903e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(498.5023, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(498.5194, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:52 | [trpo_pendulum] epoch #23 | Saving snapshot...\n",
      "2022-09-02 00:58:52 | [trpo_pendulum] epoch #23 | Saved\n",
      "2022-09-02 00:58:52 | [trpo_pendulum] epoch #23 | Time 26.30 s\n",
      "2022-09-02 00:58:52 | [trpo_pendulum] epoch #23 | EpochTime 1.17 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -618.672\n",
      "Evaluation/AverageReturn            -6167.15\n",
      "Evaluation/Iteration                   23\n",
      "Evaluation/MaxReturn                -6145.09\n",
      "Evaluation/MinReturn                -6189.21\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   22.0566\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       47952\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0665, -0.6901,  0.2902,  ..., -0.5803,  0.0979,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0667, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(177.9538)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-1.8928e+05,  2.9937e+01],\n",
      "        [ 2.9937e+01,  2.5094e-04]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.5519e-06, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-1.0098e-04, -2.2225e-07, -2.5042e-05,  ..., -1.5768e-05,\n",
      "        -2.5499e-05, -1.3030e-05])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(501.7514, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(501.7510, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:53 | [trpo_pendulum] epoch #24 | Saving snapshot...\n",
      "2022-09-02 00:58:53 | [trpo_pendulum] epoch #24 | Saved\n",
      "2022-09-02 00:58:53 | [trpo_pendulum] epoch #24 | Time 27.37 s\n",
      "2022-09-02 00:58:53 | [trpo_pendulum] epoch #24 | EpochTime 1.06 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -629.333\n",
      "Evaluation/AverageReturn            -6226.43\n",
      "Evaluation/Iteration                   24\n",
      "Evaluation/MaxReturn                -6169.95\n",
      "Evaluation/MinReturn                -6282.91\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   56.4768\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       49950\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0667, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(128.4588)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[1.8279e+06, 1.8095e+00],\n",
      "        [1.8095e+00, 1.1326e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 4.2107e-07, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 5.6755e-05, -2.1316e-06,  7.0065e-06,  ...,  4.0697e-06,\n",
      "         7.7320e-06,  2.8016e-06])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(494.1866, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(494.1879, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:54 | [trpo_pendulum] epoch #25 | Saving snapshot...\n",
      "2022-09-02 00:58:54 | [trpo_pendulum] epoch #25 | Saved\n",
      "2022-09-02 00:58:54 | [trpo_pendulum] epoch #25 | Time 28.39 s\n",
      "2022-09-02 00:58:54 | [trpo_pendulum] epoch #25 | EpochTime 1.02 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -619.116\n",
      "Evaluation/AverageReturn            -6157.16\n",
      "Evaluation/Iteration                   25\n",
      "Evaluation/MaxReturn                -6076.05\n",
      "Evaluation/MinReturn                -6238.27\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   81.1088\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       51948\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(220.3439)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-7.3299e+07, -3.1964e+01],\n",
      "        [-3.1964e+01, -4.4890e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 2.5062e-07, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-2.6172e-05, -4.8059e-06, -4.3827e-06,  ...,  3.9916e-06,\n",
      "        -5.0690e-06,  3.0743e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(491.7603, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(491.7593, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:55 | [trpo_pendulum] epoch #26 | Saving snapshot...\n",
      "2022-09-02 00:58:56 | [trpo_pendulum] epoch #26 | Saved\n",
      "2022-09-02 00:58:56 | [trpo_pendulum] epoch #26 | Time 29.45 s\n",
      "2022-09-02 00:58:56 | [trpo_pendulum] epoch #26 | EpochTime 1.05 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -612.622\n",
      "Evaluation/AverageReturn            -6142.69\n",
      "Evaluation/Iteration                   26\n",
      "Evaluation/MaxReturn                -6127.35\n",
      "Evaluation/MinReturn                -6158.03\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   15.3431\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       53946\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(139.1776)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-6.0886e+06, -2.6488e+00],\n",
      "        [-2.6488e+00, -1.5227e-06]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 2.5496e-07, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 1.4547e-05,  9.2720e-06,  6.0155e-06,  ..., -2.5898e-06,\n",
      "         5.1542e-06,  4.6855e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(492.4299, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(492.4301, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:57 | [trpo_pendulum] epoch #27 | Saving snapshot...\n",
      "2022-09-02 00:58:57 | [trpo_pendulum] epoch #27 | Saved\n",
      "2022-09-02 00:58:57 | [trpo_pendulum] epoch #27 | Time 30.49 s\n",
      "2022-09-02 00:58:57 | [trpo_pendulum] epoch #27 | EpochTime 1.05 s\n",
      "----------------------------------  -----------\n",
      "Evaluation/AverageDiscountedReturn   -622.492\n",
      "Evaluation/AverageReturn            -6175.32\n",
      "Evaluation/Iteration                   27\n",
      "Evaluation/MaxReturn                -6172.59\n",
      "Evaluation/MinReturn                -6178.05\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    2.72583\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       55944\n",
      "----------------------------------  -----------\n",
      "old policy para is\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(143.4240)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 4.1729e+06, -1.5030e+00],\n",
      "        [-1.5030e+00,  1.0443e-07]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([3.8362e-08, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 7.2833e-06,  5.7213e-06,  3.9269e-06,  ..., -1.2225e-06,\n",
      "         3.1604e-06,  4.5870e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(489.6480, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(489.6480, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:58 | [trpo_pendulum] epoch #28 | Saving snapshot...\n",
      "2022-09-02 00:58:58 | [trpo_pendulum] epoch #28 | Saved\n",
      "2022-09-02 00:58:58 | [trpo_pendulum] epoch #28 | Time 31.52 s\n",
      "2022-09-02 00:58:58 | [trpo_pendulum] epoch #28 | EpochTime 1.03 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -607.822\n",
      "Evaluation/AverageReturn            -6149.31\n",
      "Evaluation/Iteration                   28\n",
      "Evaluation/MaxReturn                -6124.49\n",
      "Evaluation/MinReturn                -6174.12\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   24.8174\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       57942\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(28.2511)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 4.4125e+06, -4.9478e-01],\n",
      "        [-4.9478e-01,  1.1892e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([3.5714e-08, 5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 3.5559e-06,  2.5442e-06,  1.7519e-06,  ..., -5.8951e-07,\n",
      "         1.4322e-06,  1.8010e-07])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(487.9704, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(487.9704, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:58:59 | [trpo_pendulum] epoch #29 | Saving snapshot...\n",
      "2022-09-02 00:58:59 | [trpo_pendulum] epoch #29 | Saved\n",
      "2022-09-02 00:58:59 | [trpo_pendulum] epoch #29 | Time 32.67 s\n",
      "2022-09-02 00:58:59 | [trpo_pendulum] epoch #29 | EpochTime 1.14 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -623.054\n",
      "Evaluation/AverageReturn            -6163.84\n",
      "Evaluation/Iteration                   29\n",
      "Evaluation/MaxReturn                -6113.49\n",
      "Evaluation/MinReturn                -6214.18\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   50.3469\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       59940\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(35.6724)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-4.0123e+04,  1.7137e-02],\n",
      "        [ 1.7137e-02,  6.2922e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([ 1.7438e-07, -5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-9.7301e-07, -8.4959e-07, -6.2110e-07,  ...,  1.2352e-07,\n",
      "        -5.9869e-07, -9.3396e-08])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(489.1146, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(489.1146, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:59:00 | [trpo_pendulum] epoch #30 | Saving snapshot...\n",
      "2022-09-02 00:59:00 | [trpo_pendulum] epoch #30 | Saved\n",
      "2022-09-02 00:59:00 | [trpo_pendulum] epoch #30 | Time 33.62 s\n",
      "2022-09-02 00:59:00 | [trpo_pendulum] epoch #30 | EpochTime 0.95 s\n",
      "----------------------------------  ---------\n",
      "Evaluation/AverageDiscountedReturn   -630.956\n",
      "Evaluation/AverageReturn            -6199.36\n",
      "Evaluation/Iteration                   30\n",
      "Evaluation/MaxReturn                -6177.72\n",
      "Evaluation/MinReturn                -6221.01\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   21.646\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       61938\n",
      "----------------------------------  ---------\n",
      "old policy para is\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(148.7482)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 7.4999e+06, -5.0438e-01],\n",
      "        [-5.0438e-01, -2.4207e-08]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-1.4139e-09,  5.0000e-01])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([-4.8884e-07, -4.1825e-07, -3.0140e-07,  ...,  7.2247e-08,\n",
      "        -2.8487e-07, -4.9780e-08])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(491.2676, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(491.2676, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:59:01 | [trpo_pendulum] epoch #31 | Saving snapshot...\n",
      "2022-09-02 00:59:01 | [trpo_pendulum] epoch #31 | Saved\n",
      "2022-09-02 00:59:01 | [trpo_pendulum] epoch #31 | Time 34.78 s\n",
      "2022-09-02 00:59:01 | [trpo_pendulum] epoch #31 | EpochTime 1.15 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -627.33\n",
      "Evaluation/AverageReturn            -6233.61\n",
      "Evaluation/Iteration                   31\n",
      "Evaluation/MaxReturn                -6215.46\n",
      "Evaluation/MinReturn                -6251.77\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   18.1513\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       63936\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(120.7394)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[-3.2178e+07, -8.5524e-02],\n",
      "        [-8.5524e-02, -5.5260e-09]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([5.0000e-01, 1.3289e-09])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 0.6276, 87.9784, 42.1138,  ..., -8.5605, 30.4638,  5.9914])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(488.5670, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(-0., grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:59:02 | [trpo_pendulum] epoch #32 | Saving snapshot...\n",
      "2022-09-02 00:59:02 | [trpo_pendulum] epoch #32 | Saved\n",
      "2022-09-02 00:59:02 | [trpo_pendulum] epoch #32 | Time 35.82 s\n",
      "2022-09-02 00:59:02 | [trpo_pendulum] epoch #32 | EpochTime 1.04 s\n",
      "----------------------------------  ----------\n",
      "Evaluation/AverageDiscountedReturn   -620.269\n",
      "Evaluation/AverageReturn            -6216.04\n",
      "Evaluation/Iteration                   32\n",
      "Evaluation/MaxReturn                -6191.52\n",
      "Evaluation/MinReturn                -6240.57\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                   24.5243\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       65934\n",
      "----------------------------------  ----------\n",
      "old policy para is\n",
      "tensor([ 0.0666, -0.6900,  0.2903,  ..., -0.5802,  0.0980,  0.8284])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([ 0.6942, 87.2884, 42.4042,  ..., -9.1408, 30.5618,  6.8198])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(798154.2500)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[ 2.8961e+14,  1.1899e+11],\n",
      "        [ 1.1899e+11, -1.2928e+10]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([-0.0750, -0.4943])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([ 20.4336, -43.4918, -20.8188,  ...,   0.6629, -18.6287,   0.6077])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(671282.1250, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(0.0028, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:59:03 | [trpo_pendulum] epoch #33 | Saving snapshot...\n",
      "2022-09-02 00:59:03 | [trpo_pendulum] epoch #33 | Saved\n",
      "2022-09-02 00:59:03 | [trpo_pendulum] epoch #33 | Time 36.93 s\n",
      "2022-09-02 00:59:03 | [trpo_pendulum] epoch #33 | EpochTime 1.10 s\n",
      "----------------------------------  -----------------\n",
      "Evaluation/AverageDiscountedReturn  -725979\n",
      "Evaluation/AverageReturn                 -7.42556e+06\n",
      "Evaluation/Iteration                     33\n",
      "Evaluation/MaxReturn                     -7.42362e+06\n",
      "Evaluation/MinReturn                     -7.4275e+06\n",
      "Evaluation/NumEpisodes                    2\n",
      "Evaluation/StdReturn                   1938.09\n",
      "Evaluation/TerminationRate                0\n",
      "TotalEnvSteps                         67932\n",
      "----------------------------------  -----------------\n",
      "old policy para is\n",
      "tensor([ 0.6942, 87.2884, 42.4042,  ..., -9.1408, 30.5618,  6.8198])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([21.1278, 43.7966, 21.5853,  ..., -8.4779, 11.9331,  7.4275])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(7.4929e+17)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[       nan, 1.1137e+37],\n",
      "        [1.1137e+37, 6.0745e+20]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(2.0477e+19, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "2022-09-02 00:59:04 | [trpo_pendulum] epoch #34 | Saving snapshot...\n",
      "2022-09-02 00:59:04 | [trpo_pendulum] epoch #34 | Saved\n",
      "2022-09-02 00:59:04 | [trpo_pendulum] epoch #34 | Time 38.01 s\n",
      "2022-09-02 00:59:04 | [trpo_pendulum] epoch #34 | EpochTime 1.08 s\n",
      "----------------------------------  ---------------\n",
      "Evaluation/AverageDiscountedReturn     -2.22914e+19\n",
      "Evaluation/AverageReturn               -2.26637e+20\n",
      "Evaluation/Iteration                   34\n",
      "Evaluation/MaxReturn                   -2.12774e+20\n",
      "Evaluation/MinReturn                   -2.405e+20\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                    1.38626e+19\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       69930\n",
      "----------------------------------  ---------------\n",
      "old policy para is\n",
      "tensor([21.1278, 43.7966, 21.5853,  ..., -8.4779, 11.9331,  7.4275])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:05 | [trpo_pendulum] epoch #35 | Saving snapshot...\n",
      "2022-09-02 00:59:05 | [trpo_pendulum] epoch #35 | Saved\n",
      "2022-09-02 00:59:05 | [trpo_pendulum] epoch #35 | Time 39.11 s\n",
      "2022-09-02 00:59:05 | [trpo_pendulum] epoch #35 | EpochTime 1.10 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   35\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       71928\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:06 | [trpo_pendulum] epoch #36 | Saving snapshot...\n",
      "2022-09-02 00:59:06 | [trpo_pendulum] epoch #36 | Saved\n",
      "2022-09-02 00:59:06 | [trpo_pendulum] epoch #36 | Time 40.37 s\n",
      "2022-09-02 00:59:06 | [trpo_pendulum] epoch #36 | EpochTime 1.25 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   36\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       73926\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:07 | [trpo_pendulum] epoch #37 | Saving snapshot...\n",
      "2022-09-02 00:59:08 | [trpo_pendulum] epoch #37 | Saved\n",
      "2022-09-02 00:59:08 | [trpo_pendulum] epoch #37 | Time 41.44 s\n",
      "2022-09-02 00:59:08 | [trpo_pendulum] epoch #37 | EpochTime 1.06 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   37\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       75924\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:09 | [trpo_pendulum] epoch #38 | Saving snapshot...\n",
      "2022-09-02 00:59:09 | [trpo_pendulum] epoch #38 | Saved\n",
      "2022-09-02 00:59:09 | [trpo_pendulum] epoch #38 | Time 42.55 s\n",
      "2022-09-02 00:59:09 | [trpo_pendulum] epoch #38 | EpochTime 1.11 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   38\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       77922\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:10 | [trpo_pendulum] epoch #39 | Saving snapshot...\n",
      "2022-09-02 00:59:10 | [trpo_pendulum] epoch #39 | Saved\n",
      "2022-09-02 00:59:10 | [trpo_pendulum] epoch #39 | Time 43.64 s\n",
      "2022-09-02 00:59:10 | [trpo_pendulum] epoch #39 | EpochTime 1.08 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   39\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       79920\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:11 | [trpo_pendulum] epoch #40 | Saving snapshot...\n",
      "2022-09-02 00:59:11 | [trpo_pendulum] epoch #40 | Saved\n",
      "2022-09-02 00:59:11 | [trpo_pendulum] epoch #40 | Time 44.80 s\n",
      "2022-09-02 00:59:11 | [trpo_pendulum] epoch #40 | EpochTime 1.16 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   40\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       81918\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:12 | [trpo_pendulum] epoch #41 | Saving snapshot...\n",
      "2022-09-02 00:59:12 | [trpo_pendulum] epoch #41 | Saved\n",
      "2022-09-02 00:59:12 | [trpo_pendulum] epoch #41 | Time 45.92 s\n",
      "2022-09-02 00:59:12 | [trpo_pendulum] epoch #41 | EpochTime 1.11 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   41\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       83916\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:13 | [trpo_pendulum] epoch #42 | Saving snapshot...\n",
      "2022-09-02 00:59:13 | [trpo_pendulum] epoch #42 | Saved\n",
      "2022-09-02 00:59:13 | [trpo_pendulum] epoch #42 | Time 47.07 s\n",
      "2022-09-02 00:59:13 | [trpo_pendulum] epoch #42 | EpochTime 1.14 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   42\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       85914\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:14 | [trpo_pendulum] epoch #43 | Saving snapshot...\n",
      "2022-09-02 00:59:14 | [trpo_pendulum] epoch #43 | Saved\n",
      "2022-09-02 00:59:14 | [trpo_pendulum] epoch #43 | Time 48.21 s\n",
      "2022-09-02 00:59:14 | [trpo_pendulum] epoch #43 | EpochTime 1.14 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   43\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       87912\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:15 | [trpo_pendulum] epoch #44 | Saving snapshot...\n",
      "2022-09-02 00:59:15 | [trpo_pendulum] epoch #44 | Saved\n",
      "2022-09-02 00:59:15 | [trpo_pendulum] epoch #44 | Time 49.27 s\n",
      "2022-09-02 00:59:15 | [trpo_pendulum] epoch #44 | EpochTime 1.05 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   44\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       89910\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:16 | [trpo_pendulum] epoch #45 | Saving snapshot...\n",
      "2022-09-02 00:59:16 | [trpo_pendulum] epoch #45 | Saved\n",
      "2022-09-02 00:59:16 | [trpo_pendulum] epoch #45 | Time 50.38 s\n",
      "2022-09-02 00:59:16 | [trpo_pendulum] epoch #45 | EpochTime 1.11 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   45\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       91908\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:17 | [trpo_pendulum] epoch #46 | Saving snapshot...\n",
      "2022-09-02 00:59:18 | [trpo_pendulum] epoch #46 | Saved\n",
      "2022-09-02 00:59:18 | [trpo_pendulum] epoch #46 | Time 51.41 s\n",
      "2022-09-02 00:59:18 | [trpo_pendulum] epoch #46 | EpochTime 1.03 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   46\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       93906\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:19 | [trpo_pendulum] epoch #47 | Saving snapshot...\n",
      "2022-09-02 00:59:19 | [trpo_pendulum] epoch #47 | Saved\n",
      "2022-09-02 00:59:19 | [trpo_pendulum] epoch #47 | Time 52.52 s\n",
      "2022-09-02 00:59:19 | [trpo_pendulum] epoch #47 | EpochTime 1.10 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   47\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       95904\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:20 | [trpo_pendulum] epoch #48 | Saving snapshot...\n",
      "2022-09-02 00:59:20 | [trpo_pendulum] epoch #48 | Saved\n",
      "2022-09-02 00:59:20 | [trpo_pendulum] epoch #48 | Time 53.70 s\n",
      "2022-09-02 00:59:20 | [trpo_pendulum] epoch #48 | EpochTime 1.17 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   48\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       97902\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:21 | [trpo_pendulum] epoch #49 | Saving snapshot...\n",
      "2022-09-02 00:59:21 | [trpo_pendulum] epoch #49 | Saved\n",
      "2022-09-02 00:59:21 | [trpo_pendulum] epoch #49 | Time 54.91 s\n",
      "2022-09-02 00:59:21 | [trpo_pendulum] epoch #49 | EpochTime 1.21 s\n",
      "----------------------------------  -----\n",
      "Evaluation/AverageDiscountedReturn    nan\n",
      "Evaluation/AverageReturn              nan\n",
      "Evaluation/Iteration                   49\n",
      "Evaluation/MaxReturn                  nan\n",
      "Evaluation/MinReturn                  nan\n",
      "Evaluation/NumEpisodes                  2\n",
      "Evaluation/StdReturn                  nan\n",
      "Evaluation/TerminationRate              0\n",
      "TotalEnvSteps                       99900\n",
      "----------------------------------  -----\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:22 | [trpo_pendulum] epoch #50 | Saving snapshot...\n",
      "2022-09-02 00:59:22 | [trpo_pendulum] epoch #50 | Saved\n",
      "2022-09-02 00:59:22 | [trpo_pendulum] epoch #50 | Time 56.09 s\n",
      "2022-09-02 00:59:22 | [trpo_pendulum] epoch #50 | EpochTime 1.18 s\n",
      "----------------------------------  ------\n",
      "Evaluation/AverageDiscountedReturn     nan\n",
      "Evaluation/AverageReturn               nan\n",
      "Evaluation/Iteration                    50\n",
      "Evaluation/MaxReturn                   nan\n",
      "Evaluation/MinReturn                   nan\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   nan\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       101898\n",
      "----------------------------------  ------\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:23 | [trpo_pendulum] epoch #51 | Saving snapshot...\n",
      "2022-09-02 00:59:23 | [trpo_pendulum] epoch #51 | Saved\n",
      "2022-09-02 00:59:23 | [trpo_pendulum] epoch #51 | Time 57.16 s\n",
      "2022-09-02 00:59:23 | [trpo_pendulum] epoch #51 | EpochTime 1.07 s\n",
      "----------------------------------  ------\n",
      "Evaluation/AverageDiscountedReturn     nan\n",
      "Evaluation/AverageReturn               nan\n",
      "Evaluation/Iteration                    51\n",
      "Evaluation/MaxReturn                   nan\n",
      "Evaluation/MinReturn                   nan\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   nan\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       103896\n",
      "----------------------------------  ------\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:24 | [trpo_pendulum] epoch #52 | Saving snapshot...\n",
      "2022-09-02 00:59:24 | [trpo_pendulum] epoch #52 | Saved\n",
      "2022-09-02 00:59:24 | [trpo_pendulum] epoch #52 | Time 58.36 s\n",
      "2022-09-02 00:59:24 | [trpo_pendulum] epoch #52 | EpochTime 1.19 s\n",
      "----------------------------------  ------\n",
      "Evaluation/AverageDiscountedReturn     nan\n",
      "Evaluation/AverageReturn               nan\n",
      "Evaluation/Iteration                    52\n",
      "Evaluation/MaxReturn                   nan\n",
      "Evaluation/MinReturn                   nan\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   nan\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       105894\n",
      "----------------------------------  ------\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:25 | [trpo_pendulum] epoch #53 | Saving snapshot...\n",
      "2022-09-02 00:59:25 | [trpo_pendulum] epoch #53 | Saved\n",
      "2022-09-02 00:59:25 | [trpo_pendulum] epoch #53 | Time 59.40 s\n",
      "2022-09-02 00:59:25 | [trpo_pendulum] epoch #53 | EpochTime 1.04 s\n",
      "----------------------------------  ------\n",
      "Evaluation/AverageDiscountedReturn     nan\n",
      "Evaluation/AverageReturn               nan\n",
      "Evaluation/Iteration                    53\n",
      "Evaluation/MaxReturn                   nan\n",
      "Evaluation/MinReturn                   nan\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   nan\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       107892\n",
      "----------------------------------  ------\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:27 | [trpo_pendulum] epoch #54 | Saving snapshot...\n",
      "2022-09-02 00:59:27 | [trpo_pendulum] epoch #54 | Saved\n",
      "2022-09-02 00:59:27 | [trpo_pendulum] epoch #54 | Time 60.57 s\n",
      "2022-09-02 00:59:27 | [trpo_pendulum] epoch #54 | EpochTime 1.16 s\n",
      "----------------------------------  ------\n",
      "Evaluation/AverageDiscountedReturn     nan\n",
      "Evaluation/AverageReturn               nan\n",
      "Evaluation/Iteration                    54\n",
      "Evaluation/MaxReturn                   nan\n",
      "Evaluation/MinReturn                   nan\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   nan\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       109890\n",
      "----------------------------------  ------\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:28 | [trpo_pendulum] epoch #55 | Saving snapshot...\n",
      "2022-09-02 00:59:28 | [trpo_pendulum] epoch #55 | Saved\n",
      "2022-09-02 00:59:28 | [trpo_pendulum] epoch #55 | Time 61.62 s\n",
      "2022-09-02 00:59:28 | [trpo_pendulum] epoch #55 | EpochTime 1.05 s\n",
      "----------------------------------  ------\n",
      "Evaluation/AverageDiscountedReturn     nan\n",
      "Evaluation/AverageReturn               nan\n",
      "Evaluation/Iteration                    55\n",
      "Evaluation/MaxReturn                   nan\n",
      "Evaluation/MinReturn                   nan\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   nan\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       111888\n",
      "----------------------------------  ------\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:29 | [trpo_pendulum] epoch #56 | Saving snapshot...\n",
      "2022-09-02 00:59:29 | [trpo_pendulum] epoch #56 | Saved\n",
      "2022-09-02 00:59:29 | [trpo_pendulum] epoch #56 | Time 62.69 s\n",
      "2022-09-02 00:59:29 | [trpo_pendulum] epoch #56 | EpochTime 1.07 s\n",
      "----------------------------------  ------\n",
      "Evaluation/AverageDiscountedReturn     nan\n",
      "Evaluation/AverageReturn               nan\n",
      "Evaluation/Iteration                    56\n",
      "Evaluation/MaxReturn                   nan\n",
      "Evaluation/MinReturn                   nan\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   nan\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       113886\n",
      "----------------------------------  ------\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-02 00:59:30 | [trpo_pendulum] epoch #57 | Saving snapshot...\n",
      "2022-09-02 00:59:30 | [trpo_pendulum] epoch #57 | Saved\n",
      "2022-09-02 00:59:30 | [trpo_pendulum] epoch #57 | Time 63.81 s\n",
      "2022-09-02 00:59:30 | [trpo_pendulum] epoch #57 | EpochTime 1.11 s\n",
      "----------------------------------  ------\n",
      "Evaluation/AverageDiscountedReturn     nan\n",
      "Evaluation/AverageReturn               nan\n",
      "Evaluation/Iteration                    57\n",
      "Evaluation/MaxReturn                   nan\n",
      "Evaluation/MinReturn                   nan\n",
      "Evaluation/NumEpisodes                   2\n",
      "Evaluation/StdReturn                   nan\n",
      "Evaluation/TerminationRate               0\n",
      "TotalEnvSteps                       115884\n",
      "----------------------------------  ------\n",
      "old policy para is\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "params now is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "----------------------------------\n",
      "g norm is:\n",
      "tensor(nan)\n",
      "-------------------\n",
      "G is:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan]])\n",
      "-------------------\n",
      "alpha is: \n",
      "tensor([nan, nan])\n",
      "-------------------\n",
      "decent step is:\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan])\n",
      "--------------------\n",
      "loss before is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n",
      "loss now is:\n",
      "tensor(nan, grad_fn=<NegBackward>)\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-807143eac63d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrpo_pendulum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1234\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\garage\\experiment\\experiment.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[0mctxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop_prefix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f72a775cf3be>\u001b[0m in \u001b[0;36mtrpo_pendulum\u001b[1;34m(ctxt, seed)\u001b[0m\n\u001b[0;32m     27\u001b[0m                       center_adv=False)\n\u001b[0;32m     28\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\garage\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_epochs, batch_size, plot, store_episodes, pause_for_plot)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0mdump_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m         \u001b[0maverage_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_algo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown_worker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Github\\DRSOM-for-RL\\TRPO_DRSOM.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobtain_episodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_itr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mlast_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_itr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_itr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Github\\DRSOM-for-RL\\TRPO_DRSOM.py\u001b[0m in \u001b[0;36m_train_once\u001b[1;34m(self, itr, eps)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0madvs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_advantage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         self._train(obs_flat, actions_flat, rewards_flat, returns_flat,\n\u001b[0m\u001b[0;32m     92\u001b[0m                     advs_flat, valids, itr)\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Github\\DRSOM-for-RL\\TRPO_DRSOM.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, obs, actions, rewards, returns, advs, valids, itr)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_policy_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_policy_first_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vf_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_value_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\garage\\torch\\optimizers\\optimizer_wrapper.py\u001b[0m in \u001b[0;36mget_minibatch\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_optimization_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\garage\\np\\optimizers\\minibatch_dataset.py\u001b[0m in \u001b[0;36miterate\u001b[1;34m(self, update)\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mbatch_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mitr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mbatch_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extra_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\garage\\np\\optimizers\\minibatch_dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mbatch_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mitr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mbatch_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extra_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(2.):\n",
    "    trpo_pendulum(seed=1234)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
